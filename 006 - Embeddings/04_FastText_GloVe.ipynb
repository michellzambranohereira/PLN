{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzh9Sg9UeB_J"
      },
      "source": [
        "# Cuaderno Clase PLN - GloVe y FastText\n",
        "\n",
        "**Objetivos:**\n",
        "*   Conocer enfoques alternativos a Word2Vec: GloVe (conteos globales) y FastText (subpalabras).\n",
        "*   Entender la ventaja clave de FastText para manejar palabras fuera de vocabulario (OOV - Out Of Vocabulary).\n",
        "*   Cargar y usar vectores FastText pre-entrenados.\n",
        "*   Comparar resultados y capacidades (especialmente OOV) con Word2Vec.\n",
        "*   Reflexionar sobre cómo evaluar la calidad de los embeddings y detectar sesgos.\n",
        "\n",
        "**Agenda:**\n",
        "1.  Instalaciones e Importaciones\n",
        "2.  Repaso Rápido: Word2Vec y sus limitaciones (OOV)\n",
        "3.  GloVe: Vectores Globales desde Co-ocurrencias\n",
        "4.  FastText: El Poder de las Subpalabras (¡Adiós OOV!)\n",
        "5.  Cargando Vectores FastText Pre-entrenados\n",
        "6.  Explorando FastText: Similitud, Analogías y ¡OOV!\n",
        "7.  Comparativa: Word2Vec vs FastText (foco en OOV)\n",
        "8.  Micro-Laboratorio (Ejercicio Práctico)\n",
        "9.  Brainstorming: Evaluación y Detección de Sesgos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpBrBQ7-ejz2"
      },
      "source": [
        "# 1. Instalaciones e Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG4_rcP3eXrs",
        "outputId": "d8de2749-ed0f-4569-afd2-0d6d8d8224bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.1 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Necesitamos gensim principalmente\n",
        "!pip install gensim > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L56qQY6vevNr",
        "outputId": "71bfc777-6799-43a3-a1b0-a850276af9e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gensim 4.3.3\n",
            "Uninstalling gensim-4.3.3:\n",
            "  Successfully uninstalled gensim-4.3.3\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Using cached gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "Installing collected packages: gensim\n",
            "Successfully installed gensim-4.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall gensim -y # Remove the existing gensim installation\n",
        "!pip install gensim # Reinstall gensim to align with the NumPy version\n",
        "# Restart the kernel to ensure the changes take effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwqWN5fAeBAl",
        "outputId": "148186fd-bb7f-40a4-e90d-d5fc9d0d0327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Librerías importadas.\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors, FastText # Ahora importamos FastText también\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Librerías importadas.\")\n",
        "\n",
        "# **Importante:** Para este cuaderno, idealmente necesitaremos:\n",
        "# 1. El modelo Word2Vec cargado previamente (para comparar).\n",
        "# 2. Un modelo FastText pre-entrenado para español (¡necesitamos descargarlo!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0plow6ye_fl"
      },
      "source": [
        "# 2. Repaso Rápido: Word2Vec y sus limitaciones (OOV)\n",
        "\n",
        "Recordemos:\n",
        "*   Word2Vec (CBOW/Skip-gram) aprende embeddings prediciendo palabras en contextos locales.\n",
        "*   Genera vectores densos que capturan semántica (similitud, analogías).\n",
        "*   Usamos modelos pre-entrenados porque entrenarlos es costoso.\n",
        "\n",
        "**Una limitación importante:** Word2Vec asigna un vector a cada palabra *completa* que vio durante el entrenamiento. Si en tiempo de uso aparece una palabra que **NO estaba en el vocabulario original** (Out-Of-Vocabulary - OOV):\n",
        "*   Un error tipográfico (\"computadorra\").\n",
        "*   Una palabra nueva o muy rara (\"covid\", \"guasap\").\n",
        "*   Una variante morfológica no vista (\"cantábamos\").\n",
        "\n",
        "**¿Qué pasa con Word2Vec?** ¡Generalmente da un error (`KeyError`) o asigna un vector genérico de \"desconocido\" (UNK - Unknown), perdiendo toda información específica! Esto es un problema en aplicaciones reales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8cCxcvmfc39"
      },
      "source": [
        "# 3. GloVe: Vectores Globales desde Co-ocurrencias\n",
        "\n",
        "GloVe (Global Vectors for Word Representation) de Stanford es otra técnica popular para obtener embeddings.\n",
        "\n",
        "*   **Enfoque Diferente:** En lugar de predecir contextos locales (como Word2Vec), GloVe se enfoca en las **estadísticas globales de co-ocurrencia** de palabras en todo el corpus.\n",
        "*   **Idea Central:** La *relación* entre las probabilidades de co-ocurrencia de dos palabras con una tercera palabra \"sonda\" puede revelar información sobre su significado. Por ejemplo, la probabilidad de que \"hielo\" aparezca cerca de \"sólido\" será alta, mientras que cerca de \"gas\" será baja. La probabilidad de que \"vapor\" aparezca cerca de \"gas\" será alta y cerca de \"sólido\" baja. GloVe modela estas *proporciones* de probabilidades.\n",
        "*   **Entrenamiento:** Construye una matriz gigante de co-ocurrencia (cuántas veces la palabra X aparece cerca de la palabra Y) y luego usa factorización de matrices para encontrar los vectores de palabras que mejor explican esa matriz.\n",
        "*   **Resultado:** Embeddings densos similares a Word2Vec, a menudo muy buenos para tareas de similitud y analogía.\n",
        "*   **Limitación OOV:** Al igual que Word2Vec, GloVe tradicionalmente opera a nivel de palabra completa, por lo que también sufre del problema de OOV.\n",
        "\n",
        "*(Nota: Encontrar modelos GloVe pre-entrenados y de buena calidad para español puede ser más difícil que para Word2Vec o FastText. A menudo se distribuyen en formato texto).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94o6eSPaf5Fu"
      },
      "source": [
        "# 4. FastText: El Poder de las Subpalabras (¡Adiós OOV!)\n",
        "\n",
        "FastText, desarrollado por Facebook AI Research (FAIR), es una extensión inteligente de Word2Vec (Skip-gram) que aborda directamente el problema OOV.\n",
        "\n",
        "*   **¡La Gran Idea!:** FastText no trata las palabras como unidades indivisibles. Representa cada palabra como una **bolsa de n-gramas de caracteres**, además de la palabra completa.\n",
        "    *   Ejemplo: Para la palabra \"amarillo\" y usando n=3 (trigramas):\n",
        "        *   Se consideran n-gramas como: `<am`, `ama`, `mar`, `ari`, `ril`, `ill`, `llo`, `lo>` (con `<` y `>` marcando inicio y fin).\n",
        "        *   También se considera la palabra completa: `<amarillo>`\n",
        "*   **Aprendizaje:** El modelo aprende vectores no solo para las palabras completas, sino **para todos los n-gramas de caracteres** vistos en el corpus.\n",
        "*   **Vector Final:** El vector de una palabra se obtiene **sumando los vectores de todos sus n-gramas de caracteres** (y el vector de la palabra completa si existe).\n",
        "\n",
        "**¡La Ventaja Clave - Manejo de OOV!**\n",
        "*   Si aparece una palabra nueva que no estaba en el vocabulario (ej: \"amarillento\"), FastText **aún puede construir un vector para ella**. ¿Cómo? Sumando los vectores de los n-gramas que sí conoce y que forman parte de la palabra nueva (ej: `<am`, `ama`, `mar`, `ari`, `ril`, `ill`, `lle`, `len`, `ent`, `nto`, `to>`).\n",
        "*   El vector resultante captura información de las \"partes\" de la palabra, lo que a menudo resulta en una representación razonable incluso para palabras desconocidas.\n",
        "\n",
        "**Otros Beneficios:**\n",
        "*   Funciona muy bien para **lenguajes morfológicamente ricos** (como el español) donde muchas palabras comparten raíces y afijos (ej: \"nación\", \"nacional\", \"nacionalidad\"). Comparten n-gramas, sus vectores estarán relacionados.\n",
        "*   Es más robusto a **errores tipográficos** (\"computadorra\" compartirá muchos n-gramas con \"computadora\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU1XFPYAgZDl"
      },
      "source": [
        "# 5. Cargando Vectores FastText Pre-entrenados\n",
        "\n",
        "Al igual que con Word2Vec, lo usual es usar modelos FastText pre-entrenados.\n",
        "\n",
        "**Tarea:** Descargar los vectores pre-entrenados para **español** desde el sitio oficial de FastText: [https://fasttext.cc/docs/en/pretrained-vectors.html](https://fasttext.cc/docs/en/pretrained-vectors.html)\n",
        "\n",
        "*   **Importante:** Descargar el archivo `.bin`. Este formato contiene no solo los vectores de las palabras del vocabulario, sino también la información de los n-gramas necesaria para calcular vectores de palabras OOV. El formato `.vec` es más pequeño pero pierde esta capacidad OOV.\n",
        "*   El archivo `.bin` para español también es grande (varios GB). Anoten la ruta donde lo guardan.\n",
        "\n",
        "**¡Reemplacen `'RUTA_AL_ARCHIVO_FASTTEXT.bin'` con la ruta real!**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUPyHWROFQNO",
        "outputId": "afe41835-3c7f-4cd9-f525-cf5021ac5010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x3-luubmFb2"
      },
      "outputs": [],
      "source": [
        "# --- ¡¡¡ MODIFICAR ESTA RUTA !!! ---\n",
        "# Ejemplo de ruta (si lo subieron a Colab o está en el mismo directorio):\n",
        "# path_to_fasttext = 'cc.es.300.bin' # Nombre común del archivo FastText para español\n",
        "# Ejemplo de ruta (si está en Google Drive montado):\n",
        "path_to_fasttext = '/content/drive/MyDrive/Clases/HABLA/004/PRA/vectors/wiki.es.bin' # Ajustar según su estructura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnkWSkrpmBT2",
        "outputId": "fd6a112f-75a3-4156-ae4a-8f70a43e31dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando vectores FastText (.bin)... (¡Esto puede tardar MUCHO tiempo y consumir RAM!)\n",
            "¡Vectores FastText cargados! Vocabulario (estimado): 985667 palabras. Dimensión: 300\n"
          ]
        }
      ],
      "source": [
        "# Intentar cargar el modelo FastText\n",
        "try:\n",
        "    print(\"Cargando vectores FastText (.bin)... (¡Esto puede tardar MUCHO tiempo y consumir RAM!)\")\n",
        "    # Usamos FastText.load_fasttext_format para cargar modelos .bin de FastText\n",
        "    fasttext_model = gensim.models.fasttext.load_facebook_model(path_to_fasttext)\n",
        "    # Los vectores están dentro del atributo .wv (como en Word2Vec cargado con KeyedVectors)\n",
        "    fasttext_vectors = fasttext_model.wv\n",
        "    print(f\"¡Vectores FastText cargados! Vocabulario (estimado): {len(fasttext_vectors.index_to_key)} palabras. Dimensión: {fasttext_vectors.vector_size}\")\n",
        "    # Nota: El tamaño del vocabulario explícito puede ser menor en .bin, pero puede generar para OOV.\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: No se encontró el archivo FastText en la ruta '{path_to_fasttext}'.\")\n",
        "    print(\"Por favor, descarga el archivo .bin pre-entrenado para español desde el sitio de FastText\")\n",
        "    print(\"y asegúrate de que la variable 'path_to_fasttext' tenga la ruta correcta.\")\n",
        "    fasttext_vectors = None\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error al cargar el modelo FastText: {e}\")\n",
        "    fasttext_vectors = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjLgGPmOmN8N"
      },
      "outputs": [],
      "source": [
        "# Intentemos recargar el modelo Word2Vec para comparar\n",
        "# --- ¡¡¡ MODIFICAR ESTA RUTA TAMBIÉN SI ES NECESARIO !!! ---\n",
        "path_to_word2vec = '/content/drive/MyDrive/Clases/HABLA/004/PRA/vectors/SBW-vectors-300-min5.bin.gz' # La ruta del martes\n",
        "word2vec_vectors = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9YzR49AmKAf",
        "outputId": "bcb186f4-081f-41ae-d445-7f25054a261b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recargando vectores Word2Vec del martes...\n",
            "Vectores Word2Vec recargados.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(\"\\nRecargando vectores Word2Vec...\")\n",
        "    word2vec_vectors = KeyedVectors.load_word2vec_format(path_to_word2vec, binary=True)\n",
        "    print(\"Vectores Word2Vec recargados.\")\n",
        "except Exception as e:\n",
        "    print(f\"No se pudo recargar Word2Vec desde '{path_to_word2vec}': {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CZhEmSemSPQ"
      },
      "source": [
        "# 6. Explorando FastText: Similitud, Analogías y ¡OOV!\n",
        "\n",
        "Si `fasttext_vectors` se cargó, podemos hacer pruebas similares a las de Word2Vec, pero prestando especial atención a las palabras OOV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUe0jsFLmTTc",
        "outputId": "50fc11c9-00a4-4de9-86c6-87d5868884ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Explorando FastText ---\n",
            "FastText - Más similares a 'rey': [('monarca', 0.7459232807159424), ('rey―', 0.6321003437042236), ('exmonarca', 0.6310685873031616), ('rey,', 0.6303314566612244), ('reina', 0.6260297298431396)]\n",
            "FastText - rey - hombre + mujer ≈ [('reina', 0.6612293720245361)]\n",
            "\n",
            "--- Probando Palabras Fuera de Vocabulario (OOV) ---\n",
            "\n",
            "Palabra OOV: 'computadorra'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('computador', 0.8841244578361511), ('computadora/ordenador', 0.8638773560523987), ('computadora,', 0.8576205372810364)]\n",
            "\n",
            "Palabra OOV: 'wasapear'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('dissapear', 0.6578547358512878), ('tapear', 0.6350954174995422), ('papear', 0.5647345781326294)]\n",
            "\n",
            "Palabra OOV: 'covid'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('covides', 0.7112458944320679), ('coview', 0.6201539635658264), ('covida', 0.5898890495300293)]\n",
            "\n",
            "Palabra OOV: 'programadorazo'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('programadora', 0.8951981663703918), ('programadoras', 0.8484038710594177), ('programadorcccp', 0.8139643669128418)]\n",
            "\n",
            "Palabra OOV: 'desayunábamos'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('dábamos', 0.7762657999992371), ('tomábamos', 0.7618230581283569), ('grabábamos', 0.7587261199951172)]\n",
            "\n",
            "Palabra OOV: 'internauta'\n",
            "  ¿En vocabulario explícito? True\n",
            "  ¡Vector obtenido! (Dim: (300,))\n",
            "  Similares (FastText): [('internautas', 0.8382586240768433), ('internaute', 0.8171950578689575), ('cibernauta', 0.7792933583259583)]\n"
          ]
        }
      ],
      "source": [
        "if fasttext_vectors:\n",
        "    # --- Pruebas estándar (similitud, analogía) ---\n",
        "    print(\"\\n--- Explorando FastText ---\")\n",
        "    # Similitud\n",
        "    try:\n",
        "        similares_rey_ft = fasttext_vectors.most_similar('rey', topn=5)\n",
        "        print(\"FastText - Más similares a 'rey':\", similares_rey_ft)\n",
        "    except KeyError:\n",
        "        print(\"FastText - Palabra 'rey' no encontrada.\") # Raro si el modelo es bueno\n",
        "\n",
        "    # Analogía\n",
        "    try:\n",
        "        analogia_reina_ft = fasttext_vectors.most_similar(positive=['rey', 'mujer'], negative=['hombre'], topn=1)\n",
        "        print(\"FastText - rey - hombre + mujer ≈\", analogia_reina_ft)\n",
        "    except KeyError as e:\n",
        "        print(f\"FastText - Error en analogía rey/reina: falta la palabra '{e.args[0]}'\")\n",
        "\n",
        "\n",
        "    # --- ¡La prueba OOV! ---\n",
        "    print(\"\\n--- Probando Palabras Fuera de Vocabulario (OOV) ---\")\n",
        "    palabras_oov = [\n",
        "        \"computadorra\", # Error tipográfico\n",
        "        \"wasapear\",     # Neologismo / Palabra coloquial\n",
        "        \"covid\",        # Palabra relativamente nueva (depende del corpus)\n",
        "        \"programadorazo\", # Palabra inventada con sufijo común\n",
        "        \"desayunábamos\", # Forma verbal menos frecuente\n",
        "        \"internauta\"    # Palabra estándar, veamos si está\n",
        "    ]\n",
        "\n",
        "    for palabra in palabras_oov:\n",
        "        print(f\"\\nPalabra OOV: '{palabra}'\")\n",
        "        # ¿Está explícitamente en el vocabulario? (Puede que sí si el corpus era reciente)\n",
        "        in_vocab = palabra in fasttext_vectors\n",
        "        print(f\"  ¿En vocabulario explícito? {in_vocab}\")\n",
        "\n",
        "        # Intentar obtener el vector (¡FastText debería poder!)\n",
        "        try:\n",
        "            vector_oov = fasttext_vectors[palabra]\n",
        "            print(f\"  ¡Vector obtenido! (Dim: {vector_oov.shape})\")\n",
        "\n",
        "            # Encontrar similares basados en el vector generado\n",
        "            similares_oov = fasttext_vectors.most_similar(palabra, topn=3)\n",
        "            print(f\"  Similares (FastText): {similares_oov}\")\n",
        "        except Exception as e:\n",
        "            # Este error no debería ocurrir con un modelo .bin cargado correctamente\n",
        "            print(f\"  Error inesperado obteniendo vector/similares para '{palabra}': {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron cargar los vectores FastText. Saltando la exploración.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvE8aywDmXio"
      },
      "source": [
        "# 7. Comparativa: Word2Vec vs FastText (foco en OOV)\n",
        "\n",
        "Ahora, si ambos modelos (`word2vec_vectors` y `fasttext_vectors`) están cargados, podemos comparar directamente su comportamiento con palabras OOV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Znmbfy7BmZgf",
        "outputId": "de254400-826c-4a24-f201-bd7e7a9defe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Comparación OOV: Word2Vec vs FastText ---\n",
            "\n",
            "--- Palabra: 'computadorra' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('computador', 0.8841244578361511), ('computadora/ordenador', 0.8638773560523987), ('computadora,', 0.8576205372810364)]\n",
            "\n",
            "--- Palabra: 'wasapear' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('dissapear', 0.6578547358512878), ('tapear', 0.6350954174995422), ('papear', 0.5647345781326294)]\n",
            "\n",
            "--- Palabra: 'covid' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('covides', 0.7112458944320679), ('coview', 0.6201539635658264), ('covida', 0.5898890495300293)]\n",
            "\n",
            "--- Palabra: 'programadorazo' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('programadora', 0.8951981663703918), ('programadoras', 0.8484038710594177), ('programadorcccp', 0.8139643669128418)]\n",
            "\n",
            "--- Palabra: 'desayunábamos' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ¡Encontrada! Vector: (300,), Similares: [('charlábamos', 0.7706557512283325), ('cenábamos', 0.7697142958641052), ('despertábamos', 0.769340455532074)]\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('dábamos', 0.7762657999992371), ('tomábamos', 0.7618230581283569), ('grabábamos', 0.7587261199951172)]\n",
            "\n",
            "--- Palabra: 'internauta' ---\n",
            "  Intentando con Word2Vec:\n",
            "    ¡Encontrada! Vector: (300,), Similares: [('cibernauta', 0.6111046671867371), ('Tianya', 0.5805689692497253), ('blogger', 0.5799006819725037)]\n",
            "  Intentando con FastText:\n",
            "    ¡Vector generado! Vector: (300,), Similares: [('internautas', 0.8382586240768433), ('internaute', 0.8171950578689575), ('cibernauta', 0.7792933583259583)]\n"
          ]
        }
      ],
      "source": [
        "# --- Comparación directa OOV ---\n",
        "if word2vec_vectors and fasttext_vectors:\n",
        "    print(\"\\n--- Comparación OOV: Word2Vec vs FastText ---\")\n",
        "\n",
        "    # Reutilizamos la lista de palabras OOV\n",
        "    palabras_oov = [\n",
        "        \"computadorra\", \"wasapear\", \"covid\", \"programadorazo\", \"desayunábamos\", \"internauta\"\n",
        "    ]\n",
        "\n",
        "    for palabra in palabras_oov:\n",
        "        print(f\"\\n--- Palabra: '{palabra}' ---\")\n",
        "\n",
        "        # Intentar con Word2Vec\n",
        "        print(\"  Intentando con Word2Vec:\")\n",
        "        try:\n",
        "            vector_w2v = word2vec_vectors[palabra]\n",
        "            similares_w2v = word2vec_vectors.most_similar(palabra, topn=3)\n",
        "            print(f\"    ¡Encontrada! Vector: {vector_w2v.shape}, Similares: {similares_w2v}\")\n",
        "        except KeyError:\n",
        "            print(\"    ERROR: Palabra no encontrada en el vocabulario Word2Vec (KeyError).\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR inesperado con Word2Vec: {e}\")\n",
        "\n",
        "        # Intentar con FastText\n",
        "        print(\"  Intentando con FastText:\")\n",
        "        try:\n",
        "            vector_ft = fasttext_vectors[palabra]\n",
        "            similares_ft = fasttext_vectors.most_similar(palabra, topn=3)\n",
        "            print(f\"    ¡Vector generado! Vector: {vector_ft.shape}, Similares: {similares_ft}\")\n",
        "        except Exception as e:\n",
        "            # No debería fallar por KeyError, pero podría haber otro error\n",
        "            print(f\"    ERROR inesperado con FastText: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo se pudieron cargar ambos modelos (Word2Vec y FastText) para comparar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU57C8I8mcdZ"
      },
      "source": [
        "**Conclusión de la Comparativa:**\n",
        "\n",
        "Deberían observar que para la mayoría (o todas) las palabras OOV, Word2Vec lanza un `KeyError`, mientras que FastText logra generar un vector y encontrar palabras similares (que pueden o no ser muy coherentes, dependiendo de qué tan \"rara\" sea la palabra OOV, pero *intenta* algo basado en sus partes).\n",
        "\n",
        "**¿Cuándo usar FastText?**\n",
        "*   Cuando trabajas con texto ruidoso (redes sociales, OCR) con typos.\n",
        "*   Cuando tratas con lenguajes morfológicamente ricos (español, alemán, etc.).\n",
        "*   Cuando esperas encontrar palabras nuevas o raras en tus datos de aplicación.\n",
        "*   **En general, para español, FastText suele ser una opción más robusta y recomendable que Word2Vec o GloVe.**\n",
        "\n",
        "**Desventaja:** Los modelos `.bin` de FastText son más grandes en disco y consumen más RAM porque almacenan información de los n-gramas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bPYX6xPmdU0"
      },
      "source": [
        "# 8. Micro-Laboratorio (Ejercicio Práctico)\n",
        "\n",
        "**Consigna:** (Asumiendo que `fasttext_vectors` y `word2vec_vectors` están cargados)\n",
        "\n",
        "1.  **Comparación de Resultados:**\n",
        "    *   Elegir 3 palabras que **sí** estén en ambos vocabularios (ej: 'gato', 'correr', 'inteligencia').\n",
        "    *   Para cada palabra, obtener las 5 más similares usando `word2vec_vectors.most_similar()` y `fasttext_vectors.most_similar()`.\n",
        "    *   Comparar las listas de similares. ¿Son idénticas? ¿Muy parecidas? ¿Diferentes? ¿Cuál les parece \"mejor\" o más coherente? Anotar observaciones.\n",
        "\n",
        "2.  **Test OOV Exhaustivo:**\n",
        "    *   Crear una lista propia de 10 palabras OOV. Incluyan:\n",
        "        *   Errores tipográficos comunes (ej: \"hobmre\", \"qeu\", \"dicimbre\").\n",
        "        *   Diminutivos/Aumentativos (ej: \"perrito\", \"casita\", \"libraco\").\n",
        "        *   Formas verbales conjugadas (ej: \"habíamos comido\", \"cantasteis\").\n",
        "        *   Palabras inventadas pero plausibles (ej: \"tecnoestrés\", \"computofilia\").\n",
        "    *   Para **cada** palabra OOV de su lista:\n",
        "        *   Verificar si da `KeyError` en `word2vec_vectors`.\n",
        "        *   Obtener las 3 palabras más similares usando `fasttext_vectors`. Anotar los resultados. ¿Los similares que da FastText tienen algún sentido basado en las partes de la palabra OOV?\n",
        "\n",
        "3.  **Discusión:**\n",
        "    *   ¿En qué tipo de aplicación real (ej: un chatbot de atención al cliente, un sistema de recomendación de noticias, un corrector ortográfico) creen que la capacidad OOV de FastText marcaría una diferencia significativa respecto a usar Word2Vec? ¿Por qué?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26vXMK1amgTI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0BNPC_9minX"
      },
      "source": [
        "# 9. Brainstorming: Evaluación y Detección de Sesgos\n",
        "\n",
        "Hemos visto diferentes formas de crear embeddings (Word2Vec, GloVe, FastText) y cómo usarlos. Pero, ¿cómo sabemos si un conjunto de embeddings es \"bueno\"? ¿Y cómo abordamos el problema de los sesgos que mencionamos en clases anteriores?\n",
        "\n",
        "**Pregunta:** **¿Cómo podemos evaluar la calidad de los word embeddings y detectar posibles sesgos?**\n",
        "\n",
        "*   **Evaluación Intrínseca:**\n",
        "    *   Medir qué tan bien funcionan los embeddings en tareas específicas relacionadas con las palabras mismas, **sin** una aplicación final.\n",
        "    *   Ejemplos:\n",
        "        *   **Similitud de Palabras:** Comparar la similitud coseno entre pares de palabras según los embeddings, con juicios de similitud dados por humanos (datasets estándar como WordSim-353, SimLex-999, adaptados o creados para español).\n",
        "        *   **Tareas de Analogía:** Ver qué tan bien resuelven analogías (`rey - hombre + mujer = ?`). Hay datasets estándar de analogías (Google Analogies, BATS).\n",
        "*   **Evaluación Extrínseca:**\n",
        "    *   Usar los embeddings como **características de entrada (features)** para un modelo de PLN más complejo que resuelve una tarea final (downstream task).\n",
        "    *   Ejemplos: Clasificación de sentimientos, reconocimiento de entidades nombradas, clasificación de temas.\n",
        "    *   Medir si usar estos embeddings mejora el rendimiento (precisión, F1-score, etc.) del sistema final comparado con no usarlos o usar otros embeddings. **Esta suele ser la evaluación más relevante en la práctica.**\n",
        "*   **Detección de Sesgos:**\n",
        "    *   **Analogías Específicas:** Crear analogías diseñadas para revelar sesgos sociales (género-profesión, raza-sentimiento, etc.). `programador - hombre + mujer = ?`.\n",
        "    *   **Pruebas de Asociación Implícita (como WEAT - Word Embedding Association Test):** Miden matemáticamente qué tan asociados están ciertos grupos de palabras (ej: nombres masculinos vs femeninos) con ciertos atributos (ej: carreras científicas vs artísticas, adjetivos positivos vs negativos) dentro del espacio de embeddings.\n",
        "    *   **Visualización:** A veces, proyectar grupos específicos (hombres/mujeres, profesiones) a 2D puede revelar agrupaciones o direcciones sesgadas.\n",
        "    *   **Auditoría de Vecinos Cercanos:** Ver los `most_similar` para palabras sensibles.\n",
        "\n",
        "**Una vez detectado el sesgo, ¿qué hacemos?** ¿Existen técnicas para mitigarlo (\"debiasing\")? ¿Es mejor buscar/crear corpus menos sesgados? ¿O simplemente ser transparentes sobre los sesgos del modelo?\n",
        "\n",
        "**(Discusión en grupo)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GUÍA DE ESTUDIO - FASTTEXT Y GLOVE\n",
        "\n",
        "## Preguntas y Respuestas Clave\n",
        "\n",
        "### **Innovaciones Arquitectónicas**\n",
        "\n",
        "**P: ¿Cuál es la innovación principal de FastText sobre Word2Vec?**  \n",
        "R: FastText representa palabras como suma de n-gramas de caracteres, no como unidades indivisibles. Esto permite generar vectores para palabras nunca vistas (OOV).\n",
        "\n",
        "**P: ¿Cómo funciona FastText con palabras desconocidas?**  \n",
        "R: Descompone la palabra en n-gramas (\"amarillo\" → \"<am\", \"ama\", \"mar\", etc.) y suma los vectores de los n-gramas conocidos para generar un vector para la palabra completa.\n",
        "\n",
        "**P: ¿En qué se diferencia GloVe de Word2Vec conceptualmente?**  \n",
        "R: Word2Vec predice contextos locales. GloVe usa estadísticas globales de co-ocurrencia de todo el corpus para factorizar una matriz de co-ocurrencias.\n",
        "\n",
        "### **Ventajas Específicas**\n",
        "\n",
        "**P: ¿Por qué FastText es especialmente bueno para español?**  \n",
        "R: El español es morfológicamente rico (muchas conjugaciones, derivaciones). FastText captura estas relaciones morfológicas a través de n-gramas compartidos.\n",
        "\n",
        "**P: ¿Qué ventajas tiene FastText para texto ruidoso?**  \n",
        "R: Es robusto a errores tipográficos porque \"computadorra\" comparte muchos n-gramas con \"computadora\", generando un vector similar.\n",
        "\n",
        "**P: ¿Cuándo elegir GloVe sobre Word2Vec?**  \n",
        "R: Cuando necesitas mejor aprovechamiento de estadísticas corpus-wide o tienes computational budget limitado para múltiples pasadas sobre el corpus.\n",
        "\n",
        "### **Implementación Práctica**\n",
        "\n",
        "**P: ¿Por qué es importante descargar archivos .bin de FastText?**  \n",
        "R: El formato .bin incluye información de n-gramas necesaria para manejar OOV. El formato .vec solo tiene vectores de palabras conocidas.\n",
        "\n",
        "**P: ¿Qué indica que una palabra no está en vocabulario explícito pero FastText puede procesarla?**  \n",
        "R: `palabra in fasttext_vectors` devuelve False, pero `fasttext_vectors[palabra]` aún funciona generando un vector dinámicamente.\n",
        "\n",
        "### **Evaluación y Comparación**\n",
        "\n",
        "**P: ¿Cómo evaluar si FastText produce mejores embeddings que Word2Vec?**  \n",
        "R: Evaluación intrínseca (tareas de similitud, analogías) y extrínseca (performance en tareas downstream como clasificación de texto).\n",
        "\n",
        "**P: ¿Qué trade-offs tiene FastText?**  \n",
        "R: Pro: maneja OOV, mejor morfología. Contra: mayor tamaño de modelo, mayor uso de RAM, menor interpretabilidad de n-gramas.\n",
        "\n",
        "### **Casos de Uso**\n",
        "\n",
        "**P: ¿En qué aplicaciones sería crítica la capacidad OOV de FastText?**  \n",
        "R: Redes sociales (jerga, typos), análisis de texto histórico (ortografía variable), corpus especializados (terminología técnica), lenguas con poca documentación.\n",
        "\n",
        "**P: ¿Cómo manejaría lenguaje inclusivo con FastText?**  \n",
        "R: \"Estudiantx\" se procesaría usando n-gramas de \"estudiante\" + sufijo, capturando la relación semántica automáticamente.\n",
        "\n",
        "## Puntos Clave para Recordar\n",
        "\n",
        "1. **FastText = Word2Vec + n-gramas de caracteres**\n",
        "2. **OOV no es problema** con FastText (vs KeyError en Word2Vec)\n",
        "3. **GloVe = estadísticas globales** vs predicción local\n",
        "4. **Para español: FastText > Word2Vec** por morfología rica\n",
        "5. **Trade-off: capacidad vs complejidad computacional**\n",
        "6. **Archivos .bin cruciales** para funcionalidad OOV\n",
        "\n",
        "## Consideraciones Importantes\n",
        "\n",
        "- FastText puede generar vectores para cualquier string, incluso nonsense\n",
        "- La calidad del vector OOV depende de qué tan informativos sean los n-gramas\n",
        "- GloVe puede ser difícil de encontrar pre-entrenado para español\n",
        "- Modelos más grandes requieren más RAM y tiempo de carga\n",
        "- No todos los n-gramas son igualmente informativos\n",
        "\n",
        "## Conexión con Próxima Clase\n",
        "\n",
        "Ahora conocen múltiples arquitecturas de embeddings. La próxima clase integrará todos estos métodos en **pipelines prácticos**, comparando su rendimiento en tareas reales y decidiendo cuándo usar cada uno.\n",
        "\n",
        "---\n",
        "*Consejo: Testa FastText con argentinismos o errores de tipeo comunes en WhatsApp. ¿Genera vectores sensatos para \"kjjjj\" o \"messirve\"?*"
      ],
      "metadata": {
        "id": "qhM--vWsgAbX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}