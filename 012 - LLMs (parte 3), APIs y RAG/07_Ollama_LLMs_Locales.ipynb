{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama: Modelos de Lenguaje en Tu Computadora\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "\n",
    "En este cuaderno vas a aprender:\n",
    "- Qué es Ollama y por qué es útil ejecutar modelos localmente\n",
    "- Cómo configurar y usar modelos de lenguaje sin APIs pagas\n",
    "- Técnicas de prompting con system roles\n",
    "- Extracción de datos estructurados (JSON) desde texto\n",
    "- Procesamiento automatizado de archivos\n",
    "- Creación de interfaces de usuario con Gradio\n",
    "\n",
    "## ¿Qué es Ollama?\n",
    "\n",
    "**Ollama** es una herramienta que te permite ejecutar modelos de lenguaje grandes (LLMs) en tu propia computadora, sin necesidad de conexión a internet ni APIs pagas.\n",
    "\n",
    "### Ventajas de usar Ollama\n",
    "\n",
    "1. **Privacidad**: Tus datos no salen de tu computadora\n",
    "2. **Costo $0**: No pagás por tokens ni llamadas a APIs\n",
    "3. **Sin límites**: Podés hacer todas las consultas que quieras\n",
    "4. **Offline**: Funciona sin conexión a internet (después de descargar el modelo)\n",
    "5. **Rápido**: Respuestas inmediatas si tenés buena GPU\n",
    "\n",
    "### Casos de uso ideales\n",
    "\n",
    "- Procesamiento de datos sensibles (historiales médicos, información personal)\n",
    "- Prototipos y experimentos con muchas iteraciones\n",
    "- Aplicaciones que requieren procesamiento en tiempo real\n",
    "- Entornos sin acceso a internet\n",
    "- Educación y aprendizaje\n",
    "\n",
    "## Prerequisitos\n",
    "\n",
    "Para usar este cuaderno necesitás:\n",
    "1. **Ollama instalado** en tu sistema: https://ollama.ai/download\n",
    "2. **Un modelo descargado**. En este cuaderno usamos Gemma 3\n",
    "3. **Requisitos de hardware**:\n",
    "   - Mínimo: 8GB RAM, CPU moderna\n",
    "   - Recomendado: 16GB RAM + GPU NVIDIA\n",
    "\n",
    "### Instalación rápida de Ollama\n",
    "\n",
    "**En Linux/Mac**:\n",
    "```bash\n",
    "curl https://ollama.ai/install.sh | sh\n",
    "```\n",
    "\n",
    "**En Windows**:\n",
    "Descargá el instalador desde https://ollama.ai/download\n",
    "\n",
    "**Descargar modelo**:\n",
    "```bash\n",
    "# Modelo pequeño y rápido (recomendado para empezar)\n",
    "ollama pull gemma:2b\n",
    "\n",
    "# Modelo más potente (requiere más RAM)\n",
    "ollama pull gemma:7b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración del Entorno\n",
    "\n",
    "Instalamos las dependencias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías\n",
    "!pip install -q ollama pandas gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from ollama import chat\n",
    "\n",
    "# Configuración del modelo\n",
    "# Si descargaste otro modelo, cambiá este nombre\n",
    "MODEL_NAME = \"gemma:2b\"  # Modelo pequeño y rápido\n",
    "# MODEL_NAME = \"gemma:7b\"  # Modelo más potente\n",
    "# MODEL_NAME = \"llama2\"  # Alternativa popular\n",
    "\n",
    "print(f\"Configuración completada\")\n",
    "print(f\"Modelo a usar: {MODEL_NAME}\")\n",
    "print(\"\\nVerificá que Ollama esté corriendo: ollama list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera Consulta: Análisis de Sentimiento\n",
    "\n",
    "Empecemos con algo simple: analizar el sentimiento de un comentario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comentario a analizar\n",
    "comentario = \"\"\"\n",
    "Fui al cine a ver la nueva de Marvel y la verdad que una decepción total.\n",
    "La trama no tiene sentido, los efectos especiales están bien pero ya aburren.\n",
    "Encima me cobraron una fortuna por la entrada y el pochoclo.\n",
    "No la recomiendo para nada.\n",
    "\"\"\"\n",
    "\n",
    "# Estructura del mensaje\n",
    "mensaje = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Analiza el sentimiento de este comentario: {comentario}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Enviando consulta al modelo local...\")\n",
    "print(f\"Comentario: {comentario}\")\n",
    "\n",
    "# Llamada a Ollama\n",
    "response = chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=mensaje\n",
    ")\n",
    "\n",
    "# Resultado\n",
    "respuesta = response.message.content\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS DEL MODELO:\")\n",
    "print(\"=\"*80)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Role: Definiendo el Comportamiento\n",
    "\n",
    "El **system role** es un mensaje especial que define cómo debe comportarse el asistente. Es como darle instrucciones permanentes sobre:\n",
    "- Su personalidad y tono\n",
    "- Su expertise\n",
    "- Reglas que debe seguir\n",
    "- Formato de respuestas\n",
    "\n",
    "### Comparación: Sin System Role vs Con System Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = \"¿Qué opinás de la última película de Tarantino?\"\n",
    "\n",
    "# SIN SYSTEM ROLE\n",
    "print(\"=\"*80)\n",
    "print(\"1. SIN SYSTEM ROLE (comportamiento genérico)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "respuesta_generica = chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": consulta}]\n",
    ")\n",
    "print(respuesta_generica.message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. CON SYSTEM ROLE (crítico de cine especializado)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CON SYSTEM ROLE\n",
    "system_prompt_cine = \"\"\"\n",
    "Sos un crítico de cine argentino especializado en cine independiente y de autor.\n",
    "Tenés 15 años de experiencia escribiendo para revistas culturales.\n",
    "Tu estilo es:\n",
    "- Análisis profundo de narrativa y dirección\n",
    "- Referencias a cine clásico y contemporáneo\n",
    "- Tono profesional pero accesible\n",
    "- Comparaciones con otras obras del mismo director\n",
    "Respondés en español rioplatense.\n",
    "\"\"\"\n",
    "\n",
    "respuesta_especializada = chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt_cine},\n",
    "        {\"role\": \"user\", \"content\": consulta}\n",
    "    ]\n",
    ")\n",
    "print(respuesta_especializada.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de Datos Estructurados (JSON)\n",
    "\n",
    "Una de las aplicaciones más útiles de los LLMs es convertir texto no estructurado en datos estructurados (JSON).\n",
    "\n",
    "### Caso de uso: Analizar reviews de productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_from_text(text: str):\n",
    "    \"\"\"\n",
    "    Extrae JSON de texto que puede contener explicaciones adicionales.\n",
    "    Los modelos a veces agregan texto antes/después del JSON.\n",
    "    \"\"\"\n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        candidate = text[start:end+1]\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except:\n",
    "            return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review de un producto\n",
    "review_producto = \"\"\"\n",
    "REVIEW: Auriculares Bluetooth XYZ-500\n",
    "Comprador: Juan P. - Buenos Aires\n",
    "Fecha: 15/03/2024\n",
    "\n",
    "Los compré hace un mes y la verdad que re contentos. El sonido es excelente,\n",
    "los graves se sienten mucho. La batería dura fácil 8 horas, perfecto para el laburo.\n",
    "Lo único malo es que son medio pesados después de usarlos mucho rato, me cansan las orejas.\n",
    "El precio me pareció razonable, 35 lucas, considerando la calidad que tienen.\n",
    "La conexión Bluetooth es instantánea y nunca se corta.\n",
    "Recomendación: 8/10, muy buenos para el precio.\n",
    "\"\"\"\n",
    "\n",
    "# System prompt para análisis estructurado\n",
    "system_prompt_analisis = \"\"\"\n",
    "Sos un sistema de análisis de reviews que extrae información estructurada.\n",
    "Tu tarea es convertir reviews en formato JSON con los siguientes campos:\n",
    "- producto: nombre del producto\n",
    "- comprador: nombre del comprador\n",
    "- fecha: fecha de la review\n",
    "- calificacion: número del 1 al 10\n",
    "- aspectos_positivos: lista de aspectos positivos mencionados\n",
    "- aspectos_negativos: lista de aspectos negativos mencionados\n",
    "- precio_mencionado: precio si lo menciona, null si no\n",
    "- sentimiento_general: \"positivo\", \"negativo\" o \"neutral\"\n",
    "\n",
    "IMPORTANTE: Devolvé SOLO el JSON, sin explicaciones adicionales.\n",
    "\"\"\"\n",
    "\n",
    "# Prompt de extracción\n",
    "prompt_extraccion = f\"\"\"\n",
    "Extrae la información de esta review en formato JSON:\n",
    "\n",
    "{review_producto}\n",
    "\n",
    "Devolvé SOLO el JSON válido.\n",
    "\"\"\"\n",
    "\n",
    "# Procesamiento\n",
    "print(\"Extrayendo datos estructurados de la review...\")\n",
    "\n",
    "response = chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt_analisis},\n",
    "        {\"role\": \"user\", \"content\": prompt_extraccion}\n",
    "    ]\n",
    ")\n",
    "\n",
    "texto_respuesta = response.message.content\n",
    "print(\"\\nRespuesta del modelo:\")\n",
    "print(texto_respuesta)\n",
    "\n",
    "# Extraer JSON\n",
    "datos_estructurados = extract_json_from_text(texto_respuesta)\n",
    "\n",
    "if datos_estructurados:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATOS ESTRUCTURADOS EXTRAÍDOS:\")\n",
    "    print(\"=\"*60)\n",
    "    print(json.dumps(datos_estructurados, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    df = pd.DataFrame([datos_estructurados])\n",
    "    print(\"\\nDataFrame generado:\")\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"\\nNo se pudo extraer JSON válido de la respuesta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de Archivos: De Texto a Datos Estructurados\n",
    "\n",
    "Una aplicación práctica: procesar archivos de texto y convertirlos en CSVs estructurados.\n",
    "\n",
    "### Ejemplo: Análisis de comentarios de redes sociales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un archivo de ejemplo con comentarios\n",
    "comentarios_ejemplo = \"\"\"\n",
    "COMENTARIOS DEL VIDEO: \"RECORRIDA POR PALERMO SOHO 2024\"\n",
    "Canal: Buenos Aires Travel\n",
    "Fecha: 10/03/2024\n",
    "Reproducciones: 45,230\n",
    "\n",
    "---\n",
    "Usuario: @MartinBA\n",
    "Excelente video! Me encanta Palermo, tiene los mejores bares y restaurantes de la ciudad.\n",
    "La zona está re linda ahora, mucho mejor que hace unos años.\n",
    "\n",
    "Usuario: @SofiTraveler\n",
    "Muy bueno el recorrido, pero la verdad que Palermo está carísimo.\n",
    "Un café sale una fortuna. Prefiero San Telmo que tiene más onda y es más barato.\n",
    "\n",
    "Usuario: @LucasARG\n",
    "¡Qué nostalgia! Viví en Palermo 5 años y extraño un montón.\n",
    "Los domingos en la plaza Serrano eran lo mejor. Gracias por el video.\n",
    "\n",
    "Usuario: @TuristaChile\n",
    "Voy en mayo a Buenos Aires, este video me sirve un montón para planificar.\n",
    "¿Alguien sabe si los bares de la zona tienen wifi? Necesito trabajar remoto.\n",
    "\n",
    "Usuario: @PalermoLocal\n",
    "Vivo acá hace 10 años. El barrio cambió mucho, se llenó de turistas.\n",
    "Ya no es lo que era, perdió un poco de autenticidad. Pero igual es lindo.\n",
    "\"\"\"\n",
    "\n",
    "# Guardamos el archivo\n",
    "archivo_comentarios = \"comentarios_palermo.txt\"\n",
    "with open(archivo_comentarios, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(comentarios_ejemplo)\n",
    "\n",
    "print(f\"Archivo de ejemplo creado: {archivo_comentarios}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt para análisis de comentarios\n",
    "system_prompt_comentarios = \"\"\"\n",
    "Sos un analista de redes sociales que procesa comentarios de YouTube.\n",
    "Extraes información estructurada de cada comentario:\n",
    "- Usuario\n",
    "- Sentimiento (positivo/negativo/neutral/pregunta)\n",
    "- Tema principal\n",
    "- Menciona precio (sí/no)\n",
    "- Intención (opinión/pregunta/nostalgia/recomendación)\n",
    "\n",
    "Generás un CSV con columnas: usuario,sentimiento,tema,menciona_precio,intencion,texto_resumen\n",
    "\"\"\"\n",
    "\n",
    "# Leer el archivo\n",
    "with open(archivo_comentarios, \"r\", encoding=\"utf-8\") as f:\n",
    "    contenido = f.read()\n",
    "\n",
    "# Prompt para generar CSV\n",
    "prompt_csv = f\"\"\"\n",
    "Analiza estos comentarios y genera un CSV con la estructura:\n",
    "usuario,sentimiento,tema,menciona_precio,intencion,texto_resumen\n",
    "\n",
    "COMENTARIOS:\n",
    "{contenido}\n",
    "\n",
    "Devolvé SOLO el CSV, sin explicaciones. Primera fila: nombres de columnas.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Procesando comentarios...\")\n",
    "\n",
    "response = chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt_comentarios},\n",
    "        {\"role\": \"user\", \"content\": prompt_csv}\n",
    "    ]\n",
    ")\n",
    "\n",
    "csv_resultado = response.message.content\n",
    "\n",
    "print(\"\\nCSV generado:\")\n",
    "print(\"=\"*80)\n",
    "print(csv_resultado)\n",
    "\n",
    "# Intentar cargar como DataFrame\n",
    "try:\n",
    "    from io import StringIO\n",
    "    df_comentarios = pd.read_csv(StringIO(csv_resultado))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANÁLISIS DE COMENTARIOS:\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_comentarios)\n",
    "    \n",
    "    # Estadísticas\n",
    "    print(\"\\nESTADÍSTICAS:\")\n",
    "    print(f\"Total de comentarios: {len(df_comentarios)}\")\n",
    "    if 'sentimiento' in df_comentarios.columns:\n",
    "        print(\"\\nDistribución de sentimientos:\")\n",
    "        print(df_comentarios['sentimiento'].value_counts())\n",
    "    \n",
    "    # Guardar CSV\n",
    "    df_comentarios.to_csv('analisis_comentarios.csv', index=False)\n",
    "    print(\"\\nDatos guardados en: analisis_comentarios.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nNo se pudo parsear como CSV: {e}\")\n",
    "    print(\"El modelo quizás agregó texto extra antes/después del CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz de Usuario con Gradio\n",
    "\n",
    "Crear una interfaz web simple para que usuarios no técnicos puedan usar el modelo.\n",
    "\n",
    "### Aplicación: Analizador de Sentimiento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def analizar_sentimiento(texto, detalle):\n",
    "    \"\"\"\n",
    "    Analiza el sentimiento de un texto usando el modelo local.\n",
    "    \n",
    "    Parámetros:\n",
    "    - texto: El texto a analizar\n",
    "    - detalle: Nivel de detalle del análisis\n",
    "    \"\"\"\n",
    "    if not texto.strip():\n",
    "        return \"Por favor ingresá un texto para analizar.\"\n",
    "    \n",
    "    # Configurar el prompt según el nivel de detalle\n",
    "    if detalle == \"Simple\":\n",
    "        prompt = f\"\"\"Analiza el sentimiento de este texto y respondé en una palabra:\n",
    "        Positivo, Negativo o Neutral.\n",
    "        \n",
    "        Texto: {texto}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Analiza detalladamente el sentimiento de este texto.\n",
    "        Incluye:\n",
    "        - Sentimiento general (positivo/negativo/neutral)\n",
    "        - Intensidad (baja/media/alta)\n",
    "        - Emociones detectadas\n",
    "        - Tono del texto\n",
    "        \n",
    "        Texto: {texto}\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = chat(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\\n\\nVerificá que Ollama esté corriendo.\"\n",
    "\n",
    "# Crear la interfaz\n",
    "with gr.Blocks(title=\"Analizador de Sentimiento Local\") as demo:\n",
    "    gr.Markdown(\"\"\"\n",
    "    # Analizador de Sentimiento con IA Local\n",
    "    ### Procesamiento 100% privado en tu computadora\n",
    "    \n",
    "    Esta herramienta usa Ollama para analizar el sentimiento de cualquier texto\n",
    "    sin enviar datos a servidores externos.\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            texto_input = gr.Textbox(\n",
    "                label=\"Texto a analizar\",\n",
    "                placeholder=\"Pegá acá el texto que querés analizar...\",\n",
    "                lines=5\n",
    "            )\n",
    "            \n",
    "            detalle_input = gr.Radio(\n",
    "                choices=[\"Simple\", \"Detallado\"],\n",
    "                value=\"Simple\",\n",
    "                label=\"Nivel de detalle\"\n",
    "            )\n",
    "            \n",
    "            btn_analizar = gr.Button(\n",
    "                \"Analizar Sentimiento\",\n",
    "                variant=\"primary\"\n",
    "            )\n",
    "        \n",
    "        with gr.Column():\n",
    "            resultado_output = gr.Textbox(\n",
    "                label=\"Resultado del análisis\",\n",
    "                lines=10\n",
    "            )\n",
    "    \n",
    "    # Ejemplos predefinidos\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"La película estuvo increíble, la mejor que vi este año!\", \"Simple\"],\n",
    "            [\"No me gustó para nada, perdí mi tiempo y mi plata.\", \"Detallado\"],\n",
    "            [\"Estuvo bien, nada del otro mundo pero entretenida.\", \"Detallado\"],\n",
    "        ],\n",
    "        inputs=[texto_input, detalle_input]\n",
    "    )\n",
    "    \n",
    "    # Conectar botón con función\n",
    "    btn_analizar.click(\n",
    "        fn=analizar_sentimiento,\n",
    "        inputs=[texto_input, detalle_input],\n",
    "        outputs=resultado_output\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"\"\"\n",
    "    ---\n",
    "    **Características de Privacidad:**\n",
    "    - Todo el procesamiento se hace localmente\n",
    "    - Ningún dato se envía a servidores externos\n",
    "    - Funciona sin conexión a internet\n",
    "    - Costo $0 en APIs\n",
    "    \"\"\")\n",
    "\n",
    "# Lanzar la interfaz\n",
    "print(\"Iniciando interfaz web...\")\n",
    "print(f\"Modelo: {MODEL_NAME}\")\n",
    "print(\"La interfaz se abrirá en tu navegador\")\n",
    "\n",
    "demo.launch(\n",
    "    server_name=\"127.0.0.1\",\n",
    "    server_port=7860,\n",
    "    share=False  # No compartir públicamente\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen y Conceptos Clave\n",
    "\n",
    "### Lo que aprendimos:\n",
    "\n",
    "1. **Ollama**: Plataforma para ejecutar LLMs localmente\n",
    "   - Privacidad total\n",
    "   - Sin costos de API\n",
    "   - Funciona offline\n",
    "\n",
    "2. **System Role**: Define el comportamiento del asistente\n",
    "   - Personalidad y tono\n",
    "   - Reglas y restricciones\n",
    "   - Formato de respuestas\n",
    "\n",
    "3. **Extracción de JSON**: Convertir texto no estructurado en datos\n",
    "   - Siempre pedir \"SOLO JSON\"\n",
    "   - Usar función de extracción robusta\n",
    "   - Validar resultados\n",
    "\n",
    "4. **Procesamiento de archivos**: Automatización de análisis\n",
    "   - Leer archivos de texto\n",
    "   - Generar CSVs estructurados\n",
    "   - Crear DataFrames para análisis\n",
    "\n",
    "5. **Interfaces con Gradio**: Democratizar el acceso\n",
    "   - UI simple para usuarios no técnicos\n",
    "   - Ejemplos predefinidos\n",
    "   - Validación de inputs\n",
    "\n",
    "### Comparación: Ollama vs APIs Cloud\n",
    "\n",
    "| Característica | Ollama Local | APIs Cloud (OpenAI/Gemini) |\n",
    "|----------------|--------------|----------------------------|\n",
    "| Privacidad | Total | Datos en servidores externos |\n",
    "| Costo | $0 (después de HW) | Por token/request |\n",
    "| Velocidad | Depende de tu HW | Generalmente rápido |\n",
    "| Calidad | Modelos más pequeños | Modelos muy potentes |\n",
    "| Internet | No necesario | Obligatorio |\n",
    "| Setup | Requiere instalación | Instant\n\no con API key |\n",
    "\n",
    "### Cuándo usar cada opción:\n",
    "\n",
    "**Usar Ollama cuando**:\n",
    "- Necesitás privacidad absoluta\n",
    "- Hacés muchas consultas (costo)\n",
    "- Trabajás con datos sensibles\n",
    "- No tenés conexión a internet confiable\n",
    "- Querés aprender sin gastar\n",
    "\n",
    "**Usar APIs Cloud cuando**:\n",
    "- Necesitás la mejor calidad posible\n",
    "- No tenés hardware potente\n",
    "- Querés empezar rápido sin configuración\n",
    "- El volumen de consultas es bajo\n",
    "\n",
    "### Próximos pasos:\n",
    "\n",
    "Ahora que conocés tanto APIs cloud como ejecución local, podés:\n",
    "- Combinar ambas estrategias (prototipos con Ollama, producción con cloud)\n",
    "- Implementar sistemas RAG con embeddings locales\n",
    "- Crear aplicaciones que protegen la privacidad del usuario\n",
    "- Experimentar sin límites de costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glosario\n",
    "\n",
    "**Ollama**: Plataforma open-source para ejecutar modelos de lenguaje localmente en tu computadora.\n",
    "\n",
    "**System Role**: Mensaje especial que define el comportamiento global del asistente de IA.\n",
    "\n",
    "**Local LLM**: Modelo de lenguaje grande que se ejecuta en tu propia computadora, sin necesidad de internet.\n",
    "\n",
    "**Inferencia**: Proceso de usar un modelo entrenado para generar respuestas (diferente de entrenamiento).\n",
    "\n",
    "**Gradio**: Librería de Python para crear interfaces web simples para modelos de ML.\n",
    "\n",
    "**GGUF**: Formato de archivo optimizado para almacenar y ejecutar LLMs de forma eficiente.\n",
    "\n",
    "**Quantization**: Técnica para reducir el tamaño de modelos (ej: 4-bit) manteniendo calidad aceptable.\n",
    "\n",
    "**Context Window**: Cantidad máxima de tokens (texto) que el modelo puede procesar de una vez.\n",
    "\n",
    "**Prompt Engineering**: Arte de escribir instrucciones efectivas para obtener mejores respuestas de LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas Frecuentes\n",
    "\n",
    "**P: ¿Cuánto espacio en disco ocupan los modelos?**\n",
    "\n",
    "R: Depende del tamaño. Gemma 2B ocupa ~1.4GB, Gemma 7B ~4.8GB, Llama2 7B ~3.8GB. Los modelos más grandes (70B+) pueden ocupar 40GB o más.\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Necesito una GPU para usar Ollama?**\n",
    "\n",
    "R: No es obligatorio, pero MUY recomendado. Con GPU (NVIDIA) las respuestas son casi instantáneas. Con solo CPU funciona pero puede tardar varios segundos por respuesta.\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Puedo usar modelos diferentes para diferentes tareas?**\n",
    "\n",
    "R: Sí. Podés tener varios modelos instalados y cambiar según la tarea. Modelos pequeños para análisis rápidos, modelos grandes para tareas complejas.\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Cómo actualizo un modelo?**\n",
    "\n",
    "R: Simplemente ejecutá `ollama pull nombre-modelo` de nuevo. Ollama solo descarga las diferencias si hay una versión nueva.\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Puedo entrenar o fine-tunear modelos con Ollama?**\n",
    "\n",
    "R: Ollama es solo para inferencia (usar modelos). Para entrenar o fine-tunear necesitás otras herramientas (Hugging Face Transformers, LLaMA.cpp, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Es legal usar estos modelos comercialmente?**\n",
    "\n",
    "R: Depende de la licencia del modelo. Llama 2 tiene restricciones comerciales si tenés muchos usuarios. Gemma es más permisiva. Siempre revisá la licencia específica del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias y Recursos\n",
    "\n",
    "**Ollama**:\n",
    "- Sitio oficial: https://ollama.ai\n",
    "- Documentación: https://github.com/ollama/ollama\n",
    "- Librería modelos: https://ollama.ai/library\n",
    "\n",
    "**Modelos Recomendados**:\n",
    "- Gemma (Google): Buena calidad, open-source, varias versiones\n",
    "- Llama 2 (Meta): Popular, bien documentado\n",
    "- Mistral: Excelente calidad/tamaño, muy eficiente\n",
    "- Phi-2 (Microsoft): Modelo pequeño y sorprendentemente capaz\n",
    "\n",
    "**Gradio**:\n",
    "- Documentación: https://gradio.app/docs\n",
    "- Ejemplos: https://gradio.app/demos\n",
    "\n",
    "**Comunidad**:\n",
    "- r/LocalLLaMA (Reddit): Comunidad activa de LLMs locales\n",
    "- Ollama Discord: Soporte y discusiones\n",
    "\n",
    "**Papers Relevantes**:\n",
    "- \"Llama 2: Open Foundation and Fine-Tuned Chat Models\"\n",
    "- \"Gemma: Open Models Based on Gemini Research and Technology\"\n",
    "- \"GGML: Large Language Models for Everyone\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}