{"cells":[{"cell_type":"markdown","metadata":{"id":"header"},"source":["<table style=\"width:100%\">\n","<tr>\n","<td style=\"vertical-align:middle; text-align:left;\">\n","<font size=\"2\">\n","Material adaptado para la materia <strong>Introducción al Procesamiento del Habla y LLMs</strong><br>\n","Basado en el libro <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> de <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n","<br>Repositorio original: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n","</font>\n","</td>\n","</tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"title"},"source":["# Intro a LLMs\n","\n","##Trabajando con Datos de Texto"]},{"cell_type":"markdown","metadata":{"id":"setup-colab"},"source":["## Configuración para Google Colab\n","\n","Si estás ejecutando este cuaderno en Google Colab, primero necesitamos descargar el archivo de texto:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"colab-setup","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761086684685,"user_tz":180,"elapsed":21408,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"ab80c6f4-d341-4417-9b0c-800cdb2341e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Archivo encontrado en: /content/drive/MyDrive/Clases/000 - 2DO Cuatrimestre/HABLA/009 - Intro a LLMs/PRA/001-texto/001-texto/tlon-uqbar-orbis-tertius.txt\n"]}],"source":["# Descargar el texto de Borges si no existe\n","import os\n","import requests\n","from google.colab import drive\n","\n","# Montar Google Drive\n","drive.mount('/content/drive')\n","\n","# Ruta del archivo en Google Drive (asegúrate de que esta ruta sea correcta)\n","# Por ejemplo, si el archivo está en la carpeta \"Mi Drive\"\n","ruta_archivo_drive = \"/content/drive/MyDrive/Clases/000 - 2DO Cuatrimestre/HABLA/009 - Intro a LLMs/PRA/001-texto/001-texto/tlon-uqbar-orbis-tertius.txt\"\n","\n","# Verificar si el archivo existe en Drive antes de intentar leerlo\n","if not os.path.exists(ruta_archivo_drive):\n","    print(f\"Error: El archivo no se encontró en la ruta especificada: {ruta_archivo_drive}\")\n","    print(\"Asegúrate de que el archivo 'tlon-uqbar-orbis-tertius.txt' esté en tu Google Drive\")\n","    print(\"y que la ruta en el código sea correcta.\")\n","else:\n","    print(f\"Archivo encontrado en: {ruta_archivo_drive}\")"]},{"cell_type":"markdown","metadata":{"id":"packages"},"source":["## Instalación de paquetes\n","\n","Paquetes que se utilizan en este cuaderno:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"install-packages","executionInfo":{"status":"ok","timestamp":1761086722592,"user_tz":180,"elapsed":7208,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["# Instalar dependencias en Colab\n","!pip install -q torch tiktoken"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"check-versions","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761081702607,"user_tz":180,"elapsed":56,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"deb8feb6-faeb-40ab-b97c-dc801f70e13b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Versión de torch: 2.8.0+cu126\n","Versión de tiktoken: 0.12.0\n"]}],"source":["from importlib.metadata import version\n","\n","print(\"Versión de torch:\", version(\"torch\"))\n","print(\"Versión de tiktoken:\", version(\"tiktoken\"))"]},{"cell_type":"markdown","metadata":{"id":"intro"},"source":["- Este capítulo cubre la preparación y el muestreo de datos para preparar los datos de entrada para un LLM\n","- Trabajaremos con el cuento \"Tlön, Uqbar, Orbis Tertius\" de Jorge Luis Borges"]},{"cell_type":"markdown","metadata":{"id":"diagram-1"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"section-2-1"},"source":["## 2.1 Entendiendo los word embeddings"]},{"cell_type":"markdown","metadata":{"id":"section-2-1-note"},"source":["- No hay código en esta sección"]},{"cell_type":"markdown","metadata":{"id":"embeddings-intro"},"source":["- Existen muchas formas de embeddings; en este libro nos enfocamos en embeddings de texto"]},{"cell_type":"markdown","metadata":{"id":"diagram-2"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"embeddings-dimensions"},"source":["- Los LLMs trabajan con embeddings en espacios de alta dimensionalidad (es decir, miles de dimensiones)\n","- Como no podemos visualizar espacios de tan alta dimensionalidad (los humanos pensamos en 1, 2 o 3 dimensiones), la figura siguiente ilustra un espacio de embeddings de 2 dimensiones"]},{"cell_type":"markdown","metadata":{"id":"diagram-3"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp\" width=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"section-2-2"},"source":["## 2.2 Tokenizando texto"]},{"cell_type":"markdown","metadata":{"id":"tokenization-intro"},"source":["- En esta sección, tokenizamos texto, lo que significa dividir el texto en unidades más pequeñas, como palabras individuales y caracteres de puntuación"]},{"cell_type":"markdown","metadata":{"id":"diagram-4"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp\" width=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"load-text"},"source":["- Cargamos el texto con el que queremos trabajar\n","- [\"Tlön, Uqbar, Orbis Tertius\" de Jorge Luis Borges](https://ciudadseva.com/texto/tlon-uqbar-orbis-tertius/)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"read-text","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087325737,"user_tz":180,"elapsed":376,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"727bddaf-4c76-48a6-f77b-d4caedb7797a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Número total de caracteres: 32516\n","Debo a la conjunción de un espejo y de una enciclopedia el descubrimiento de Uqbar. El espejo inquietaba el fondo de un corredor en una quinta de la calle Gaona, en Ramos Mejía; la enciclopedia falazm\n"]}],"source":["# Utilizamos la ruta del archivo en Google Drive que verificamos anteriormente\n","ruta_archivo_drive = \"/content/drive/MyDrive/Clases/000 - 2DO Cuatrimestre/HABLA/009 - Intro a LLMs/PRA/001-texto/001-texto/tlon-uqbar-orbis-tertius.txt\"\n","\n","with open(ruta_archivo_drive, \"r\", encoding=\"utf-8\") as archivo:\n","    texto_crudo = archivo.read()\n","\n","print(\"Número total de caracteres:\", len(texto_crudo))\n","print(texto_crudo[:200])"]},{"cell_type":"markdown","metadata":{"id":"tokenization-goal"},"source":["- El objetivo es tokenizar y embeber este texto para un LLM\n","- Desarrollemos un tokenizador simple basado en un texto de ejemplo que después podamos aplicar al texto completo\n","- La siguiente expresión regular dividirá el texto en espacios en blanco"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"regex-basic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087349587,"user_tz":180,"elapsed":11,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"bde2aaa9-a9d8-40c6-9158-65d923076508"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Hola,', ' ', 'mundo.', ' ', 'Esto,', ' ', 'es', ' ', 'una', ' ', 'prueba.']\n"]}],"source":["import re\n","\n","texto = \"Hola, mundo. Esto, es una prueba.\"\n","resultado = re.split(r'(\\s)', texto)\n","\n","print(resultado)"]},{"cell_type":"markdown","metadata":{"id":"regex-punctuation"},"source":["- No solo queremos dividir por espacios en blanco sino también por comas y puntos, así que modifiquemos la expresión regular para hacer eso también"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"regex-comma-period","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087392621,"user_tz":180,"elapsed":69,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"26fdc3fd-e5f4-4725-d79e-c791ad6657fa"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Hola', ',', '', ' ', 'mundo', '.', '', ' ', 'Esto', ',', '', ' ', 'es', ' ', 'una', ' ', 'prueba', '.', '']\n"]}],"source":["resultado = re.split(r'([,.]|\\s)', texto)\n","\n","print(resultado)"]},{"cell_type":"markdown","metadata":{"id":"remove-empty"},"source":["- Como podemos ver, esto crea strings vacías, eliminémoslas"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"filter-empty","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087398081,"user_tz":180,"elapsed":11,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"7d54fc7e-70c4-4a91-a678-61749ec6ca5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Hola', ',', 'mundo', '.', 'Esto', ',', 'es', 'una', 'prueba', '.']\n"]}],"source":["# Eliminamos los espacios en blanco de cada ítem y luego filtramos cualquier string vacía.\n","resultado = [item for item in resultado if item.strip()]\n","print(resultado)"]},{"cell_type":"markdown","metadata":{"id":"more-punctuation"},"source":["- Esto se ve bastante bien, pero también manejemos otros tipos de puntuación, como puntos, signos de interrogación, y demás"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"regex-full","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087419416,"user_tz":180,"elapsed":44,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"dd9387c6-9468-4bb0-d562-2079a8847202"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Hola', ',', 'mundo', '.', '¿Es', 'esto', '--', 'una', 'prueba', '?']\n"]}],"source":["texto = \"Hola, mundo. ¿Es esto-- una prueba?\"\n","\n","resultado = re.split(r'([,.:;?_!\"()\\']|--|\\s)', texto)\n","resultado = [item.strip() for item in resultado if item.strip()]\n","print(resultado)"]},{"cell_type":"markdown","metadata":{"id":"apply-to-full"},"source":["- Esto está bastante bien, y ahora estamos listos para aplicar esta tokenización al texto completo"]},{"cell_type":"markdown","metadata":{"id":"diagram-5"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp\" width=\"350px\">"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tokenize-full","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087443986,"user_tz":180,"elapsed":60,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"bd25634e-1fd0-4a4e-e25d-60c9fd852e7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Debo', 'a', 'la', 'conjunción', 'de', 'un', 'espejo', 'y', 'de', 'una', 'enciclopedia', 'el', 'descubrimiento', 'de', 'Uqbar', '.', 'El', 'espejo', 'inquietaba', 'el', 'fondo', 'de', 'un', 'corredor', 'en', 'una', 'quinta', 'de', 'la', 'calle']\n"]}],"source":["preprocesado = re.split(r'([,.:;?_!\"()\\']|--|\\s)', texto_crudo)\n","preprocesado = [item.strip() for item in preprocesado if item.strip()]\n","print(preprocesado[:30])"]},{"cell_type":"markdown","metadata":{"id":"count-tokens"},"source":["- Calculemos el número total de tokens"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"print-count","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087463414,"user_tz":180,"elapsed":11,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"1d8cc63b-3a58-4097-9edc-3dc8ce1311a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["6109\n"]}],"source":["print(len(preprocesado))"]},{"cell_type":"markdown","metadata":{"id":"section-2-3"},"source":["## 2.3 Convirtiendo tokens en IDs de tokens"]},{"cell_type":"markdown","metadata":{"id":"token-ids-intro"},"source":["- A continuación, convertimos los tokens de texto en IDs de tokens que podemos procesar mediante capas de embedding más adelante"]},{"cell_type":"markdown","metadata":{"id":"diagram-6"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"build-vocab"},"source":["- A partir de estos tokens, ahora podemos construir un vocabulario que consiste en todos los tokens únicos"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"create-vocab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087495969,"user_tz":180,"elapsed":10,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"d3a01e88-32e9-4ee5-de12-dc8de76dce86"},"outputs":[{"output_type":"stream","name":"stdout","text":["2104\n"]}],"source":["todas_palabras = sorted(set(preprocesado))\n","tamaño_vocab = len(todas_palabras)\n","\n","print(tamaño_vocab)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"vocab-dict","executionInfo":{"status":"ok","timestamp":1761087502809,"user_tz":180,"elapsed":27,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["vocabulario = {token: entero for entero, token in enumerate(todas_palabras)}"]},{"cell_type":"markdown","metadata":{"id":"show-vocab"},"source":["- A continuación se muestran las primeras 50 entradas en este vocabulario:"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"print-vocab","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087505304,"user_tz":180,"elapsed":20,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"4e36d032-8ac0-4f4b-a26a-63281865ddb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["('(', 0)\n","(')', 1)\n","(',', 2)\n","('-Dios', 3)\n","('-Jorasán', 4)\n","('-Silas', 5)\n","('-a', 6)\n","('-con', 7)\n","('-congénitamente-', 8)\n","('-de', 9)\n","('-el', 10)\n","('-interrogaron-', 11)\n","('-la', 12)\n","('-los', 13)\n","('-meses', 14)\n","('-más', 15)\n","('-ni', 16)\n","('-o', 17)\n","('-que', 18)\n","('-siquiera', 19)\n","('-tal', 20)\n","('-traduzco', 21)\n","('-y', 22)\n","('.', 23)\n","('1', 24)\n","('10', 25)\n","('1001', 26)\n","('1641', 27)\n","('1824', 28)\n","('1828', 29)\n","('1874-figura', 30)\n","('1902', 31)\n","('1914', 32)\n","('1917', 33)\n","('1937', 34)\n","('1940', 35)\n","('1941', 36)\n","('1942', 37)\n","('1944', 38)\n","('1947', 39)\n","('2', 40)\n","('4', 41)\n","('5', 42)\n","('917', 43)\n","('918', 44)\n","('920', 45)\n","('921', 46)\n","(':', 47)\n","(';', 48)\n","('>', 49)\n","('?', 50)\n"]}],"source":["for i, item in enumerate(vocabulario.items()):\n","    print(item)\n","    if i >= 50:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"tokenizer-illustration"},"source":["- A continuación, ilustramos la tokenización de un texto de ejemplo corto usando un vocabulario pequeño:"]},{"cell_type":"markdown","metadata":{"id":"diagram-7"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"tokenizer-class"},"source":["- Juntando todo ahora en una clase tokenizadora"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"simple-tokenizer-v1","executionInfo":{"status":"ok","timestamp":1761087578053,"user_tz":180,"elapsed":21,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["class TokenizadorSimpleV1:\n","    def __init__(self, vocabulario):\n","        self.str_a_int = vocabulario\n","        self.int_a_str = {i: s for s, i in vocabulario.items()}\n","\n","    def codificar(self, texto):\n","        preprocesado = re.split(r'([,.:;?_!\"()\\']|--|\\s)', texto)\n","\n","        preprocesado = [\n","            item.strip() for item in preprocesado if item.strip()\n","        ]\n","        ids = [self.str_a_int[s] for s in preprocesado]\n","        return ids\n","\n","    def decodificar(self, ids):\n","        texto = \" \".join([self.int_a_str[i] for i in ids])\n","        # Reemplazamos espacios antes de las puntuaciones especificadas\n","        texto = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', texto)\n","        return texto"]},{"cell_type":"markdown","metadata":{"id":"encode-decode"},"source":["- La función `codificar` convierte texto en IDs de tokens\n","- La función `decodificar` convierte IDs de tokens de vuelta a texto"]},{"cell_type":"markdown","metadata":{"id":"diagram-8"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"use-tokenizer"},"source":["- Podemos usar el tokenizador para codificar (es decir, tokenizar) textos en enteros\n","- Estos enteros pueden entonces ser embebidos (más adelante) como entrada para el LLM"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"test-tokenizer","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087599235,"user_tz":180,"elapsed":31,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"f6372951-f0bb-44d9-e333-c5052cd4c659"},"outputs":[{"output_type":"stream","name":"stdout","text":["[112, 326, 1238, 622, 703, 1992, 919, 2064, 703, 1993, 871, 849, 747, 703, 307, 23]\n"]}],"source":["tokenizador = TokenizadorSimpleV1(vocabulario)\n","\n","texto = \"\"\"Debo a la conjunción de un espejo y de una enciclopedia\n","           el descubrimiento de Uqbar.\"\"\"\n","ids = tokenizador.codificar(texto)\n","print(ids)"]},{"cell_type":"markdown","metadata":{"id":"cell-48a"},"source":["### Veamos qué texto representan estos IDs:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-48b","executionInfo":{"status":"ok","timestamp":1761087635541,"user_tz":180,"elapsed":12,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"653c6582-78c9-4069-c184-35e4ac72e7ac"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["IDs: [112, 326, 1238, 622, 703, 1992, 919, 2064, 703, 1993, 871, 849, 747, 703, 307, 23]\n","\n","Texto decodificado:\n","Debo a la conjunción de un espejo y de una enciclopedia el descubrimiento de Uqbar.\n"]}],"source":["print(\"IDs:\", ids)\n","print(\"\\nTexto decodificado:\")\n","print(tokenizador.decodificar(ids))"]},{"cell_type":"markdown","metadata":{"id":"decode-back"},"source":["- Podemos decodificar los enteros de vuelta a texto"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"test-decode","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1761087686786,"user_tz":180,"elapsed":67,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"e228b8b8-21f0-4fb5-d2ab-62a9053990dc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Debo a la conjunción de un espejo y de una enciclopedia el descubrimiento de Uqbar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["tokenizador.decodificar(ids)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"roundtrip","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1761087688613,"user_tz":180,"elapsed":22,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"9af101b2-ab14-4a75-e451-743d4561593b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Debo a la conjunción de un espejo y de una enciclopedia el descubrimiento de Uqbar.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}],"source":["tokenizador.decodificar(tokenizador.codificar(texto))"]},{"cell_type":"code","source":["tokenizador.codificar(texto)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3084AAwAbAB","executionInfo":{"status":"ok","timestamp":1761087698888,"user_tz":180,"elapsed":19,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"e5a58f7a-5e85-4b7c-aca6-a0db6a5a3f47"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[112,\n"," 326,\n"," 1238,\n"," 622,\n"," 703,\n"," 1992,\n"," 919,\n"," 2064,\n"," 703,\n"," 1993,\n"," 871,\n"," 849,\n"," 747,\n"," 703,\n"," 307,\n"," 23]"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"section-2-4"},"source":["## 2.4 Agregando tokens de contexto especiales"]},{"cell_type":"markdown","metadata":{"id":"special-tokens-intro"},"source":["- Es útil agregar algunos tokens \"especiales\" para palabras desconocidas y para denotar el final de un texto"]},{"cell_type":"markdown","metadata":{"id":"diagram-9"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"special-tokens-explanation"},"source":["- Algunos tokenizadores usan tokens especiales para ayudar al LLM con contexto adicional\n","- Algunos de estos tokens especiales son:\n","  - `[BOS]` (beginning of sequence) marca el comienzo del texto\n","  - `[EOS]` (end of sequence) marca dónde termina el texto (esto generalmente se usa para concatenar múltiples textos no relacionados, por ejemplo, dos artículos diferentes de Wikipedia o dos libros diferentes, etc.)\n","  - `[PAD]` (padding) si entrenamos LLMs con un tamaño de batch mayor que 1 (podemos incluir múltiples textos con diferentes longitudes; con el token de padding rellenamos los textos más cortos a la longitud más larga para que todos los textos tengan la misma longitud)\n","  - `[UNK]` para representar palabras que no están incluidas en el vocabulario\n","\n","- Notemos que GPT-2 no necesita ninguno de los tokens mencionados arriba pero solo usa un token `<|endoftext|>` para reducir la complejidad\n","- El `<|endoftext|>` es análogo al token `[EOS]` mencionado arriba\n","- GPT también usa el `<|endoftext|>` para padding (ya que típicamente usamos una máscara cuando entrenamos con entradas en batch, no atenderíamos a los tokens de padding de todos modos, así que no importa cuáles sean estos tokens)\n","- GPT-2 no usa un token `<UNK>` para palabras fuera del vocabulario; en su lugar, GPT-2 usa un tokenizador de codificación por pares de bytes (BPE), que descompone palabras en unidades de subpalabras, lo cual discutiremos en una sección posterior"]},{"cell_type":"markdown","metadata":{"id":"endoftext-usage"},"source":["- Usamos los tokens `<|endoftext|>` entre dos fuentes independientes de texto:"]},{"cell_type":"markdown","metadata":{"id":"diagram-10"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"test-unknown"},"source":["- Veamos qué sucede si tokenizamos el siguiente texto:"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"unknown-word-error","executionInfo":{"status":"ok","timestamp":1761087735659,"user_tz":180,"elapsed":8,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["tokenizador = TokenizadorSimpleV1(vocabulario)\n","\n","texto = \"Hola, ¿te gusta el mate? ¿Es esto-- una prueba?\"\n","\n","# Esta celda producirá un error porque \"Hola\" no está en el vocabulario\n","# tokenizador.codificar(texto)"]},{"cell_type":"markdown","metadata":{"id":"add-special-tokens"},"source":["- Lo anterior produce un error porque la palabra \"Hola\" no está contenida en el vocabulario\n","- Para lidiar con tales casos, podemos agregar tokens especiales como `\"<|unk|>\"` al vocabulario para representar palabras desconocidas\n","- Ya que estamos extendiendo el vocabulario, agreguemos otro token llamado `\"<|endoftext|>\"` que se usa en el entrenamiento de GPT-2 para denotar el final de un texto (y también se usa entre textos concatenados, como si nuestro conjunto de datos de entrenamiento consiste en múltiples artículos, libros, etc.)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"extend-vocab","executionInfo":{"status":"ok","timestamp":1761087747742,"user_tz":180,"elapsed":51,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["todos_tokens = sorted(list(set(preprocesado)))\n","todos_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n","\n","vocabulario = {token: entero for entero, token in enumerate(todos_tokens)}"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"new-vocab-size","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087810831,"user_tz":180,"elapsed":13,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"693a07b4-0625-440f-8985-b26b11b4c221"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2106"]},"metadata":{},"execution_count":23}],"source":["len(vocabulario.items())"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"show-special-tokens","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087812838,"user_tz":180,"elapsed":6,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"b86c07fb-d27a-43f6-d5d4-b8941f257fa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["('“razonamiento', 2101)\n","('“todos', 2102)\n","('”', 2103)\n","('<|endoftext|>', 2104)\n","('<|unk|>', 2105)\n"]}],"source":["for i, item in enumerate(list(vocabulario.items())[-5:]):\n","    print(item)"]},{"cell_type":"markdown","metadata":{"id":"update-tokenizer"},"source":["- También necesitamos ajustar el tokenizador en consecuencia para que sepa cuándo y cómo usar el nuevo token `<unk>`"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"simple-tokenizer-v2","executionInfo":{"status":"ok","timestamp":1761087831359,"user_tz":180,"elapsed":9,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["class TokenizadorSimpleV2:\n","    def __init__(self, vocabulario):\n","        self.str_a_int = vocabulario\n","        self.int_a_str = {i: s for s, i in vocabulario.items()}\n","\n","    def codificar(self, texto):\n","        preprocesado = re.split(r'([,.:;?_!\"()\\']|--|\\s)', texto)\n","        preprocesado = [item.strip() for item in preprocesado if item.strip()]\n","        preprocesado = [\n","            item if item in self.str_a_int\n","            else \"<|unk|>\" for item in preprocesado\n","        ]\n","\n","        ids = [self.str_a_int[s] for s in preprocesado]\n","        return ids\n","\n","    def decodificar(self, ids):\n","        texto = \" \".join([self.int_a_str[i] for i in ids])\n","        # Reemplazamos espacios antes de las puntuaciones especificadas\n","        texto = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', texto)\n","        return texto"]},{"cell_type":"markdown","metadata":{"id":"test-v2"},"source":["Intentemos tokenizar texto con el tokenizador modificado:"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"test-tokenizer-v2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087835803,"user_tz":180,"elapsed":73,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"056faccd-cf1d-4e69-db82-f789a68ebef5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hola, ¿te gusta el mate? <|endoftext|> En las terrazas iluminadas por el sol del palacio.\n"]}],"source":["tokenizador = TokenizadorSimpleV2(vocabulario)\n","\n","texto1 = \"Hola, ¿te gusta el mate?\"\n","texto2 = \"En las terrazas iluminadas por el sol del palacio.\"\n","\n","texto = \" <|endoftext|> \".join((texto1, texto2))\n","\n","print(texto)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"encode-test","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761087855871,"user_tz":180,"elapsed":20,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"c0fd20a9-dccf-4ba1-da03-042db02c38fd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2105,\n"," 2,\n"," 2105,\n"," 2105,\n"," 849,\n"," 2105,\n"," 50,\n"," 2104,\n"," 125,\n"," 1242,\n"," 2105,\n"," 2105,\n"," 1596,\n"," 849,\n"," 1867,\n"," 718,\n"," 2105,\n"," 23]"]},"metadata":{},"execution_count":27}],"source":["tokenizador.codificar(texto)"]},{"cell_type":"markdown","metadata":{"id":"cell-68a"},"source":["### Veamos la correspondencia entre IDs y texto:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-68b","executionInfo":{"status":"ok","timestamp":1761087870603,"user_tz":180,"elapsed":42,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"bad82fb7-952f-474e-c713-adcf9a3d109c"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["IDs: [2105, 2, 2105, 2105, 849, 2105, 50, 2104, 125, 1242, 2105, 2105, 1596, 849, 1867, 718, 2105, 23]\n","\n","Texto original:\n","Hola, ¿te gusta el mate? <|endoftext|> En las terrazas iluminadas por el sol del palacio.\n","\n","Texto decodificado:\n","<|unk|>, <|unk|> <|unk|> el <|unk|>? <|endoftext|> En las <|unk|> <|unk|> por el sol del <|unk|>.\n"]}],"source":["ids_codificados = tokenizador.codificar(texto)\n","print(\"IDs:\", ids_codificados)\n","print(\"\\nTexto original:\")\n","print(texto)\n","print(\"\\nTexto decodificado:\")\n","print(tokenizador.decodificar(ids_codificados))"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"decode-test","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1761087916551,"user_tz":180,"elapsed":57,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"fee4d80b-1be2-4d3e-cde6-987fe31607bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|unk|>, <|unk|> <|unk|> el <|unk|>? <|endoftext|> En las <|unk|> <|unk|> por el sol del <|unk|>.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}],"source":["tokenizador.decodificar(tokenizador.codificar(texto))"]},{"cell_type":"markdown","metadata":{"id":"section-2-5"},"source":["## 2.5 Codificación por pares de bytes (BytePair encoding)"]},{"cell_type":"markdown","metadata":{"id":"bpe-intro"},"source":["- GPT-2 usó codificación por pares de bytes (BPE) como su tokenizador\n","- Permite al modelo descomponer palabras que no están en su vocabulario predefinido en unidades de subpalabras más pequeñas o incluso caracteres individuales, permitiéndole manejar palabras fuera del vocabulario\n","- Por ejemplo, si el vocabulario de GPT-2 no tiene la palabra \"palabradesconocida\", podría tokenizarla como [\"palabra\", \"desco\", \"nocida\"] o alguna otra descomposición de subpalabras, dependiendo de sus fusiones BPE entrenadas\n","- El tokenizador BPE original se puede encontrar aquí: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n","- En este capítulo, usamos el tokenizador BPE de la biblioteca de código abierto [tiktoken](https://github.com/openai/tiktoken) de OpenAI, que implementa sus algoritmos principales en Rust para mejorar el rendimiento computacional"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"import-tiktoken","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088037730,"user_tz":180,"elapsed":50,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"2bfdece0-dbb0-40ae-eb4f-2c492565396d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Versión de tiktoken: 0.12.0\n"]}],"source":["import importlib\n","import tiktoken\n","\n","print(\"Versión de tiktoken:\", importlib.metadata.version(\"tiktoken\"))"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"get-gpt2-tokenizer","executionInfo":{"status":"ok","timestamp":1761088040856,"user_tz":180,"elapsed":1327,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["tokenizador = tiktoken.get_encoding(\"gpt2\")"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"test-bpe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088044806,"user_tz":180,"elapsed":55,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"2342f2ef-9ac7-4e72-e1ae-cbde97674e1a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[39, 5708, 11, 1587, 123, 660, 35253, 64, 1288, 16133, 30, 220, 50256, 2039, 39990, 1059, 3247, 292, 4229, 7230, 38768, 16964, 1288, 1540, 390, 435, 7145, 43, 35652, 5960, 1102, 420, 17305, 13]\n"]}],"source":["texto = (\n","    \"Hola, ¿te gusta el mate? <|endoftext|> En las terrazas iluminadas\"\n","     \" por el sol de algunLugarDesconocido.\"\n",")\n","\n","enteros = tokenizador.encode(texto, allowed_special={\"<|endoftext|>\"})\n","\n","print(enteros)"]},{"cell_type":"markdown","metadata":{"id":"cell-74a"},"source":["### Veamos la correspondencia token por token:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-74b","executionInfo":{"status":"ok","timestamp":1761088052320,"user_tz":180,"elapsed":43,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"5ff3f96f-7f18-4c19-af60-2a6611809799"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["IDs: [39, 5708, 11, 1587, 123, 660, 35253, 64, 1288, 16133, 30, 220, 50256, 2039, 39990, 1059, 3247, 292, 4229, 7230, 38768, 16964, 1288, 1540, 390, 435, 7145, 43, 35652, 5960, 1102, 420, 17305, 13]\n","\n","Correspondencia token por token:\n","     39 --> 'H'\n","   5708 --> 'ola'\n","     11 --> ','\n","   1587 --> ' �'\n","    123 --> '�'\n","    660 --> 'te'\n","  35253 --> ' gust'\n","     64 --> 'a'\n","   1288 --> ' el'\n","  16133 --> ' mate'\n","     30 --> '?'\n","    220 --> ' '\n","  50256 --> '<|endoftext|>'\n","   2039 --> ' En'\n","  39990 --> ' las'\n","   1059 --> ' ter'\n","   3247 --> 'raz'\n","    292 --> 'as'\n","   4229 --> ' il'\n","   7230 --> 'umin'\n","  38768 --> 'adas'\n","  16964 --> ' por'\n","   1288 --> ' el'\n","   1540 --> ' sol'\n","    390 --> ' de'\n","    435 --> ' al'\n","   7145 --> 'gun'\n","     43 --> 'L'\n","  35652 --> 'ugar'\n","   5960 --> 'Des'\n","   1102 --> 'con'\n","    420 --> 'oc'\n","  17305 --> 'ido'\n","     13 --> '.'\n"]}],"source":["print(\"IDs:\", enteros)\n","print(\"\\nCorrespondencia token por token:\")\n","for i, id_token in enumerate(enteros):\n","    print(f\"  {id_token:5d} --> {tokenizador.decode([id_token])!r}\")"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"decode-bpe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088189389,"user_tz":180,"elapsed":14,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"0f1d24a2-8cd2-444a-955d-5d32984ac1cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hola, ¿te gusta el mate? <|endoftext|> En las terrazas iluminadas por el sol de algunLugarDesconocido.\n"]}],"source":["cadenas = tokenizador.decode(enteros)\n","\n","print(cadenas)"]},{"cell_type":"markdown","metadata":{"id":"bpe-subwords"},"source":["- Los tokenizadores BPE descomponen palabras desconocidas en subpalabras y caracteres individuales:"]},{"cell_type":"markdown","metadata":{"id":"diagram-11"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"section-2-6"},"source":["## 2.6 Muestreo de datos con una ventana deslizante"]},{"cell_type":"markdown","metadata":{"id":"sliding-window-intro"},"source":["- Entrenamos LLMs para generar una palabra a la vez, así que queremos preparar los datos de entrenamiento en consecuencia donde la siguiente palabra en una secuencia representa el objetivo a predecir:"]},{"cell_type":"markdown","metadata":{"id":"diagram-12"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"encode-full-text","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088284122,"user_tz":180,"elapsed":55,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"b3e80dc8-6dc0-4e6e-9218-1ba7b3f3b272"},"outputs":[{"output_type":"stream","name":"stdout","text":["11530\n"]}],"source":["with open(ruta_archivo_drive, \"r\", encoding=\"utf-8\") as archivo:\n","    texto_crudo = archivo.read()\n","\n","texto_codificado = tokenizador.encode(texto_crudo)\n","print(len(texto_codificado))"]},{"cell_type":"markdown","metadata":{"id":"inputs-targets"},"source":["- Para cada fragmento de texto, queremos las entradas y los objetivos\n","- Como queremos que el modelo prediga la siguiente palabra, los objetivos son las entradas desplazadas una posición a la derecha"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"sample-encoded","executionInfo":{"status":"ok","timestamp":1761088300154,"user_tz":180,"elapsed":26,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["muestra_codificada = texto_codificado[50:]"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"context-size","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088396687,"user_tz":180,"elapsed":18,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"dda91ef3-0304-42e0-fedb-6c95515bd432"},"outputs":[{"output_type":"stream","name":"stdout","text":["x: [28533, 64, 390, 8591]\n","y:      [64, 390, 8591, 2386]\n"]}],"source":["tamaño_contexto = 4\n","\n","x = muestra_codificada[:tamaño_contexto]\n","y = muestra_codificada[1:tamaño_contexto+1]\n","\n","print(f\"x: {x}\")\n","print(f\"y:      {y}\")"]},{"cell_type":"markdown","metadata":{"id":"cell-84a"},"source":["### Veamos qué texto representan:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-84b","executionInfo":{"status":"ok","timestamp":1761088398438,"user_tz":180,"elapsed":18,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"6a86237e-ecdd-4f46-d278-ed5e37dc8944"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada (x):\n","  IDs: [28533, 64, 390, 8591]\n","  Texto:  quinta de la\n","\n","Objetivo (y):\n","  IDs: [64, 390, 8591, 2386]\n","  Texto: a de la cal\n"]}],"source":["print(\"Entrada (x):\")\n","print(\"  IDs:\", x)\n","print(\"  Texto:\", tokenizador.decode(x))\n","print(\"\\nObjetivo (y):\")\n","print(\"  IDs:\", y)\n","print(\"  Texto:\", tokenizador.decode(y))"]},{"cell_type":"markdown","metadata":{"id":"prediction-pattern"},"source":["- Una por una, la predicción se vería de la siguiente manera:"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"show-predictions","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088448852,"user_tz":180,"elapsed":58,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"60864c69-4c33-40b1-d103-9e8aa75eb787"},"outputs":[{"output_type":"stream","name":"stdout","text":["[28533] ----> 64\n","[28533, 64] ----> 390\n","[28533, 64, 390] ----> 8591\n","[28533, 64, 390, 8591] ----> 2386\n"]}],"source":["for i in range(1, tamaño_contexto+1):\n","    contexto = muestra_codificada[:i]\n","    deseado = muestra_codificada[i]\n","\n","    print(contexto, \"---->\", deseado)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"show-predictions-decoded","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088459699,"user_tz":180,"elapsed":10,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"dcdccc59-9199-4726-deb8-cec45d3367b8"},"outputs":[{"output_type":"stream","name":"stdout","text":[" quint ----> a\n"," quinta ---->  de\n"," quinta de ---->  la\n"," quinta de la ---->  cal\n"]}],"source":["for i in range(1, tamaño_contexto+1):\n","    contexto = muestra_codificada[:i]\n","    deseado = muestra_codificada[i]\n","\n","    print(tokenizador.decode(contexto), \"---->\", tokenizador.decode([deseado]))"]},{"cell_type":"markdown","metadata":{"id":"dataloader-intro"},"source":["- Nos encargaremos de la predicción de la siguiente palabra en un capítulo posterior después de cubrir el mecanismo de atención\n","- Por ahora, implementamos un simple data loader que itera sobre el dataset de entrada y devuelve las entradas y los objetivos desplazados por uno"]},{"cell_type":"markdown","metadata":{"id":"pytorch-import"},"source":["- Instalamos e importamos PyTorch (ver Apéndice A para consejos de instalación)"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"check-torch","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088466206,"user_tz":180,"elapsed":4277,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"6a6decd3-15d2-47bd-f506-b95ad6ced27b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Versión de PyTorch: 2.8.0+cu126\n"]}],"source":["import torch\n","print(\"Versión de PyTorch:\", torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"diagram-13"},"source":["- Usamos un enfoque de ventana deslizante, cambiando la posición en +1:\n","\n","<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"create-dataset"},"source":["- Creamos dataset y dataloader que extraen fragmentos del dataset de texto de entrada"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"gpt-dataset","executionInfo":{"status":"ok","timestamp":1761088489507,"user_tz":180,"elapsed":26,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","\n","class DatasetGPTV1(Dataset):\n","    def __init__(self, txt, tokenizador, longitud_maxima, paso):\n","        self.ids_entrada = []\n","        self.ids_objetivo = []\n","\n","        # Tokenizamos el texto completo\n","        ids_tokens = tokenizador.encode(txt, allowed_special={\"<|endoftext|>\"})\n","        assert len(ids_tokens) > longitud_maxima, \"El número de entradas tokenizadas debe ser al menos igual a longitud_maxima+1\"\n","\n","        # Usamos una ventana deslizante para fragmentar el libro en secuencias superpuestas de longitud_maxima\n","        for i in range(0, len(ids_tokens) - longitud_maxima, paso):\n","            fragmento_entrada = ids_tokens[i:i + longitud_maxima]\n","            fragmento_objetivo = ids_tokens[i + 1: i + longitud_maxima + 1]\n","            self.ids_entrada.append(torch.tensor(fragmento_entrada))\n","            self.ids_objetivo.append(torch.tensor(fragmento_objetivo))\n","\n","    def __len__(self):\n","        return len(self.ids_entrada)\n","\n","    def __getitem__(self, idx):\n","        return self.ids_entrada[idx], self.ids_objetivo[idx]"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"create-dataloader-function","executionInfo":{"status":"ok","timestamp":1761088494241,"user_tz":180,"elapsed":42,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["def crear_dataloader_v1(txt, tamaño_batch=4, longitud_maxima=256,\n","                         paso=128, mezclar=True, descartar_ultimo=True,\n","                         num_trabajadores=0):\n","\n","    # Inicializamos el tokenizador\n","    tokenizador = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Creamos el dataset\n","    dataset = DatasetGPTV1(txt, tokenizador, longitud_maxima, paso)\n","\n","    # Creamos el dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=tamaño_batch,\n","        shuffle=mezclar,\n","        drop_last=descartar_ultimo,\n","        num_workers=num_trabajadores\n","    )\n","\n","    return dataloader"]},{"cell_type":"markdown","metadata":{"id":"test-dataloader"},"source":["- Probemos el dataloader con un tamaño de batch de 1 para un LLM con un tamaño de contexto de 4:"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"read-text-again","executionInfo":{"status":"ok","timestamp":1761088502312,"user_tz":180,"elapsed":29,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["with open(ruta_archivo_drive, \"r\", encoding=\"utf-8\") as archivo:\n","    texto_crudo = archivo.read()"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"test-batch-1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088503927,"user_tz":180,"elapsed":182,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"39eb14cb-af3f-44bd-b347-e40cadf8c668"},"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[16587,    78,   257,  8591]]), tensor([[   78,   257,  8591, 11644]])]\n"]}],"source":["dataloader = crear_dataloader_v1(\n","    texto_crudo, tamaño_batch=1, longitud_maxima=4, paso=1, mezclar=False\n",")\n","\n","iterador_datos = iter(dataloader)\n","primer_batch = next(iterador_datos)\n","print(primer_batch)"]},{"cell_type":"markdown","metadata":{"id":"cell-97a"},"source":["### Veamos qué texto representa este batch:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-97b","executionInfo":{"status":"ok","timestamp":1761088511798,"user_tz":180,"elapsed":30,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"31c4d62b-8175-4c51-c030-57348fd466d9"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada: tensor([[16587,    78,   257,  8591]])\n","Texto de entrada: Debo a la\n","\n","Objetivo: tensor([[   78,   257,  8591, 11644]])\n","Texto objetivo: o a la conj\n"]}],"source":["print(\"Entrada:\", primer_batch[0])\n","print(\"Texto de entrada:\", tokenizador.decode(primer_batch[0][0].tolist()))\n","print(\"\\nObjetivo:\", primer_batch[1])\n","print(\"Texto objetivo:\", tokenizador.decode(primer_batch[1][0].tolist()))"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"test-batch-2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088668726,"user_tz":180,"elapsed":26,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"1e212c36-9805-474c-f549-e8bf3ae12b98"},"outputs":[{"output_type":"stream","name":"stdout","text":["[tensor([[ 2574,  1658,   431,  7639],\n","        [ 7949,  1155, 15498,  1288],\n","        [16245,    78,   390,   555],\n","        [ 1162,   445,   273,   551],\n","        [  555,    64, 28533,    64],\n","        [  390,  8591,  2386,   293],\n","        [12822,  4450,    11,   551],\n","        [36692,  2185,    73, 29690]]), tensor([[ 1658,   431,  7639,  7949],\n","        [ 1155, 15498,  1288, 16245],\n","        [   78,   390,   555,  1162],\n","        [  445,   273,   551,   555],\n","        [   64, 28533,    64,   390],\n","        [ 8591,  2386,   293, 12822],\n","        [ 4450,    11,   551, 36692],\n","        [ 2185,    73, 29690,    26]])]\n"]}],"source":["segundo_batch = next(iterador_datos)\n","print(segundo_batch)"]},{"cell_type":"markdown","metadata":{"id":"cell-98a"},"source":["### Veamos el segundo batch:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-98b","executionInfo":{"status":"ok","timestamp":1761088671548,"user_tz":180,"elapsed":18,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"1b80be5f-a2d0-434c-c39f-c99795fb28ff"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada: tensor([[ 2574,  1658,   431,  7639],\n","        [ 7949,  1155, 15498,  1288],\n","        [16245,    78,   390,   555],\n","        [ 1162,   445,   273,   551],\n","        [  555,    64, 28533,    64],\n","        [  390,  8591,  2386,   293],\n","        [12822,  4450,    11,   551],\n","        [36692,  2185,    73, 29690]])\n","Texto de entrada:  El espejo\n","\n","Objetivo: tensor([[ 1658,   431,  7639,  7949],\n","        [ 1155, 15498,  1288, 16245],\n","        [   78,   390,   555,  1162],\n","        [  445,   273,   551,   555],\n","        [   64, 28533,    64,   390],\n","        [ 8591,  2386,   293, 12822],\n","        [ 4450,    11,   551, 36692],\n","        [ 2185,    73, 29690,    26]])\n","Texto objetivo:  espejo inqu\n"]}],"source":["print(\"Entrada:\", segundo_batch[0])\n","print(\"Texto de entrada:\", tokenizador.decode(segundo_batch[0][0].tolist()))\n","print(\"\\nObjetivo:\", segundo_batch[1])\n","print(\"Texto objetivo:\", tokenizador.decode(segundo_batch[1][0].tolist()))"]},{"cell_type":"markdown","metadata":{"id":"stride-example"},"source":["- Un ejemplo usando paso igual a la longitud del contexto (aquí: 4) como se muestra a continuación:"]},{"cell_type":"markdown","metadata":{"id":"diagram-14"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"batch-outputs"},"source":["- También podemos crear salidas en batch\n","- Notemos que aumentamos el paso aquí para que no tengamos superposiciones entre los batches, ya que más superposición podría llevar a un aumento del sobreajuste"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"test-batch-8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088684112,"user_tz":180,"elapsed":55,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"6560e661-385b-4d4e-a87f-2b710e40e329"},"outputs":[{"output_type":"stream","name":"stdout","text":["Entradas:\n"," tensor([[16587,    78,   257,  8591],\n","        [11644, 49652, 18840,   390],\n","        [  555,  1658,   431,  7639],\n","        [  331,   390,   555,    64],\n","        [ 2207,   291,    75,   404],\n","        [ 5507,  1288,  1715,   549],\n","        [ 3036,  1153,    78,   390],\n","        [  471,    80,  5657,    13]])\n","\n","Objetivos:\n"," tensor([[   78,   257,  8591, 11644],\n","        [49652, 18840,   390,   555],\n","        [ 1658,   431,  7639,   331],\n","        [  390,   555,    64,  2207],\n","        [  291,    75,   404,  5507],\n","        [ 1288,  1715,   549,  3036],\n","        [ 1153,    78,   390,   471],\n","        [   80,  5657,    13,  2574]])\n"]}],"source":["dataloader = crear_dataloader_v1(texto_crudo, tamaño_batch=8, longitud_maxima=4, paso=4, mezclar=False)\n","\n","iterador_datos = iter(dataloader)\n","entradas, objetivos = next(iterador_datos)\n","print(\"Entradas:\\n\", entradas)\n","print(\"\\nObjetivos:\\n\", objetivos)"]},{"cell_type":"markdown","metadata":{"id":"cell-102a"},"source":["### Veamos qué texto representan estos batches (primeras 3 muestras):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-102b","executionInfo":{"status":"ok","timestamp":1761088741189,"user_tz":180,"elapsed":31,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"63d71fa2-ef7a-4656-fac3-163eb5c94470"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Mostrando las primeras 3 muestras del batch:\n","\n","Muestra 1:\n","  Entrada (IDs): [16587, 78, 257, 8591]\n","  Entrada (texto): 'Debo a la'\n","  Objetivo (IDs): [78, 257, 8591, 11644]\n","  Objetivo (texto): 'o a la conj'\n","\n","Muestra 2:\n","  Entrada (IDs): [11644, 49652, 18840, 390]\n","  Entrada (texto): ' conjunción de'\n","  Objetivo (IDs): [49652, 18840, 390, 555]\n","  Objetivo (texto): 'unción de un'\n","\n","Muestra 3:\n","  Entrada (IDs): [555, 1658, 431, 7639]\n","  Entrada (texto): ' un espejo'\n","  Objetivo (IDs): [1658, 431, 7639, 331]\n","  Objetivo (texto): ' espejo y'\n","\n"]}],"source":["print(\"Mostrando las primeras 3 muestras del batch:\\n\")\n","for i in range(min(3, len(entradas))):\n","    print(f\"Muestra {i+1}:\")\n","    print(f\"  Entrada (IDs): {entradas[i].tolist()}\")\n","    print(f\"  Entrada (texto): {tokenizador.decode(entradas[i].tolist())!r}\")\n","    print(f\"  Objetivo (IDs): {objetivos[i].tolist()}\")\n","    print(f\"  Objetivo (texto): {tokenizador.decode(objetivos[i].tolist())!r}\")\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"section-2-7"},"source":["## 2.7 Creando embeddings de tokens"]},{"cell_type":"markdown","metadata":{"id":"Gdum6t9qkUx4"},"source":["- Los datos ya están casi listos para un LLM\n","- Pero por último embedamos los tokens en una representación vectorial continua usando una capa de embedding\n","- Usualmente, estas capas de embedding son parte del LLM mismo y se actualizan (entrenan) durante el entrenamiento del modelo"]},{"cell_type":"markdown","metadata":{"id":"diagram-15"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">"]},{"cell_type":"markdown","metadata":{"id":"example-ids"},"source":["- Supongamos que tenemos los siguientes cuatro ejemplos de entrada con IDs de entrada 2, 3, 5 y 1 (después de la tokenización):"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"input-ids","executionInfo":{"status":"ok","timestamp":1761088762033,"user_tz":180,"elapsed":26,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["ids_entrada = torch.tensor([2, 3, 5, 1])"]},{"cell_type":"markdown","metadata":{"id":"small-vocab"},"source":["- Por simplicidad, supongamos que tenemos un vocabulario pequeño de solo 6 palabras y queremos crear embeddings de tamaño 3:"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"create-embedding-layer","executionInfo":{"status":"ok","timestamp":1761088764091,"user_tz":180,"elapsed":38,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["tamaño_vocab = 6\n","dim_salida = 3\n","\n","torch.manual_seed(123)\n","capa_embedding = torch.nn.Embedding(tamaño_vocab, dim_salida)"]},{"cell_type":"markdown","metadata":{"id":"weight-matrix"},"source":["- Esto resultaría en una matriz de pesos de 6x3:"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"print-weights","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088766664,"user_tz":180,"elapsed":49,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"053eeaf4-114d-4f47-fecb-884bbc2c7563"},"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.3374, -0.1778, -0.1690],\n","        [ 0.9178,  1.5810,  1.3010],\n","        [ 1.2753, -0.2010, -0.1606],\n","        [-0.4015,  0.9666, -1.1481],\n","        [-1.1589,  0.3255, -0.6315],\n","        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"]}],"source":["print(capa_embedding.weight)"]},{"cell_type":"markdown","metadata":{"id":"embedding-explanation"},"source":["- Para aquellos que están familiarizados con la codificación one-hot, el enfoque de la capa de embedding anterior es esencialmente solo una forma más eficiente de implementar la codificación one-hot seguida de la multiplicación de matrices en una capa completamente conectada\n","- Porque la capa de embedding es solo una implementación más eficiente que es equivalente al enfoque de codificación one-hot y multiplicación de matrices, puede verse como una capa de red neuronal que puede ser optimizada mediante backpropagation"]},{"cell_type":"markdown","metadata":{"id":"single-token"},"source":["- Para convertir un token con id 3 en un vector de 3 dimensiones, hacemos lo siguiente:"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"embed-single","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088769918,"user_tz":180,"elapsed":33,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"ca31a8e4-e247-439d-b0ab-55ba656086d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"]}],"source":["print(capa_embedding(torch.tensor([3])))"]},{"cell_type":"markdown","metadata":{"id":"multiple-tokens"},"source":["- Notemos que lo anterior es la cuarta fila en la matriz de pesos `capa_embedding`\n","- Para embeber todos los cuatro valores de `ids_entrada` anteriores, hacemos"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"embed-multiple","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761088772677,"user_tz":180,"elapsed":17,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"8ec34244-b8cb-4a33-a7ca-a92b90a5a5d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.2753, -0.2010, -0.1606],\n","        [-0.4015,  0.9666, -1.1481],\n","        [-2.8400, -0.7849, -1.4096],\n","        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"]}],"source":["print(capa_embedding(ids_entrada))"]},{"cell_type":"markdown","metadata":{"id":"lookup-operation"},"source":["- Una capa de embedding es esencialmente una operación de búsqueda:"]},{"cell_type":"markdown","metadata":{"id":"diagram-16"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"section-2-8"},"source":["## 2.8 Codificando posiciones de palabras"]},{"cell_type":"markdown","metadata":{"id":"position-problem"},"source":["- La capa de embedding convierte IDs en representaciones vectoriales idénticas independientemente de dónde se encuentren en la secuencia de entrada:"]},{"cell_type":"markdown","metadata":{"id":"diagram-17"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"400px\">"]},{"cell_type":"markdown","metadata":{"id":"position-solution"},"source":["- Los embeddings posicionales se combinan con el vector de embedding de tokens para formar los embeddings de entrada para un modelo de lenguaje grande:"]},{"cell_type":"markdown","metadata":{"id":"diagram-18"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">"]},{"cell_type":"markdown","metadata":{"id":"gpt2-vocab"},"source":["- El codificador BytePair tiene un tamaño de vocabulario de 50,257\n","- Supongamos que queremos codificar los tokens de entrada en una representación vectorial de 256 dimensiones:"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"token-embedding-layer","executionInfo":{"status":"ok","timestamp":1761082156041,"user_tz":180,"elapsed":92,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["tamaño_vocab = 50257\n","dim_salida = 256\n","\n","capa_embedding_tokens = torch.nn.Embedding(tamaño_vocab, dim_salida)"]},{"cell_type":"markdown","metadata":{"id":"batch-embedding"},"source":["- Si tomamos muestras de datos del dataloader, embedamos los tokens en cada batch en un vector de 256 dimensiones\n","- Si tenemos un tamaño de batch de 8 con 4 tokens cada uno, esto resulta en un tensor de 8 x 4 x 256:"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"create-batch-data","executionInfo":{"status":"ok","timestamp":1761082157948,"user_tz":180,"elapsed":36,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["longitud_maxima = 4\n","dataloader = crear_dataloader_v1(\n","    texto_crudo, tamaño_batch=8, longitud_maxima=longitud_maxima,\n","    paso=longitud_maxima, mezclar=False\n",")\n","iterador_datos = iter(dataloader)\n","entradas, objetivos = next(iterador_datos)"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"show-token-ids","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761082159661,"user_tz":180,"elapsed":17,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"4d58e12c-f88e-4892-fb78-0624c2cb19aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["IDs de tokens:\n"," tensor([[16587,    78,   257,  8591],\n","        [11644, 49652, 18840,   390],\n","        [  555,  1658,   431,  7639],\n","        [  331,   390,   555,    64],\n","        [ 2207,   291,    75,   404],\n","        [ 5507,  1288,  1715,   549],\n","        [ 3036,  1153,    78,   390],\n","        [  471,    80,  5657,    13]])\n","\n","Forma de las entradas:\n"," torch.Size([8, 4])\n"]}],"source":["print(\"IDs de tokens:\\n\", entradas)\n","print(\"\\nForma de las entradas:\\n\", entradas.shape)"]},{"cell_type":"markdown","metadata":{"id":"cell-128a"},"source":["### Veamos qué palabras representan estos IDs (primeras 3 muestras):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cell-128b","executionInfo":{"status":"ok","timestamp":1761082162588,"user_tz":180,"elapsed":43,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"17b8a8f6-9218-4e52-fed4-9a94559d4f50"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Mostrando las primeras 3 muestras del batch:\n","\n","Muestra 1:\n","  IDs: [16587, 78, 257, 8591]\n","  Texto: 'Debo a la'\n","\n","Muestra 2:\n","  IDs: [11644, 49652, 18840, 390]\n","  Texto: ' conjunción de'\n","\n","Muestra 3:\n","  IDs: [555, 1658, 431, 7639]\n","  Texto: ' un espejo'\n","\n"]}],"source":["print(\"Mostrando las primeras 3 muestras del batch:\\n\")\n","for i in range(min(3, len(entradas))):\n","    print(f\"Muestra {i+1}:\")\n","    print(f\"  IDs: {entradas[i].tolist()}\")\n","    print(f\"  Texto: {tokenizador.decode(entradas[i].tolist())!r}\")\n","    print()"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"embed-tokens","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761082173442,"user_tz":180,"elapsed":18,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"bfe93fc5-775e-45cb-d2d7-6e593ef6b7c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 4, 256])\n"]}],"source":["embeddings_tokens = capa_embedding_tokens(entradas)\n","print(embeddings_tokens.shape)\n","\n","# descomentá y ejecutá la siguiente línea para ver cómo se ven los embeddings\n","# print(embeddings_tokens)"]},{"cell_type":"markdown","metadata":{"id":"position-embeddings"},"source":["- GPT-2 usa embeddings de posición absolutos, así que simplemente creamos otra capa de embedding:"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"position-embedding-layer","executionInfo":{"status":"ok","timestamp":1761082176874,"user_tz":180,"elapsed":13,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}}},"outputs":[],"source":["longitud_contexto = longitud_maxima\n","capa_embedding_posicion = torch.nn.Embedding(longitud_contexto, dim_salida)\n","\n","# descomentá y ejecutá la siguiente línea para ver cómo se ven los pesos de la capa de embedding\n","# print(capa_embedding_posicion.weight)"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"create-position-embeddings","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761082180180,"user_tz":180,"elapsed":18,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"213b3eaa-2d1e-4d02-db09-6c9cacb9d918"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 256])\n"]}],"source":["embeddings_posicion = capa_embedding_posicion(torch.arange(longitud_maxima))\n","print(embeddings_posicion.shape)\n","\n","# descomentá y ejecutá la siguiente línea para ver cómo se ven los embeddings\n","# print(embeddings_posicion)"]},{"cell_type":"markdown","metadata":{"id":"combine-embeddings"},"source":["- Para crear los embeddings de entrada usados en un LLM, simplemente sumamos los embeddings de tokens y los embeddings posicionales:"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"final-embeddings","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761082182707,"user_tz":180,"elapsed":17,"user":{"displayName":"BARRETO MATÍAS","userId":"15011433883683359534"}},"outputId":"69f0a70f-edde-45e4-d116-98bdc0bd1df8"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 4, 256])\n"]}],"source":["embeddings_entrada = embeddings_tokens + embeddings_posicion\n","print(embeddings_entrada.shape)\n","\n","# descomentá y ejecutá la siguiente línea para ver cómo se ven los embeddings\n","# print(embeddings_entrada)"]},{"cell_type":"markdown","metadata":{"id":"workflow"},"source":["- En la fase inicial del flujo de procesamiento de entrada, el texto de entrada se segmenta en tokens separados\n","- Después de esta segmentación, estos tokens se transforman en IDs de tokens basados en un vocabulario predefinido:"]},{"cell_type":"markdown","metadata":{"id":"diagram-19"},"source":["<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">"]},{"cell_type":"markdown","metadata":{"id":"summary"},"source":["# Resumen y puntos clave"]},{"cell_type":"markdown","metadata":{"id":"summary-content"},"source":["Este cuaderno ilustra el proceso completo de preparación de datos de texto para entrenar un LLM:\n","\n","1. **Tokenización**: Dividimos el texto en unidades más pequeñas (tokens)\n","2. **Vocabulario**: Construimos un diccionario que mapea tokens a números\n","3. **Tokens especiales**: Agregamos tokens como `<|endoftext|>` y `<|unk|>` para manejar casos especiales\n","4. **BytePair Encoding (BPE)**: Un método más sofisticado que maneja palabras desconocidas descomponiéndolas en subpalabras\n","5. **Data Loader**: Creamos batches de datos con una ventana deslizante para entrenar el modelo\n","6. **Embeddings**: Convertimos los tokens en vectores densos que el modelo puede procesar\n","7. **Embeddings posicionales**: Agregamos información sobre la posición de cada token en la secuencia\n","\n","En este caso particular, trabajamos con el texto \"Tlön, Uqbar, Orbis Tertius\" de Jorge Luis Borges, demostrando cómo estos conceptos se aplican a textos en español."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":0}