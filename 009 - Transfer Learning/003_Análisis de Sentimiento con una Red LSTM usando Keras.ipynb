{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Análisis de Sentimiento con una Red LSTM usando Keras\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "En esta actividad vas a construir un modelo de red neuronal recurrente (RNN), específicamente una LSTM, usando la API Keras de TensorFlow. El modelo va a leer frases en español y clasificar su sentimiento como positivo o negativo.\n",
    "\n",
    "### ¿Qué vamos a lograr?\n",
    "\n",
    "- Entender cómo las redes recurrentes procesan secuencias de palabras\n",
    "- Implementar una LSTM que recuerda el contexto de la frase\n",
    "- Usar embeddings de palabras para representar el significado\n",
    "- Observar cómo las LSTM superan las limitaciones de bag-of-words\n",
    "\n",
    "### ¿Qué es una LSTM?\n",
    "\n",
    "LSTM (Long Short-Term Memory) es un tipo especial de red neuronal recurrente diseñada para:\n",
    "- **Procesar secuencias**: Lee las palabras en orden, una después de la otra\n",
    "- **Mantener memoria**: Recuerda información importante de palabras anteriores\n",
    "- **Olvidar información irrelevante**: Decide qué información del pasado mantener y qué descartar\n",
    "\n",
    "A diferencia de las MLP que vimos antes, las LSTM **sí consideran el orden** de las palabras, lo cual es fundamental para entender el lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Preparación del entorno\n",
    "\n",
    "Importamos las librerías necesarias, incluyendo herramientas de Keras para procesamiento de secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(f\"TensorFlow/Keras versión: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Datos de entrenamiento\n",
    "\n",
    "Vamos a usar las mismas frases que en la actividad anterior, pero ahora las vamos a procesar como **secuencias de palabras**, no como bolsa de palabras.\n",
    "\n",
    "Esta diferencia es fundamental: la LSTM va a poder aprovechar el orden en que aparecen las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases = [\n",
    "    \"La verdad, este lugar está bárbaro. Muy recomendable\",\n",
    "    \"Una porquería de servicio, nunca más vuelvo\",\n",
    "    \"Me encantó la comida, aunque la música estaba muy fuerte\",\n",
    "    \"El envío fue lento y el producto llegó dañado. Qué desastre\",\n",
    "    \"Todo excelente. Atención de diez\",\n",
    "    \"Qué estafa, me arrepiento de haber comprado\",\n",
    "    \"Muy conforme con el resultado final\",\n",
    "    \"No me gustó para nada la experiencia\",\n",
    "    \"Superó mis expectativas, gracias\",\n",
    "    \"No lo recomiendo, mala calidad\"\n",
    "]\n",
    "\n",
    "etiquetas = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "print(f\"Total de frases: {len(frases)}\")\n",
    "print(f\"Balance: {sum(etiquetas)} positivas, {len(etiquetas) - sum(etiquetas)} negativas\\n\")\n",
    "print(\"Ejemplos:\")\n",
    "for i in range(3):\n",
    "    sentimiento = \"Positivo\" if etiquetas[i] == 1 else \"Negativo\"\n",
    "    print(f\"  {i+1}. '{frases[i][:50]}...' → {sentimiento}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Tokenización y construcción del vocabulario\n",
    "\n",
    "Con Keras, vamos a convertir las frases en secuencias de números, donde cada número representa una palabra del vocabulario.\n",
    "\n",
    "### Diferencia clave con bag-of-words:\n",
    "\n",
    "**Bag-of-words:**\n",
    "- \"Me gusta\" → [1, 0, 1, 0, 0] (solo presencia/ausencia)\n",
    "\n",
    "**Secuencia:**\n",
    "- \"Me gusta\" → [5, 12] (orden preservado, cada palabra tiene un ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización: convierte palabras a números\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(frases)\n",
    "\n",
    "# Mostramos el vocabulario construido\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 por el índice 0\n",
    "print(f\"Tamaño del vocabulario: {vocab_size} palabras únicas\\n\")\n",
    "print(\"Primeras 15 palabras del vocabulario:\")\n",
    "for palabra, idx in list(tokenizer.word_index.items())[:15]:\n",
    "    print(f\"  '{palabra}' → {idx}\")\n",
    "\n",
    "# Convertimos frases a secuencias numéricas\n",
    "secuencias = tokenizer.texts_to_sequences(frases)\n",
    "\n",
    "print(f\"\\nEjemplo de conversión:\")\n",
    "print(f\"Frase original: '{frases[0]}'\")\n",
    "print(f\"Secuencia numérica: {secuencias[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Padding: estandarizando la longitud de las secuencias\n",
    "\n",
    "Las redes neuronales necesitan entradas de tamaño fijo, pero nuestras frases tienen longitudes diferentes.\n",
    "\n",
    "**Solución: Padding**\n",
    "- Rellenamos las secuencias cortas con ceros al final\n",
    "- Todas las secuencias terminan con la misma longitud\n",
    "\n",
    "Ejemplo:\n",
    "- `[5, 12]` → `[5, 12, 0, 0, 0]` (padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos la longitud máxima\n",
    "maxlen = max(len(seq) for seq in secuencias)\n",
    "print(f\"Longitud de la frase más larga: {maxlen} palabras\\n\")\n",
    "\n",
    "# Aplicamos padding\n",
    "X = pad_sequences(secuencias, maxlen=maxlen, padding='post')\n",
    "y = np.array(etiquetas)\n",
    "\n",
    "print(f\"Forma de X después del padding: {X.shape}\")\n",
    "print(f\"  {X.shape[0]} frases × {X.shape[1]} posiciones\\n\")\n",
    "\n",
    "print(\"Ejemplo de secuencia con padding:\")\n",
    "print(f\"Frase: '{frases[0]}'\")\n",
    "print(f\"Secuencia: {X[0]}\")\n",
    "print(f\"Nota: Los ceros al final son padding (relleno)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Definición del modelo LSTM\n",
    "\n",
    "Vamos a construir una red con tres componentes clave:\n",
    "\n",
    "### 1. Capa de Embedding\n",
    "Convierte cada palabra (número) en un vector denso de dimensión fija. Estos vectores se aprenden durante el entrenamiento y capturan similitudes semánticas.\n",
    "\n",
    "**Ejemplo conceptual:**\n",
    "- \"excelente\" → [0.8, 0.9, -0.1, 0.7, ...]\n",
    "- \"bueno\" → [0.7, 0.8, -0.2, 0.6, ...] (vector similar)\n",
    "- \"malo\" → [-0.7, -0.8, 0.2, -0.6, ...] (vector opuesto)\n",
    "\n",
    "### 2. Capa LSTM\n",
    "Procesa la secuencia de embeddings manteniendo memoria del contexto.\n",
    "\n",
    "### 3. Capa Dense (salida)\n",
    "Clasifica el sentimiento basándose en la representación aprendida por la LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "###############################################################################\n# CONFIGURACIÓN DE LA ARQUITECTURA LSTM\n###############################################################################\n\n###############################################################################\n# PARÁMETRO 1: embedding_dim (Dimensión de los embeddings)\n###############################################################################\n# ¿Qué es embedding_dim?\n# Es el tamaño del vector que representa cada palabra.\n#\n# Ejemplo conceptual con embedding_dim=16:\n# \"excelente\" → [0.8, 0.3, -0.5, 0.1, ..., 0.7]  (16 números)\n# \"bueno\"     → [0.7, 0.2, -0.4, 0.0, ..., 0.6]  (16 números, similar)\n# \"malo\"      → [-0.6, -0.3, 0.4, -0.2, ..., -0.5] (16 números, opuesto)\n#\n# ¿Por qué 16?\n# - Vocabulario pequeño (~50 palabras): 16 dimensiones son suficientes\n# - Valores típicos: 50, 100, 300 para vocabularios grandes\n# - Modelos grandes como Word2Vec: 300 dimensiones\n# - Modelos masivos como GPT-3: 12,288 dimensiones\n#\n# ¿Qué pasa si cambio embedding_dim?\n# - Muy pequeño (ej: 4): No captura suficientes matices del significado\n# - Muy grande (ej: 512): Overfitting con pocos datos de entrenamiento\n# - Regla práctica: Entre 16 y 100 para datasets pequeños\n#\n# Diferencia con bag-of-words:\n# - Bag-of-words: Vector sparse de tamaño=vocab_size, casi todo ceros\n#   Ejemplo: [0, 0, 1, 0, 0, ..., 0] (50 posiciones, 49 ceros)\n# - Embedding: Vector denso de tamaño=embedding_dim, todos valores útiles\n#   Ejemplo: [0.8, 0.3, -0.5, ..., 0.7] (16 valores significativos)\nembedding_dim = 16\n\n###############################################################################\n# PARÁMETRO 2: lstm_units (Unidades en la capa LSTM)\n###############################################################################\n# ¿Qué es lstm_units?\n# Es el tamaño del \"estado oculto\" de la LSTM.\n# Determina cuánta información puede \"recordar\" la red.\n#\n# Internamente, la LSTM tiene 4 componentes por unidad:\n# 1. Input gate: Qué información nueva agregar\n# 2. Forget gate: Qué información olvidar\n# 3. Output gate: Qué información sacar\n# 4. Cell state: Memoria a largo plazo\n#\n# ¿Por qué 32 unidades?\n# - Es el doble del embedding_dim (16 × 2 = 32)\n# - Valor típico para secuencias cortas\n# - Suficiente para capturar patrones en frases de ~10 palabras\n#\n# ¿Qué pasa si cambio lstm_units?\n# - Muy pequeño (ej: 8): Poca capacidad de memoria\n# - Muy grande (ej: 512): Overfitting, y entrenamiento lento\n# - Valores típicos: 32, 64, 128, 256\n#\n# Comparación de parámetros:\n# Con 32 unidades, la capa LSTM tiene aproximadamente:\n# 4 × lstm_units × (lstm_units + embedding_dim + 1) parámetros\n# = 4 × 32 × (32 + 16 + 1) = 4 × 32 × 49 = 6,272 parámetros\n#\n# Mucho más que la MLP del laboratorio anterior (145 parámetros)\nlstm_units = 32\n\n###############################################################################\n# CONSTRUCCIÓN DEL MODELO SECUENCIAL\n###############################################################################\n# Sequential permite apilar capas una tras otra\n# Flujo de datos: Input → Embedding → LSTM → Dense → Output\nmodelo = Sequential([\n    \n    ###########################################################################\n    # CAPA 1: Embedding (Representación de palabras)\n    ###########################################################################\n    # Embedding(input_dim, output_dim, input_length)\n    #\n    # ¿Qué hace esta capa?\n    # Convierte cada palabra (número entero) en un vector denso de números reales.\n    #\n    # Parámetros:\n    # - input_dim=vocab_size: Tamaño del vocabulario (~50 palabras únicas)\n    # - output_dim=embedding_dim: Dimensión del vector (16)\n    # - input_length=maxlen: Longitud de las secuencias (~10 palabras)\n    #\n    # ¿Cómo se inicializan los embeddings?\n    # Keras usa inicialización uniforme aleatoria por defecto:\n    # Valores entre -0.05 y +0.05\n    #\n    # ¿Cómo se aprenden?\n    # Durante backpropagation, los vectores se ajustan para que:\n    # - Palabras con sentimiento similar tengan vectores cercanos\n    # - Palabras positivas y negativas estén separadas en el espacio vectorial\n    #\n    # Ejemplo de transformación:\n    # Input:  [5, 12, 0, 0, ...]  (secuencia de IDs de palabras)\n    # Output: [[0.8, 0.3, ...],    (vector de \"palabra 5\")\n    #          [0.2, -0.5, ...],   (vector de \"palabra 12\")\n    #          [0.0, 0.0, ...],    (vector de padding)\n    #          ...]\n    #\n    # Forma de salida: (batch_size, maxlen, embedding_dim)\n    #                  (batch, 10, 16)\n    #\n    # Parámetros totales: vocab_size × embedding_dim ≈ 50 × 16 = 800\n    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),\n    \n    ###########################################################################\n    # CAPA 2: LSTM (Long Short-Term Memory)\n    ###########################################################################\n    # LSTM(units)\n    #\n    # ¿Qué hace esta capa?\n    # Procesa la secuencia de embeddings de izquierda a derecha,\n    # manteniendo un \"estado oculto\" que actúa como memoria.\n    #\n    # ¿Cómo funciona internamente?\n    # En cada paso temporal t (cada palabra), la LSTM:\n    #\n    # 1. Recibe:\n    #    - xₜ: embedding de la palabra actual (16 dimensiones)\n    #    - hₜ₋₁: estado oculto anterior (32 dimensiones)\n    #    - cₜ₋₁: cell state (memoria) anterior (32 dimensiones)\n    #\n    # 2. Calcula 3 puertas (gates) con funciones sigmoid σ:\n    #    - iₜ = σ(Wᵢ·xₜ + Uᵢ·hₜ₋₁ + bᵢ)  [input gate: qué agregar]\n    #    - fₜ = σ(Wf·xₜ + Uf·hₜ₋₁ + bf)  [forget gate: qué olvidar]\n    #    - oₜ = σ(Wₒ·xₜ + Uₒ·hₜ₋₁ + bₒ)  [output gate: qué sacar]\n    #\n    # 3. Actualiza cell state (memoria a largo plazo):\n    #    - c̃ₜ = tanh(Wc·xₜ + Uc·hₜ₋₁ + bc)  [candidato a agregar]\n    #    - cₜ = fₜ ⊙ cₜ₋₁ + iₜ ⊙ c̃ₜ          [memoria actualizada]\n    #\n    # 4. Calcula nuevo estado oculto:\n    #    - hₜ = oₜ ⊙ tanh(cₜ)\n    #\n    # Donde:\n    # - W, U: matrices de pesos\n    # - b: vectores de bias\n    # - ⊙: multiplicación elemento a elemento\n    # - σ: sigmoid (rango 0-1)\n    # - tanh: tangente hiperbólica (rango -1 a 1)\n    #\n    # ¿Por qué LSTM y no RNN simple?\n    # RNN simple sufre de \"vanishing gradient\":\n    # - No puede recordar información de hace muchos pasos\n    # - Los gradientes se hacen muy pequeños durante backpropagation\n    #\n    # LSTM soluciona esto con:\n    # - Cell state: camino directo para información a largo plazo\n    # - Gates: control fino sobre qué recordar/olvidar\n    #\n    # Ejemplo de uso de memoria:\n    # Frase: \"La comida es buena pero el servicio es malo\"\n    #         ↓  palabra \"pero\" activa forget gate\n    #         ↓  olvida parcialmente sentimiento positivo previo\n    #         ↓  presta atención a lo que viene después\n    #\n    # Parámetros:\n    # - units=32: Tamaño del estado oculto\n    # - return_sequences=False (por defecto): Solo devuelve último estado\n    #\n    # ¿Qué significa return_sequences=False?\n    # - True: Devuelve estado oculto para CADA palabra (shape: batch, maxlen, 32)\n    #   Uso: Cuando apilamos varias capas LSTM\n    # - False: Solo devuelve último estado oculto (shape: batch, 32)\n    #   Uso: Cuando vamos a clasificación (nuestro caso)\n    #\n    # Forma de entrada: (batch, maxlen, embedding_dim) = (batch, 10, 16)\n    # Forma de salida: (batch, lstm_units) = (batch, 32)\n    #\n    # Parámetros totales: 4 × lstm_units × (lstm_units + embedding_dim + 1)\n    #                   = 4 × 32 × (32 + 16 + 1)\n    #                   = 4 × 32 × 49\n    #                   = 6,272 parámetros\n    #\n    # Estos parámetros corresponden a las 4 operaciones:\n    # - Input gate (32 × 49 = 1,568)\n    # - Forget gate (32 × 49 = 1,568)\n    # - Output gate (32 × 49 = 1,568)\n    # - Cell state candidate (32 × 49 = 1,568)\n    LSTM(units=lstm_units),\n    \n    ###########################################################################\n    # CAPA 3: Dense (Clasificación final)\n    ###########################################################################\n    # Dense(1, activation='sigmoid')\n    #\n    # ¿Qué hace?\n    # Transforma el vector de 32 dimensiones (salida de LSTM)\n    # en un único valor entre 0 y 1 (probabilidad).\n    #\n    # Operación matemática:\n    # salida = sigmoid(W·h + b)\n    #\n    # Donde:\n    # - h: último estado oculto de LSTM (32 valores)\n    # - W: vector de pesos (32 valores)\n    # - b: bias (1 valor)\n    # - sigmoid: convierte a rango [0, 1]\n    #\n    # Forma de entrada: (batch, 32)\n    # Forma de salida: (batch, 1)\n    #\n    # Parámetros: 32 + 1 = 33\n    #\n    # Interpretación:\n    # - Salida ≥ 0.5 → Predicción: Positivo (clase 1)\n    # - Salida < 0.5 → Predicción: Negativo (clase 0)\n    Dense(1, activation='sigmoid')\n])\n\n###############################################################################\n# COMPILACIÓN DEL MODELO\n###############################################################################\n# La compilación configura cómo se va a entrenar el modelo\n\nmodelo.compile(\n    # Loss: Binary Cross Entropy (igual que en MLP)\n    # Mide qué tan diferentes son las predicciones de las etiquetas reales\n    loss='binary_crossentropy',\n    \n    # Optimizer: Adam con learning rate por defecto (0.001 en Keras)\n    # Nota: En PyTorch usamos lr=0.01, Keras usa 0.001 por defecto\n    # Keras tiende a usar valores más conservadores\n    optimizer='adam',\n    \n    # Metrics: Seguimos la accuracy durante el entrenamiento\n    # accuracy = (predicciones correctas) / (total de predicciones)\n    metrics=['accuracy']\n)\n\nprint(\"Arquitectura del modelo:\")\nmodelo.summary()\n# La salida de summary() muestra:\n# - Nombre de cada capa\n# - Forma de salida de cada capa\n# - Cantidad de parámetros por capa\n#\n# Ejemplo de salida esperada:\n# Layer (type)                Output Shape              Param #\n# =================================================================\n# embedding (Embedding)       (None, 10, 16)            800\n# lstm (LSTM)                 (None, 32)                6,272\n# dense (Dense)               (None, 1)                 33\n# =================================================================\n# Total params: 7,105\n\nprint(f\"\\nParámetros totales: {modelo.count_params():,}\")\n\n# Comparación de complejidad:\n# - Perceptrón: 17 parámetros\n# - MLP: 145 parámetros\n# - LSTM: ~7,105 parámetros (casi 50 veces más que MLP)\n#\n# ¿Por qué tantos parámetros?\n# La LSTM necesita 4 transformaciones por cada paso temporal\n# (input gate, forget gate, output gate, cell update)\n# Cada una requiere matrices de pesos para combinar\n# el input actual (16 dim) con el estado oculto previo (32 dim)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento\n",
    "\n",
    "Entrenamos el modelo por varias épocas. La LSTM va a aprender:\n",
    "- Qué palabras son importantes para el sentimiento\n",
    "- Cómo el orden afecta el significado\n",
    "- Qué patrones secuenciales indican positivo o negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "###############################################################################\n# PARÁMETROS DEL ENTRENAMIENTO\n###############################################################################\n\nprint(\"=\"*60)\nprint(\"INICIANDO ENTRENAMIENTO\")\nprint(\"=\"*60)\n\n# ¿Por qué 20 épocas?\n# Es suficiente para este dataset pequeño (10 frases)\n# - Con más datos, usaríamos más épocas (50, 100, etc.)\n# - Con early stopping, pararíamos automáticamente cuando no mejore\nprint(f\"Épocas: 20\")\n\n# ¿Qué es batch_size y por qué 2?\n# El batch size es cuántos ejemplos procesa el modelo antes de actualizar pesos.\n#\n# Batch size = 2 significa:\n# - En cada iteración, procesa 2 frases simultáneamente\n# - Calcula gradientes promediando sobre esas 2 frases\n# - Actualiza pesos usando ese gradiente promedio\n#\n# ¿Por qué 2 y no otro número?\n# - Muy pequeño (1): \"Stochastic Gradient Descent\", ruidoso pero actualiza rápido\n# - Medio (2-32): Buen balance, gradientes más estables\n# - Grande (128, 256): Más eficiente computacionalmente, pero necesita más datos\n#\n# Con 10 ejemplos y batch_size=2:\n# - Cada época procesa 10/2 = 5 batches\n# - 20 épocas × 5 batches = 100 actualizaciones de pesos totales\n#\n# Comparación con laboratorios anteriores:\n# - Perceptrón: Actualizaba después de CADA ejemplo (batch_size=1)\n# - MLP: Procesaba TODOS los ejemplos juntos (batch_size=10)\n# - LSTM: Procesa en mini-batches (batch_size=2)\n#\n# ¿Por qué mini-batches son mejores?\n# - Más eficientes que batch_size=1 (menos actualizaciones ruidosas)\n# - Mejor generalización que batch_size=total (no memoriza tanto)\n# - Aprovecha mejor GPUs (operaciones vectorizadas)\nprint(f\"Batch size: 2\\n\")\n\n###############################################################################\n# ENTRENAMIENTO CON model.fit()\n###############################################################################\n# En Keras, model.fit() hace TODO el ciclo de entrenamiento automáticamente:\n# 1. Divide datos en batches\n# 2. Para cada época:\n#    - Para cada batch:\n#      a. Forward pass\n#      b. Calcula loss\n#      c. Backward pass (backpropagation)\n#      d. Actualiza pesos\n#    - Reporta métricas (loss, accuracy)\n#\n# Es equivalente al bucle manual que hicimos en PyTorch,\n# pero más compacto y optimizado.\n#\n###############################################################################\n\nhistory = modelo.fit(\n    # X: Datos de entrada (10 frases × 10 palabras)\n    # y: Etiquetas (10 valores, 0 o 1)\n    X, y,\n    \n    # epochs: Cantidad de veces que recorre todo el dataset\n    epochs=20,\n    \n    # batch_size: Cantidad de ejemplos por batch\n    # Con 10 ejemplos y batch_size=2, habrá 5 batches por época\n    batch_size=2,\n    \n    # verbose: Nivel de información durante entrenamiento\n    # 0 = silencioso\n    # 1 = barra de progreso (elegido)\n    # 2 = una línea por época\n    verbose=1\n    \n    # Parámetros adicionales que podríamos usar:\n    # validation_split=0.2  → Reserva 20% para validación\n    # validation_data=(X_val, y_val)  → Usa datos de validación externos\n    # callbacks=[EarlyStopping(...)]  → Para cuando no mejore\n)\n\n# El objeto 'history' contiene el historial de entrenamiento:\n# - history.history['loss']: pérdida en cada época\n# - history.history['accuracy']: accuracy en cada época\n# - history.history['val_loss']: pérdida de validación (si usamos validation)\n# - history.history['val_accuracy']: accuracy de validación\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ENTRENAMIENTO FINALIZADO\")\nprint(\"=\"*60)\n\n# ¿Qué pasó durante el entrenamiento?\n#\n# Época 1:\n# - Embeddings aleatorios, LSTM sin entrenar\n# - Loss alta (~0.7), accuracy baja (~50%, como adivinar al azar)\n#\n# Épocas intermedias (5-10):\n# - Embeddings empiezan a capturar polaridad (positivo/negativo)\n# - LSTM aprende patrones secuenciales\n# - Loss baja (~0.3-0.4), accuracy mejora (~70-80%)\n#\n# Épocas finales (15-20):\n# - Modelo converge\n# - Loss muy baja (~0.1 o menos), accuracy alta (~90-100%)\n# - Puede haber overfitting si accuracy = 100% (memoriza training data)\n#\n# Señales de buen entrenamiento:\n# ✓ Loss disminuye consistentemente\n# ✓ Accuracy aumenta consistentemente\n# ✓ No hay saltos erráticos en las métricas\n#\n# Señales de problemas:\n# ✗ Loss aumenta o se estanca\n# ✗ Accuracy no mejora\n# ✗ Valores NaN en loss (gradientes explotaron)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Análisis del entrenamiento\n",
    "\n",
    "Observá cómo evolucionan la pérdida (loss) y la precisión (accuracy) durante el entrenamiento.\n",
    "\n",
    "- **Loss decrece**: El modelo comete menos errores\n",
    "- **Accuracy aumenta**: Más predicciones correctas\n",
    "\n",
    "Si la accuracy llega a 1.0, significa que el modelo clasificó perfectamente todos los ejemplos de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 8. Evaluación con frases nuevas\n",
    "\n",
    "Ahora vamos a probar el modelo con frases que no vio durante el entrenamiento. Esta es la verdadera prueba de si aprendió patrones generalizables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "frases_nuevas = [\n",
    "    \"Muy buena atención, quedé encantado\",\n",
    "    \"Horrible experiencia, no vuelvo más\",\n",
    "    \"Todo excelente, gracias por la atención\",\n",
    "    \"Me arrepiento completamente, fue un desastre\",\n",
    "    \"Un servicio impecable y rápido\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUACIÓN EN FRASES NUEVAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tokenizamos y aplicamos padding\n",
    "secuencias_nuevas = tokenizer.texts_to_sequences(frases_nuevas)\n",
    "X_nuevo = pad_sequences(secuencias_nuevas, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Predicción\n",
    "predicciones = modelo.predict(X_nuevo, verbose=0)\n",
    "\n",
    "# Mostrar resultados\n",
    "for i, (frase, pred) in enumerate(zip(frases_nuevas, predicciones), 1):\n",
    "    probabilidad = pred[0]\n",
    "    clase = \"Positivo\" if probabilidad >= 0.5 else \"Negativo\"\n",
    "    \n",
    "    print(f\"\\nFrase {i}: '{frase}'\")\n",
    "    print(f\"  Predicción: {clase} (probabilidad: {probabilidad:.2f})\")\n",
    "    \n",
    "    # Indicador de confianza\n",
    "    if probabilidad >= 0.8 or probabilidad <= 0.2:\n",
    "        print(f\"  Confianza: Alta\")\n",
    "    elif probabilidad >= 0.6 or probabilidad <= 0.4:\n",
    "        print(f\"  Confianza: Media\")\n",
    "    else:\n",
    "        print(f\"  Confianza: Baja (ambiguo)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 9. Comparación con enfoques anteriores\n",
    "\n",
    "Recapitulemos lo que mejoramos con cada modelo:\n",
    "\n",
    "### Perceptrón simple:\n",
    "- ✗ No considera orden de palabras\n",
    "- ✗ Representación binaria (0/1)\n",
    "- ✗ Modelo lineal\n",
    "- ✓ Muy simple de entender\n",
    "\n",
    "### MLP (Red Multicapa):\n",
    "- ✗ No considera orden de palabras\n",
    "- ✗ Representación binaria (0/1)\n",
    "- ✓ Puede aprender patrones no lineales\n",
    "- ✓ Mejor capacidad de generalización\n",
    "\n",
    "### LSTM:\n",
    "- ✓ **Considera el orden de las palabras**\n",
    "- ✓ **Embeddings aprendidos** (vectores densos)\n",
    "- ✓ **Memoria del contexto** (puede recordar palabras anteriores)\n",
    "- ✓ Puede aprender patrones no lineales complejos\n",
    "\n",
    "La LSTM es un avance significativo porque finalmente podemos procesar el lenguaje como una secuencia, no como una bolsa desordenada de palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 10. Reflexión final\n",
    "\n",
    "### ¿Qué aprendimos?\n",
    "\n",
    "1. **Procesamiento de secuencias**: Las LSTM pueden leer frases palabra por palabra, manteniendo memoria del contexto.\n",
    "\n",
    "2. **Embeddings de palabras**: En lugar de vectores binarios (0/1), cada palabra se representa con un vector denso que captura su significado.\n",
    "\n",
    "3. **Orden importa**: \"No me gusta\" y \"Me gusta, no\" ahora se procesan diferente (antes eran idénticos con bag-of-words).\n",
    "\n",
    "4. **Tokenización automática**: Keras construye el vocabulario automáticamente y maneja palabras desconocidas con `<OOV>`.\n",
    "\n",
    "### Ventajas sobre MLP con bag-of-words:\n",
    "\n",
    "- Captura el orden de las palabras\n",
    "- Aprende representaciones semánticas (embeddings)\n",
    "- Puede detectar patrones secuenciales\n",
    "- Mejor manejo de frases largas\n",
    "\n",
    "### Limitaciones que aún persisten:\n",
    "\n",
    "1. **Procesamiento secuencial**: La LSTM lee de izquierda a derecha, puede \"olvidar\" información del principio en frases muy largas\n",
    "\n",
    "2. **No puede mirar hacia adelante**: Al procesar una palabra, no sabe qué viene después\n",
    "\n",
    "3. **Dataset pequeño**: Con solo 10 ejemplos, los embeddings no se entrenan bien\n",
    "\n",
    "4. **Vocabulario limitado**: Solo conoce las palabras que aparecieron en el entrenamiento\n",
    "\n",
    "### ¿Qué sigue?\n",
    "\n",
    "En la próxima actividad vamos a ver cómo los **modelos preentrenados** como BETO (BERT en español) resuelven muchas de estas limitaciones:\n",
    "\n",
    "- Ya fueron entrenados con millones de textos\n",
    "- Tienen embeddings muy ricos\n",
    "- Usan arquitectura Transformer (no secuencial, con **atención**)\n",
    "- Pueden hacer análisis de sentimiento sin necesidad de entrenar desde cero\n",
    "\n",
    "Esto nos va a llevar al concepto de **transfer learning**, que revolucionó el NLP y es la base de los LLMs modernos como GPT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}