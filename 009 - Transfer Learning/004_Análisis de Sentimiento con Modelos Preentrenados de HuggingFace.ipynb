{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Análisis de Sentimiento con Modelos Preentrenados de HuggingFace\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "En esta actividad vamos a utilizar un modelo de estado del arte ya entrenado para analizar el sentimiento de frases en español con apenas unas líneas de código, gracias a la librería HuggingFace Transformers.\n",
    "\n",
    "Vamos a mostrar cómo es posible aprovechar el poder de los Transformers como BERT sin necesidad de entrenar redes neuronales desde cero.\n",
    "\n",
    "### ¿Por qué esto es revolucionario?\n",
    "\n",
    "Hasta ahora entrenamos modelos desde cero:\n",
    "- Perceptrón con 6 ejemplos\n",
    "- MLP con 10 ejemplos\n",
    "- LSTM con 10 ejemplos\n",
    "\n",
    "**El problema**: Con tan pocos datos, los modelos aprenden poco y generalizan mal.\n",
    "\n",
    "**La solución**: Usar modelos que ya fueron entrenados con millones de textos. Esto se llama **transfer learning** (aprendizaje por transferencia)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Instalación de la librería transformers\n",
    "\n",
    "Si estás en Google Colab, instalá la librería con el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. ¿Qué es BETO?\n",
    "\n",
    "Antes de usar el modelo, entendamos qué estamos cargando.\n",
    "\n",
    "### BETO = BERT en Español\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) es una arquitectura de modelo de lenguaje desarrollada por Google en 2018 que revolucionó el NLP.\n",
    "\n",
    "**BETO** es la versión en español de BERT, entrenada específicamente con textos en español.\n",
    "\n",
    "### Características de BETO:\n",
    "\n",
    "- **Entrenado con**: Wikipedia en español (millones de artículos)\n",
    "- **Arquitectura**: Transformer con atención bidireccional\n",
    "- **Parámetros**: ~110 millones\n",
    "- **Tiempo de entrenamiento**: Días en múltiples GPUs\n",
    "- **Costo de entrenamiento**: Miles de dólares en recursos computacionales\n",
    "\n",
    "### ¿Qué significa \"preentrenado\"?\n",
    "\n",
    "BETO ya aprendió:\n",
    "- Gramática del español\n",
    "- Vocabulario extenso\n",
    "- Relaciones semánticas entre palabras\n",
    "- Contextos en los que aparecen las palabras\n",
    "\n",
    "Nosotros vamos a usar ese conocimiento para análisis de sentimiento, sin tener que entrenar nada desde cero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 3. Cargando el modelo de análisis de sentimiento\n",
    "\n",
    "Vamos a usar `finiteautomata/beto-sentiment-analysis`, que es BETO ya ajustado específicamente para clasificar sentimientos en español.\n",
    "\n",
    "### ¿Qué es un pipeline?\n",
    "\n",
    "Un **pipeline** en HuggingFace encapsula todo el proceso:\n",
    "1. Tokenización del texto\n",
    "2. Conversión a formato del modelo\n",
    "3. Inferencia (predicción)\n",
    "4. Post-procesamiento de resultados\n",
    "\n",
    "Todo esto en una sola línea de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "###############################################################################\n# CARGA DEL MODELO PREENTRENADO: BETO para Análisis de Sentimiento\n###############################################################################\n\nfrom transformers import pipeline\n\nprint(\"Cargando modelo BETO para análisis de sentimiento...\")\nprint(\"Este proceso descarga el modelo (puede tardar unos segundos la primera vez)\\n\")\n\n###############################################################################\n# ¿Qué es un pipeline en HuggingFace?\n###############################################################################\n# Un pipeline es una abstracción que encapsula TODO el proceso de inferencia:\n#\n# 1. TOKENIZACIÓN:\n#    - Convierte texto a tokens (sub-palabras)\n#    - Ejemplo: \"increíble\" → [\"in\", \"##creíble\"] o [\"increíble\"]\n#    - Agrega tokens especiales: [CLS] al inicio, [SEP] al final\n#    - Convierte tokens a IDs numéricos según vocabulario del modelo\n#\n# 2. PREPARACIÓN DE ENTRADA:\n#    - Crea tensors de PyTorch/TensorFlow\n#    - Agrega padding si es necesario\n#    - Crea attention masks (qué posiciones son padding, qué son reales)\n#\n# 3. INFERENCIA (PREDICCIÓN):\n#    - Pasa los tensors por la red neuronal\n#    - BETO procesa la secuencia con 12 capas de Transformers\n#    - Cada capa aplica self-attention y feed-forward\n#    - La última capa clasifica: positivo, negativo, o neutral\n#\n# 4. POST-PROCESAMIENTO:\n#    - Convierte logits (números crudos) a probabilidades con softmax\n#    - Selecciona la clase con mayor probabilidad\n#    - Formatea resultado como diccionario: {'label': ..., 'score': ...}\n#\n# Todo esto pasa internamente cuando hacemos: clasificador(\"texto\")\n#\n###############################################################################\n\n###############################################################################\n# Creación del pipeline con modelo específico\n###############################################################################\n# pipeline(tarea, model=nombre_del_modelo)\n#\n# Parámetro 1: \"sentiment-analysis\"\n# Especifica la tarea de NLP que queremos realizar.\n# HuggingFace soporta múltiples tareas:\n# - \"sentiment-analysis\": Clasificación de sentimiento\n# - \"text-generation\": Generación de texto (como GPT)\n# - \"question-answering\": Responder preguntas sobre un contexto\n# - \"fill-mask\": Completar palabras enmascaradas (como BERT original)\n# - \"ner\": Named Entity Recognition (detectar nombres, lugares, etc.)\n# - \"translation\": Traducción entre idiomas\n# - \"summarization\": Resumir textos\n# - y muchas más...\n#\n# Parámetro 2: model=\"finiteautomata/beto-sentiment-analysis\"\n# Especifica QUÉ modelo preentrenado usar.\n#\n# ¿Qué significa este nombre?\n# - \"finiteautomata\": Organización que publicó el modelo en HuggingFace Hub\n# - \"beto-sentiment-analysis\": Nombre del modelo específico\n#\n# ¿Qué es este modelo exactamente?\n# Es BETO (BERT en español) que pasó por DOS etapas de entrenamiento:\n#\n# ETAPA 1: Preentrenamiento de BETO (hecho por dccuchile/beto)\n#   - Corpus: Wikipedia en español (millones de artículos)\n#   - Tarea: Masked Language Modeling (predecir palabras enmascaradas)\n#   - Duración: Días en GPUs potentes\n#   - Resultado: BETO entiende gramática, semántica, y contexto del español\n#\n# ETAPA 2: Fine-tuning para sentimiento (hecho por finiteautomata)\n#   - Corpus: Dataset etiquetado con sentimientos (positivo/negativo/neutral)\n#   - Tarea: Clasificación de sentimiento\n#   - Duración: Horas/días\n#   - Resultado: BETO ajustado específicamente para analizar sentimientos\n#\n# ¿Cuántos parámetros tiene este modelo?\n# Aproximadamente 110 millones de parámetros:\n# - 12 capas Transformer\n# - 768 dimensiones de embedding\n# - 12 attention heads por capa\n# - Vocabulario de ~31,000 tokens\n#\n# Comparación:\n# - Nuestro perceptrón: 17 parámetros\n# - Nuestra MLP: 145 parámetros\n# - Nuestra LSTM: ~7,000 parámetros\n# - BETO: ~110,000,000 parámetros (15,000 veces más que LSTM!)\n#\n# ¿Dónde se descarga el modelo?\n# Primera vez: Se descarga desde HuggingFace Hub (~400 MB)\n# Se guarda en cache local: ~/.cache/huggingface/\n# Próximas veces: Se carga desde cache (instantáneo)\n#\n# ¿Qué archivos se descargan?\n# - config.json: Configuración del modelo (capas, dimensiones, etc.)\n# - pytorch_model.bin o model.safetensors: Pesos del modelo (~400 MB)\n# - vocab.txt: Vocabulario (lista de 31,000 tokens)\n# - tokenizer_config.json: Configuración del tokenizador\n# - special_tokens_map.json: Tokens especiales ([CLS], [SEP], etc.)\n#\nclasificador = pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n\n###############################################################################\n# ¿Qué contiene el objeto 'clasificador'?\n###############################################################################\n# Es un objeto Pipeline que internamente tiene:\n#\n# 1. clasificador.tokenizer: \n#    - Tokenizador específico de BETO\n#    - Usa WordPiece tokenization (como BERT original)\n#    - Convierte texto → tokens → IDs\n#\n# 2. clasificador.model:\n#    - La red neuronal BETO completa\n#    - 12 capas Transformer (encoder)\n#    - Capa de clasificación al final (3 clases: POS, NEG, NEU)\n#    - ~110 millones de parámetros ya entrenados\n#\n# 3. clasificador.postprocessor:\n#    - Convierte salida cruda del modelo a formato amigable\n#    - Aplica softmax para obtener probabilidades\n#    - Mapea IDs de clases a etiquetas legibles\n#\n# ¿Cómo se usa?\n# Simplemente: clasificador(\"texto a clasificar\")\n# Devuelve: [{'label': 'POS', 'score': 0.95}]\n#\n###############################################################################\n\nprint(\"Modelo cargado exitosamente\")\nprint(f\"Modelo: {clasificador.model.config._name_or_path}\")\nprint(f\"\\nEste modelo fue ajustado (fine-tuned) específicamente para español.\")\n\n###############################################################################\n# Información adicional sobre el modelo\n###############################################################################\nprint(\"\\nCaracterísticas técnicas de BETO:\")\nprint(f\"  - Arquitectura: Transformer (BERT-base)\")\nprint(f\"  - Parámetros: ~110 millones\")\nprint(f\"  - Capas: 12 capas de self-attention\")\nprint(f\"  - Dimensión de embeddings: 768\")\nprint(f\"  - Attention heads por capa: 12\")\nprint(f\"  - Vocabulario: ~31,000 tokens (WordPiece)\")\nprint(f\"  - Contexto máximo: 512 tokens\")\nprint(f\"  - Entrenado en: Wikipedia en español\")\nprint(f\"  - Fine-tuned para: Clasificación de sentimiento (3 clases)\")\n\nprint(\"\\n¿Por qué NO necesitamos entrenar?\")\nprint(\"  1. El modelo YA aprendió español (preentrenamiento)\")\nprint(\"  2. El modelo YA aprendió sentimientos (fine-tuning)\")\nprint(\"  3. Solo necesitamos usarlo para inferencia\")\nprint(\"  4. Esto se llama TRANSFER LEARNING\")\n\nprint(\"\\nVentajas de usar modelos preentrenados:\")\nprint(\"  ✓ No necesitamos millones de ejemplos de entrenamiento\")\nprint(\"  ✓ No necesitamos GPUs potentes ni días de entrenamiento\")\nprint(\"  ✓ Obtenemos resultados de estado del arte inmediatamente\")\nprint(\"  ✓ El modelo ya entiende lenguaje natural complejo\")\nprint(\"  ✓ Podemos enfocarnos en la aplicación, no en el entrenamiento\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4. Comparación con nuestros modelos anteriores\n",
    "\n",
    "Antes de probar BETO, recordemos las limitaciones de nuestros modelos:\n",
    "\n",
    "| Aspecto | Nuestros modelos | BETO |\n",
    "|---------|-----------------|------|\n",
    "| Datos de entrenamiento | 6-10 frases | Millones de textos |\n",
    "| Vocabulario | 10-50 palabras | ~30,000 tokens |\n",
    "| Parámetros | Cientos/Miles | 110 millones |\n",
    "| Tiempo de entrenamiento | Segundos | Días en GPUs |\n",
    "| Comprensión del lenguaje | Básica | Avanzada |\n",
    "\n",
    "Esta diferencia de escala es lo que hace que BETO sea tan poderoso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 5. Evaluación de frases variadas\n",
    "\n",
    "Ahora vamos a probar el modelo con frases reales, incluyendo expresiones típicas de Argentina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "###############################################################################\n# USANDO EL CLASIFICADOR: ¿Qué pasa cuando predecimos?\n###############################################################################\n\nfrases = [\n    \"Este lugar es espectacular, lo recomiendo totalmente\",\n    \"Una decepción total. No pienso volver\",\n    \"Más o menos... esperaba otra cosa\",\n    \"Qué buena onda la atención, me encantó\",\n    \"Mala calidad, pésimo servicio\",\n    \"Zafa, pero nada especial\",\n    \"Me sentí muy bien atendido\",\n    \"Una estafa. Me arrepiento totalmente\",\n    \"Todo excelente, 10 puntos\",\n    \"Nunca más. Fue un desastre\"\n]\n\nprint(\"=\"*70)\nprint(\"PREDICCIONES DE BETO EN FRASES VARIADAS\")\nprint(\"=\"*70)\n\n###############################################################################\n# ¿Qué hace clasificador(frases) internamente?\n###############################################################################\n# Paso a paso del proceso de inferencia:\n#\n# 1. TOKENIZACIÓN (para cada frase):\n#    Ejemplo con \"Este lugar es espectacular\"\n#    a. Separar en tokens: [\"Este\", \"lugar\", \"es\", \"espectacular\"]\n#    b. Agregar tokens especiales: [\"[CLS]\", \"Este\", \"lugar\", \"es\", \"espectacular\", \"[SEP]\"]\n#    c. Convertir a IDs: [101, 2156, 3421, 2098, 15678, 102]\n#       donde 101=[CLS], 102=[SEP], y los demás son IDs del vocabulario\n#\n# 2. PREPARACIÓN DE TENSORS:\n#    - Crear tensor de input_ids: [[101, 2156, 3421, 2098, 15678, 102], ...]\n#    - Crear attention_mask: [[1, 1, 1, 1, 1, 1], ...] (1=real, 0=padding)\n#    - Si las frases tienen diferente largo, se agrega padding al final\n#\n# 3. PROCESAMIENTO POR BETO (12 capas):\n#\n#    Capa de Embeddings:\n#    - Token embeddings: Convierte cada ID a vector de 768 dimensiones\n#    - Position embeddings: Agrega información de posición en la frase\n#    - Segment embeddings: Para distinguir segmentos (no usado aquí)\n#    - Suma: embedding_total = token_emb + pos_emb + seg_emb\n#\n#    Capas 1-12 (Transformer Encoder):\n#    Cada capa hace:\n#    \n#    a. Multi-Head Self-Attention (12 heads):\n#       - Cada token \"presta atención\" a todos los demás tokens\n#       - Aprende qué palabras son importantes en el contexto\n#       - Ejemplo: \"no\" presta mucha atención a \"me gustó\"\n#       - Fórmula: Attention(Q,K,V) = softmax(Q·Kᵀ/√d)·V\n#       - Q, K, V se calculan con matrices aprendidas\n#    \n#    b. Feed-Forward Network:\n#       - Dos capas lineales con activación GELU\n#       - Procesa cada posición independientemente\n#       - Expande a 3072 dimensiones y vuelve a 768\n#    \n#    c. Normalización y conexiones residuales:\n#       - Layer normalization después de cada sub-capa\n#       - Conexiones skip para facilitar el entrenamiento\n#\n#    Output de las 12 capas:\n#    - Representación contextual de cada token (768 dimensiones)\n#    - El token [CLS] contiene una representación de toda la frase\n#\n# 4. CLASIFICACIÓN:\n#    - Toma el vector del token [CLS] (768 dimensiones)\n#    - Pasa por una capa Dense: [768] → [3]\n#    - 3 salidas (logits) para: NEG, NEU, POS\n#    - Aplica softmax: convierte logits a probabilidades\n#    - Suma de probabilidades = 1.0\n#\n# 5. POST-PROCESAMIENTO:\n#    - Selecciona la clase con mayor probabilidad\n#    - Mapea índice a etiqueta: 0→NEG, 1→NEU, 2→POS\n#    - Formatea como diccionario: {'label': 'POS', 'score': 0.95}\n#\n###############################################################################\n\n# Clasificamos cada frase\n# Este llamado ejecuta TODO el proceso descrito arriba\nresultados = clasificador(frases)\n\n# ¿Qué contiene 'resultados'?\n# Una lista de diccionarios, uno por cada frase:\n# [\n#   {'label': 'POS', 'score': 0.9523},\n#   {'label': 'NEG', 'score': 0.9876},\n#   ...\n# ]\n\n# Mostramos los resultados\nfor i, (frase, resultado) in enumerate(zip(frases, resultados), 1):\n    sentimiento = resultado['label']\n    confianza = resultado['score']\n    \n    print(f\"\\nFrase {i}: '{frase}'\")\n    print(f\"  Predicción: {sentimiento} (confianza: {confianza:.2f})\")\n    \n    # Indicador de confianza\n    if confianza >= 0.9:\n        print(f\"  Confianza: Muy alta\")\n    elif confianza >= 0.7:\n        print(f\"  Confianza: Alta\")\n    elif confianza >= 0.5:\n        print(f\"  Confianza: Media\")\n    else:\n        print(f\"  Confianza: Baja (poco seguro)\")\n\nprint(\"\\n\" + \"=\"*70)\n\n###############################################################################\n# Comparación: LSTM vs BETO\n###############################################################################\nprint(\"\\n¿Por qué BETO funciona mejor que nuestra LSTM?\")\nprint(\"\\n1. Arquitectura Transformer vs LSTM:\")\nprint(\"   LSTM: Procesa secuencialmente (izquierda → derecha)\")\nprint(\"         Puede 'olvidar' información del inicio\")\nprint(\"   BETO: Self-attention mira TODA la frase simultáneamente\")\nprint(\"         Cada palabra puede 'ver' todas las demás\")\nprint(\"\\n2. Contexto bidireccional:\")\nprint(\"   LSTM: Solo ve palabras anteriores al procesar cada token\")\nprint(\"   BETO: Ve contexto completo (antes y después) para cada token\")\nprint(\"\\n3. Capacidad del modelo:\")\nprint(\"   LSTM (nuestro): 7,000 parámetros, entrenado con 10 frases\")\nprint(\"   BETO: 110,000,000 parámetros, entrenado con millones de textos\")\nprint(\"\\n4. Vocabulario:\")\nprint(\"   LSTM (nuestro): ~50 palabras únicas\")\nprint(\"   BETO: ~31,000 tokens (incluyendo sub-palabras)\")\nprint(\"\\n5. Embeddings:\")\nprint(\"   LSTM (nuestro): 16 dimensiones, aprendidos de 10 frases\")\nprint(\"   BETO: 768 dimensiones, aprendidos de Wikipedia completa\")\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 6. Análisis de expresiones rioplatenses\n",
    "\n",
    "Vamos a probar específicamente con expresiones típicas de Argentina para ver si BETO las entiende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "expresiones_argentinas = [\n",
    "    \"Está re copado este lugar\",\n",
    "    \"Qué garrón, me clavaron\",\n",
    "    \"La pasé bomba, volvería sin dudarlo\",\n",
    "    \"Un bodrio total, no lo banco más\",\n",
    "    \"Me re sirvió, gracias\",\n",
    "    \"Qué embole, fue un bajón\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BETO CON EXPRESIONES RIOPLATENSES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "resultados = clasificador(expresiones_argentinas)\n",
    "\n",
    "for frase, resultado in zip(expresiones_argentinas, resultados):\n",
    "    print(f\"\\nFrase: '{frase}'\")\n",
    "    print(f\"  {resultado['label']} ({resultado['score']:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Observá cómo BETO entiende modismos argentinos como:\")\n",
    "print(\"  'copado', 'garrón', 'bomba', 'bodrio', 'embole', 'bajón'\")\n",
    "print(\"Esto es gracias a su entrenamiento con textos en español variados.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 7. Casos desafiantes: Opiniones mixtas y matices\n",
    "\n",
    "Probemos con casos más complejos que suelen confundir a modelos simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_complejos = [\n",
    "    \"La comida estuvo bien pero el servicio fue horrible\",\n",
    "    \"No estuvo mal, aunque esperaba más por el precio\",\n",
    "    \"Pensé que iba a ser un desastre pero me sorprendió para bien\",\n",
    "    \"Excelente producto, lástima la demora en el envío\",\n",
    "    \"No puedo decir que no me gustó, porque sería mentir\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CASOS COMPLEJOS: OPINIONES MIXTAS Y MATICES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "resultados = clasificador(casos_complejos)\n",
    "\n",
    "for i, (frase, resultado) in enumerate(zip(casos_complejos, resultados), 1):\n",
    "    print(f\"\\nCaso {i}: '{frase}'\")\n",
    "    print(f\"  Predicción: {resultado['label']} (confianza: {resultado['score']:.2f})\")\n",
    "    \n",
    "    # Análisis del caso\n",
    "    if i == 1:\n",
    "        print(f\"  Análisis: Opinión mixta (comida bien, servicio mal)\")\n",
    "    elif i == 2:\n",
    "        print(f\"  Análisis: Sentimiento tibio con matices\")\n",
    "    elif i == 3:\n",
    "        print(f\"  Análisis: Giro de expectativa negativa a positiva\")\n",
    "    elif i == 4:\n",
    "        print(f\"  Análisis: Positivo con salvedad\")\n",
    "    elif i == 5:\n",
    "        print(f\"  Análisis: Doble negación (no... no = sí)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 8. Comparación directa: ¿Cómo se compara con LSTM?\n",
    "\n",
    "Recordemos que con LSTM entrenamos con 10 frases. BETO, en cambio, fue entrenado con millones de textos.\n",
    "\n",
    "### Ventajas observadas de BETO:\n",
    "\n",
    "1. **Vocabulario extenso**: Entiende palabras que nunca vio en nuestro pequeño dataset\n",
    "2. **Contexto bidireccional**: Mira toda la frase simultáneamente, no solo de izquierda a derecha\n",
    "3. **Modismos y expresiones**: Reconoce jerga argentina sin entrenamiento específico\n",
    "4. **Matices**: Maneja mejor opiniones mixtas y giros en el sentimiento\n",
    "5. **Confianza calibrada**: Los scores reflejan mejor la certeza del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 9. Probá tus propias frases\n",
    "\n",
    "Ahora te toca experimentar. Agregá frases propias y observá cómo se comporta BETO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregá tus propias frases acá\n",
    "mis_frases = [\n",
    "    \"Tu frase 1\",\n",
    "    \"Tu frase 2\",\n",
    "    \"Tu frase 3\"\n",
    "]\n",
    "\n",
    "# Descomentá estas líneas cuando tengas tus frases\n",
    "# resultados_propios = clasificador(mis_frases)\n",
    "# for frase, resultado in zip(mis_frases, resultados_propios):\n",
    "#     print(f\"\\nFrase: '{frase}'\")\n",
    "#     print(f\"  Predicción: {resultado['label']} ({resultado['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 10. Reflexión final\n",
    "\n",
    "### ¿Qué aprendimos?\n",
    "\n",
    "1. **Transfer learning es poderoso**: No necesitamos entrenar desde cero para obtener buenos resultados en NLP.\n",
    "\n",
    "2. **Modelos preentrenados en español**: Existen modelos como BETO que entienden texto coloquial, incluso modismos regionales.\n",
    "\n",
    "3. **Facilidad de uso**: Con HuggingFace Transformers podemos usar modelos de estado del arte con apenas unas líneas de código.\n",
    "\n",
    "4. **Escala importa**: Un modelo entrenado con millones de ejemplos es cualitativamente diferente a uno entrenado con 10.\n",
    "\n",
    "### Comparación con nuestros modelos anteriores:\n",
    "\n",
    "**Perceptrón (Lab 1)**:\n",
    "- Entrenamiento: 6 frases, segundos\n",
    "- Limitación: Modelo lineal, bag-of-words\n",
    "- Resultado: Clasificación básica\n",
    "\n",
    "**MLP (Lab 2)**:\n",
    "- Entrenamiento: 10 frases, segundos\n",
    "- Mejora: Patrones no lineales\n",
    "- Limitación: Sigue sin orden, bag-of-words\n",
    "\n",
    "**LSTM (Lab 3)**:\n",
    "- Entrenamiento: 10 frases, minutos\n",
    "- Mejora: Procesa secuencias, embeddings\n",
    "- Limitación: Dataset pequeño, vocabulario limitado\n",
    "\n",
    "**BETO (Lab 4)**:\n",
    "- Entrenamiento: Millones de textos, días en GPUs\n",
    "- Mejora: Comprensión profunda del español\n",
    "- Uso: Sin entrenar, solo inferencia\n",
    "\n",
    "### ¿Por qué esto cambió el NLP?\n",
    "\n",
    "Antes de 2018 (pre-BERT), cada tarea requería:\n",
    "1. Conseguir dataset grande específico\n",
    "2. Entrenar modelo desde cero\n",
    "3. Esperar que generalizara\n",
    "\n",
    "Después de 2018 (era Transformer):\n",
    "1. Usar modelo preentrenado\n",
    "2. Opcionalmente: ajustar con pocos ejemplos (fine-tuning)\n",
    "3. Obtener resultados superiores\n",
    "\n",
    "### Conceptos clave para entender LLMs:\n",
    "\n",
    "Lo que vimos hoy es la base de cómo funcionan los LLMs modernos:\n",
    "\n",
    "- **Preentrenamiento masivo**: GPT, BERT, LLaMA se entrenan con billones de palabras\n",
    "- **Arquitectura Transformer**: Atención en lugar de procesamiento secuencial\n",
    "- **Transfer learning**: El conocimiento se transfiere entre tareas\n",
    "- **Emergencia de capacidades**: Con suficiente escala, surgen habilidades no programadas explícitamente\n",
    "\n",
    "### ¿Qué sigue?\n",
    "\n",
    "En las próximas clases vamos a profundizar en:\n",
    "- Arquitectura Transformer en detalle\n",
    "- Mecanismo de atención\n",
    "- Cómo se entrenan modelos como GPT\n",
    "- Fine-tuning y prompting\n",
    "- Construcción de aplicaciones con LLMs\n",
    "\n",
    "Hoy vimos **qué** pueden hacer estos modelos. Próximamente veremos **cómo** funcionan por dentro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}