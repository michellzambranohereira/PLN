{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Perceptrón para Análisis de Sentimiento en Español\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "En esta actividad vas a implementar desde cero un modelo de Perceptrón en Python usando solo NumPy. Vamos a entrenarlo con frases breves escritas en español rioplatense para que aprenda a reconocer si una frase tiene un sentimiento positivo o negativo.\n",
    "\n",
    "El objetivo es entender cómo funciona una neurona artificial básica, cómo se ajustan sus pesos durante el aprendizaje, y cómo puede hacer predicciones en base a un conjunto pequeño de frases.\n",
    "\n",
    "### ¿Qué es un perceptrón?\n",
    "\n",
    "El perceptrón es el modelo más simple de neurona artificial. Fue propuesto en 1958 por Frank Rosenblatt y representa la base fundamental de las redes neuronales modernas.\n",
    "\n",
    "**Funcionamiento básico:**\n",
    "1. Recibe múltiples entradas (features)\n",
    "2. Multiplica cada entrada por un peso\n",
    "3. Suma todos los resultados y agrega un sesgo (bias)\n",
    "4. Aplica una función de activación para decidir la salida\n",
    "\n",
    "Es un modelo lineal que solo puede aprender patrones que sean linealmente separables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Datos de entrenamiento\n",
    "\n",
    "Vamos a usar un pequeño conjunto de frases etiquetadas como positivas (1) o negativas (0). Las frases son simples como las que podríamos encontrar en una conversación cotidiana en Argentina.\n",
    "\n",
    "También definimos un vocabulario básico de palabras clave que aparecen con frecuencia y que nos pueden ayudar a inferir el sentimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Frases con su etiqueta de sentimiento (1 = positivo, 0 = negativo)\n",
    "frases = [\n",
    "    \"Amo el verano en Buenos Aires\",\n",
    "    \"No me gusta el tráfico matutino\",\n",
    "    \"Este asado está espectacular\",\n",
    "    \"Qué bajón, perdí el colectivo\",\n",
    "    \"Me encanta salir los domingos\",\n",
    "    \"Detesto el calor húmedo\"\n",
    "]\n",
    "\n",
    "etiquetas = np.array([1, 0, 1, 0, 1, 0])  # Etiquetas correspondientes\n",
    "\n",
    "# Vocabulario manual con palabras claves de carga emocional\n",
    "vocabulario = [\"amo\", \"no\", \"gusta\", \"asado\", \"espectacular\", \"bajón\", \"perdí\", \"detesto\", \"calor\", \"húmedo\"]\n",
    "\n",
    "print(f\"Total de frases: {len(frases)}\")\n",
    "print(f\"Vocabulario: {len(vocabulario)} palabras clave\")\n",
    "print(f\"\\nPrimera frase: '{frases[0]}' → Sentimiento: {'Positivo' if etiquetas[0] == 1 else 'Negativo'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Representación numérica: Vectorización de frases\n",
    "\n",
    "Las redes neuronales no pueden procesar texto directamente. Necesitamos convertir cada frase en un vector numérico.\n",
    "\n",
    "### Bag of Words (Bolsa de Palabras)\n",
    "\n",
    "Vamos a usar una representación simple llamada **bag of words**: para cada frase, creamos un vector binario que indica si cada palabra del vocabulario aparece (1) o no (0) en la frase.\n",
    "\n",
    "**Ejemplo:**\n",
    "- Vocabulario: [\"amo\", \"no\", \"gusta\", \"asado\"]\n",
    "- Frase: \"Amo el asado\"\n",
    "- Vector: [1, 0, 0, 1] (tiene \"amo\" y \"asado\", no tiene \"no\" ni \"gusta\")\n",
    "\n",
    "**Limitación importante:** Este método no captura el orden de las palabras ni el contexto. \"No me gusta\" y \"me gusta\" tendrían vectores muy similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizar(frase, vocabulario):\n",
    "    \"\"\"\n",
    "    Convierte una frase en un vector binario según el vocabulario.\n",
    "    \n",
    "    Args:\n",
    "        frase: String con la frase a vectorizar\n",
    "        vocabulario: Lista de palabras clave\n",
    "    \n",
    "    Returns:\n",
    "        Array de numpy con 1s y 0s\n",
    "    \"\"\"\n",
    "    tokens = frase.lower().split()\n",
    "    return np.array([1 if palabra in tokens else 0 for palabra in vocabulario])\n",
    "\n",
    "# Aplicamos la función a todas las frases\n",
    "X = np.array([vectorizar(frase, vocabulario) for frase in frases])\n",
    "\n",
    "print(\"Matriz de features (X):\")\n",
    "print(f\"Forma: {X.shape} (6 frases × 10 features)\\n\")\n",
    "print(\"Vectores generados:\")\n",
    "for i, (frase, vector) in enumerate(zip(frases, X)):\n",
    "    print(f\"Frase {i+1}: {vector} → Sentimiento: {'Positivo' if etiquetas[i] == 1 else 'Negativo'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Definición del modelo: el Perceptrón\n",
    "\n",
    "Un perceptrón es una función matemática que:\n",
    "\n",
    "1. **Multiplica** cada entrada por un peso (weight)\n",
    "2. **Suma** los resultados y agrega un sesgo (bias)\n",
    "3. **Aplica** una función de activación para decidir si \"dispara\" o no\n",
    "\n",
    "### Fórmula matemática:\n",
    "\n",
    "```\n",
    "z = (x₁ × w₁) + (x₂ × w₂) + ... + (xₙ × wₙ) + bias\n",
    "salida = función_activación(z)\n",
    "```\n",
    "\n",
    "### Función de activación escalón:\n",
    "\n",
    "```\n",
    "Si z > 0 → salida = 1 (positivo)\n",
    "Si z ≤ 0 → salida = 0 (negativo)\n",
    "```\n",
    "\n",
    "Vamos a inicializar los pesos aleatoriamente y entrenar el modelo para que aprenda de los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "###############################################################################\n# CONFIGURACIÓN DEL PERCEPTRÓN: INICIALIZACIÓN DE PARÁMETROS\n###############################################################################\n\n# ¿Por qué fijamos la semilla aleatoria?\n# np.random.seed(42) hace que los valores aleatorios sean reproducibles.\n# Cada vez que ejecutás este notebook, obtenés los mismos pesos iniciales.\n# Esto es fundamental para debugging y para que los resultados sean comparables.\nnp.random.seed(42)\n\n# ¿Por qué inicializamos los pesos aleatoriamente?\n# Si todos los pesos fueran iguales (ej: todos en 0), el modelo no podría aprender.\n# Necesitamos romper la simetría: cada peso debe empezar con un valor distinto.\n#\n# np.random.randn() genera números de una distribución normal (Gaussiana):\n# - Media = 0\n# - Desviación estándar = 1\n# - Valores típicos: entre -2 y +2 (68% entre -1 y +1)\n#\n# ¿Por qué esta distribución?\n# - Valores pequeños evitan saturación en funciones de activación\n# - Centrados en 0 para que el modelo no tenga sesgo inicial hacia positivo o negativo\n# - Distribución normal es la más común en inicialización de pesos\npesos = np.random.randn(len(vocabulario))\n\n# ¿Por qué el bias empieza en 0.0?\n# El bias (o sesgo) es un término que se suma a la combinación lineal.\n# Matemáticamente: z = w₁x₁ + w₂x₂ + ... + wₙxₙ + bias\n#\n# Empezamos en 0.0 porque:\n# - No tenemos información previa sobre si el dataset es mayormente positivo o negativo\n# - El bias se ajustará durante el entrenamiento según los datos\n# - Valor neutro que no predispone al modelo\nbias = 0.0\n\nprint(\"Pesos iniciales (aleatorios):\")\nfor i, (palabra, peso) in enumerate(zip(vocabulario, pesos)):\n    print(f\"  w[{palabra}] = {peso:.3f}\")\nprint(f\"\\nBias inicial: {bias}\")\n\n###############################################################################\n# FUNCIÓN DE ACTIVACIÓN: ESCALÓN (STEP FUNCTION)\n###############################################################################\n\ndef activar(suma):\n    \"\"\"\n    Función de activación escalón (step function).\n    \n    ¿Qué hace?\n    Convierte un número real cualquiera en una decisión binaria (0 o 1).\n    \n    ¿Por qué usamos esta función?\n    El perceptrón clásico de Rosenblatt (1958) usaba esta función porque:\n    - Es simple de implementar y entender\n    - Produce una decisión clara: positivo (1) o negativo (0)\n    - Modela una neurona biológica que \"dispara\" o \"no dispara\"\n    \n    Matemáticamente:\n    f(z) = 1  si z > 0\n    f(z) = 0  si z ≤ 0\n    \n    Limitación importante:\n    Esta función NO es diferenciable en z=0, lo que impide usar\n    técnicas modernas como backpropagation. Por eso las redes modernas\n    usan funciones como ReLU o Sigmoid.\n    \n    Args:\n        suma: Valor numérico de la combinación lineal z = w·x + b\n    \n    Returns:\n        1 si suma > 0, 0 en caso contrario\n    \"\"\"\n    return 1 if suma > 0 else 0\n\n###############################################################################\n# FUNCIÓN DE PREDICCIÓN: CÁLCULO DE LA SALIDA DEL PERCEPTRÓN\n###############################################################################\n\ndef predecir(x):\n    \"\"\"\n    Calcula la predicción del perceptrón para un vector de entrada.\n    \n    ¿Cómo funciona?\n    1. Producto punto: np.dot(x, pesos) calcula x₁w₁ + x₂w₂ + ... + xₙwₙ\n    2. Suma el bias: z = producto_punto + bias\n    3. Aplica activación: salida = activar(z)\n    \n    Ejemplo numérico:\n    Supongamos:\n    - x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0]  (vector de una frase)\n    - pesos = [0.5, -0.3, 0.2, ...]       (pesos aprendidos)\n    - bias = 0.1\n    \n    Entonces:\n    - suma = (1)(0.5) + (0)(-0.3) + (1)(0.2) + ... + 0.1\n    - suma = 0.5 + 0 + 0.2 + ... + 0.1 = 0.8\n    - activar(0.8) = 1  (porque 0.8 > 0)\n    - Predicción: Positivo\n    \n    ¿Por qué usamos producto punto?\n    Es la operación fundamental del álgebra lineal que permite:\n    - Combinar múltiples features en un solo valor\n    - Ponderar cada feature por su importancia (peso)\n    - Implementación eficiente en NumPy (optimizada en C)\n    \n    Args:\n        x: Vector de features (array de numpy con 1s y 0s)\n    \n    Returns:\n        Predicción: 0 (negativo) o 1 (positivo)\n    \"\"\"\n    suma = np.dot(x, pesos) + bias\n    return activar(suma)\n\nprint(\"\\nFunciones definidas: activar() y predecir()\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del modelo\n",
    "\n",
    "Ahora vamos a entrenar el perceptrón ajustando los pesos según los errores que comete.\n",
    "\n",
    "### Regla de aprendizaje del perceptrón:\n",
    "\n",
    "Para cada ejemplo:\n",
    "1. Calcular la predicción\n",
    "2. Calcular el error: `error = etiqueta_real - predicción`\n",
    "3. Si hay error (≠ 0), ajustar los pesos:\n",
    "   - `peso_nuevo = peso_viejo + (tasa_aprendizaje × error × entrada)`\n",
    "   - `bias_nuevo = bias_viejo + (tasa_aprendizaje × error)`\n",
    "\n",
    "### Parámetros de entrenamiento:\n",
    "\n",
    "- **Tasa de aprendizaje**: Controla qué tan grande es cada ajuste (0.1 es un valor común)\n",
    "- **Épocas**: Cantidad de veces que recorremos todo el dataset\n",
    "\n",
    "El entrenamiento se detiene cuando el modelo clasifica correctamente todos los ejemplos o llega al máximo de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "###############################################################################\n# CONFIGURACIÓN DEL ENTRENAMIENTO: HIPERPARÁMETROS\n###############################################################################\n\n# ¿Qué es la tasa de aprendizaje (learning rate)?\n# Es el tamaño del \"paso\" que damos cuando ajustamos los pesos.\n#\n# Matemáticamente: peso_nuevo = peso_viejo + (tasa_aprendizaje × error × entrada)\n#\n# ¿Por qué 0.1?\n# - Muy grande (ej: 1.0): El modelo puede \"saltar\" la solución y no converger\n# - Muy pequeña (ej: 0.001): El aprendizaje es muy lento, necesita muchas épocas\n# - 0.1 es un valor moderado que suele funcionar bien para problemas simples\n#\n# Analogía: Si estás bajando una montaña en la niebla:\n# - Tasa grande = pasos largos (rápido pero podés pasar la base)\n# - Tasa pequeña = pasos cortos (lento pero seguro)\ntasa_aprendizaje = 0.1\n\n# ¿Qué es una época?\n# Una época es un recorrido completo por todos los ejemplos de entrenamiento.\n# En este caso, 1 época = procesar las 6 frases una vez.\n#\n# ¿Por qué 20 épocas?\n# - Es un número arbitrario pero suficiente para este dataset pequeño\n# - El perceptrón puede converger en pocas épocas si los datos son linealmente separables\n# - Si no converge en 20 épocas, probablemente los datos no sean linealmente separables\n#\n# Nota: El entrenamiento puede terminar antes si el modelo clasifica todo correctamente\nepocas = 20\n\nprint(\"=\"*60)\nprint(\"INICIANDO ENTRENAMIENTO\")\nprint(\"=\"*60)\nprint(f\"Tasa de aprendizaje: {tasa_aprendizaje}\")\nprint(f\"Épocas máximas: {epocas}\")\nprint(f\"Ejemplos de entrenamiento: {len(X)}\\n\")\n\n###############################################################################\n# ALGORITMO DE ENTRENAMIENTO DEL PERCEPTRÓN\n###############################################################################\n# \n# ¿Cómo aprende el perceptrón?\n# Usa la \"Regla de aprendizaje del perceptrón\" propuesta por Rosenblatt (1958):\n#\n# Para cada ejemplo:\n#   1. Hacer una predicción con los pesos actuales\n#   2. Calcular el error: error = etiqueta_real - predicción\n#   3. Si error ≠ 0, ajustar los pesos:\n#      - Si error = 1 (predicción 0, real 1): aumentar pesos de features activos\n#      - Si error = -1 (predicción 1, real 0): disminuir pesos de features activos\n#\n# ¿Por qué funciona?\n# Matemáticamente, se demuestra que si los datos son linealmente separables,\n# el perceptrón SIEMPRE converge a una solución (Teorema de Convergencia del Perceptrón).\n#\n###############################################################################\n\n# Bucle de entrenamiento\nfor epoca in range(epocas):\n    errores = 0\n    \n    # Recorremos cada ejemplo del dataset\n    for i in range(len(X)):\n        x_i = X[i]              # Vector de features de la frase i\n        y_real = etiquetas[i]   # Etiqueta verdadera (0 o 1)\n        y_pred = predecir(x_i)  # Predicción del modelo\n        error = y_real - y_pred # Error: -1, 0, o 1\n        \n        # Solo ajustamos pesos si hubo error\n        if error != 0:\n            # Actualización de pesos:\n            # Si error = 1 (falso negativo): aumentamos pesos donde x_i = 1\n            # Si error = -1 (falso positivo): disminuimos pesos donde x_i = 1\n            #\n            # Ejemplo numérico:\n            # Si x_i = [1, 0, 1], error = 1, tasa = 0.1\n            # Entonces:\n            # pesos += 0.1 × 1 × [1, 0, 1] = [0.1, 0, 0.1]\n            # Solo los pesos de palabras presentes (x_i=1) cambian\n            pesos += tasa_aprendizaje * error * x_i\n            \n            # Actualización del bias:\n            # El bias no depende de ninguna entrada específica, solo del error\n            bias += tasa_aprendizaje * error\n            \n            errores += 1\n    \n    print(f\"Época {epoca + 1:2d}: Errores = {errores}\")\n    \n    # Condición de convergencia:\n    # Si errores = 0, el modelo clasificó correctamente TODOS los ejemplos\n    # No tiene sentido seguir entrenando\n    if errores == 0:\n        print(f\"\\nConvergencia alcanzada en época {epoca + 1}\")\n        print(\"El modelo clasifica correctamente todos los ejemplos de entrenamiento.\")\n        break\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ENTRENAMIENTO FINALIZADO\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Análisis de los pesos aprendidos\n",
    "\n",
    "Después del entrenamiento, los pesos nos indican qué tan importante es cada palabra para determinar el sentimiento.\n",
    "\n",
    "- **Pesos positivos**: Palabras asociadas a sentimiento positivo\n",
    "- **Pesos negativos**: Palabras asociadas a sentimiento negativo\n",
    "- **Pesos cercanos a 0**: Palabras poco informativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pesos aprendidos después del entrenamiento:\\n\")\n",
    "print(f\"{'Palabra':<15} {'Peso':>10} {'Interpretación'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for palabra, peso in sorted(zip(vocabulario, pesos), key=lambda x: x[1], reverse=True):\n",
    "    if peso > 0.1:\n",
    "        interpretacion = \"→ Positivo\"\n",
    "    elif peso < -0.1:\n",
    "        interpretacion = \"→ Negativo\"\n",
    "    else:\n",
    "        interpretacion = \"→ Neutral\"\n",
    "    \n",
    "    print(f\"{palabra:<15} {peso:>10.3f}  {interpretacion}\")\n",
    "\n",
    "print(f\"\\nBias final: {bias:.3f}\")\n",
    "print(\"\\nEl bias representa el umbral base del modelo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Prueba con nuevas frases\n",
    "\n",
    "Ahora vamos a ver cómo se comporta nuestro perceptrón con frases nuevas que no vio durante el entrenamiento.\n",
    "\n",
    "Esta es la verdadera prueba: ¿puede generalizar lo que aprendió a casos nuevos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frases nuevas para testeo\n",
    "frases_prueba = [\n",
    "    \"No aguanto este calor\",\n",
    "    \"Qué hermoso día para pasear\",\n",
    "    \"Detesto levantarme temprano\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREDICCIONES EN FRASES NUEVAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Vectorizamos las frases nuevas\n",
    "X_prueba = np.array([vectorizar(frase, vocabulario) for frase in frases_prueba])\n",
    "predicciones = [predecir(x) for x in X_prueba]\n",
    "\n",
    "# Mostramos los resultados\n",
    "for i, (frase, pred, vector) in enumerate(zip(frases_prueba, predicciones, X_prueba), 1):\n",
    "    resultado = \"Positivo\" if pred == 1 else \"Negativo\"\n",
    "    print(f\"\\nFrase {i}: '{frase}'\")\n",
    "    print(f\"  Vector: {vector}\")\n",
    "    print(f\"  Predicción: {resultado}\")\n",
    "    \n",
    "    # Mostramos qué palabras del vocabulario detectó\n",
    "    palabras_detectadas = [vocabulario[j] for j in range(len(vocabulario)) if vector[j] == 1]\n",
    "    if palabras_detectadas:\n",
    "        print(f\"  Palabras clave detectadas: {', '.join(palabras_detectadas)}\")\n",
    "    else:\n",
    "        print(f\"  No se detectaron palabras del vocabulario\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Reflexión final\n",
    "\n",
    "### ¿Qué aprendimos?\n",
    "\n",
    "1. **Funcionamiento de una neurona artificial básica**: El perceptrón es el bloque fundamental de las redes neuronales. Aprendimos cómo combina entradas ponderadas y aplica una función de activación.\n",
    "\n",
    "2. **Proceso de entrenamiento**: Vimos cómo un modelo aprende ajustando sus pesos iterativamente basándose en los errores que comete. Este principio se extiende a redes neuronales más complejas.\n",
    "\n",
    "3. **Representación de texto**: Usamos bag-of-words, una técnica simple pero efectiva para convertir texto en números que las máquinas pueden procesar.\n",
    "\n",
    "### Limitaciones observadas\n",
    "\n",
    "1. **No considera el orden**: \"No me gusta\" vs \"Me gusta, no\" se representan igual\n",
    "2. **Vocabulario limitado**: Solo conoce las palabras que definimos manualmente\n",
    "3. **Modelo lineal**: Solo puede aprender patrones linealmente separables\n",
    "4. **Sin contexto**: No entiende sarcasmo, ironía o matices del lenguaje\n",
    "5. **Dataset pequeño**: Con solo 6 ejemplos, la generalización es limitada\n",
    "\n",
    "### ¿Qué sigue?\n",
    "\n",
    "En el próximo laboratorio, vamos a ver cómo las **redes neuronales multicapa** (MLP) pueden capturar patrones más complejos usando múltiples perceptrones organizados en capas. Esto nos va a permitir:\n",
    "\n",
    "- Aprender representaciones no lineales\n",
    "- Capturar interacciones entre features\n",
    "- Mejorar la capacidad de generalización\n",
    "\n",
    "Más adelante veremos cómo las **redes recurrentes** (LSTM) pueden procesar el orden de las palabras y, finalmente, cómo los **Transformers** revolucionaron el procesamiento de lenguaje natural."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}