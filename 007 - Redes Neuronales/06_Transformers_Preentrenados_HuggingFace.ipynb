{"cells":[{"cell_type":"markdown","metadata":{"id":"JSliuGhpb2sC"},"source":["# Transformers Pre-entrenados con HuggingFace\n","\n","**Materiales desarrollados por Mat√≠as Barreto, 2025**\n","\n","**Tecnicatura en Ciencia de Datos - IFTS**\n","\n","**Asignatura:** Procesamiento de Lenguaje Natural\n","\n","---\n","\n","## Introducci√≥n\n","\n","Este notebook marca un **punto de inflexi√≥n** en el curso. Hasta ahora construimos modelos desde cero (perceptr√≥n, MLP, LSTM) para entender los fundamentos. Ahora damos el salto al **paradigma moderno de NLP**: usar modelos transformers pre-entrenados con transfer learning.\n","\n","### El momento \"aj√°\" del curso\n","\n","Despu√©s de 5 notebooks programando y entrenando modelos, ahora vamos a ver que:\n","- **3 l√≠neas de c√≥digo** superan todo lo anterior\n","- No necesitamos entrenar desde cero\n","- Transfer learning es el est√°ndar de la industria\n","\n","Esto no invalida lo que aprendimos. Todo ese conocimiento te permite entender **qu√© est√° pasando bajo el cap√≥** de HuggingFace.\n","\n","### ¬øQu√© son los Transformers?\n","\n","Arquitectura revolucionaria introducida en \"Attention is All You Need\" (Vaswani et al., 2017):\n","- **Attention mechanism**: Todas las palabras se ven entre s√≠ simult√°neamente\n","- **Paralelizable**: No procesa secuencialmente como LSTM\n","- **Escalable**: Funciona con modelos gigantes (GPT-3: 175B par√°metros)\n","- **Transfer learning**: Pre-entrenamiento + fine-tuning\n","\n","### Objetivos de aprendizaje\n","\n","1. Usar HuggingFace Transformers library\n","2. Cargar modelos pre-entrenados (BETO, RoBERTuito)\n","3. Aplicar pipelines para tareas comunes\n","4. Entender el concepto de transfer learning\n","5. Trabajar con modelos en espa√±ol\n","6. Prepararse para fine-tuning (pr√≥ximas semanas del curso)\n","\n","### ¬øPor qu√© HuggingFace?\n","\n","- **Model Hub**: Miles de modelos pre-entrenados\n","- **API unificada**: Mismo c√≥digo para BERT, GPT, T5, etc.\n","- **Comunidad**: Open source, muy activa\n","- **Industria**: Est√°ndar de facto en producci√≥n\n","- **Espa√±ol**: Excelentes modelos para espa√±ol (BETO, RoBERTuito, MarIA)"]},{"cell_type":"markdown","metadata":{"id":"P4Eg2131b2sK"},"source":["---\n","\n","## 1. Instalaci√≥n de HuggingFace Transformers\n","\n","En Google Colab, instalamos la librer√≠a (ya viene en algunas versiones, pero mejor asegurarnos)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ay8EDK-b2sM"},"outputs":[],"source":["# Instalamos transformers de HuggingFace\n","# -q: quiet mode (menos output)\n","!pip install -q transformers\n","\n","print(\"Librer√≠a 'transformers' instalada correctamente.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lYdAr3Mb2sO"},"outputs":[],"source":["# Importamos librer√≠as\n","from transformers import pipeline\n","import warnings\n","\n","# Suprimimos warnings menores para output m√°s limpio\n","warnings.filterwarnings('ignore')\n","\n","print(\"Librer√≠as importadas.\")\n","print(\"\\nNota: La primera vez que uses un modelo, HuggingFace lo descargar√°.\")\n","print(\"Esto puede tardar unos minutos dependiendo del tama√±o del modelo.\")"]},{"cell_type":"markdown","metadata":{"id":"aw59VnWzb2sP"},"source":["---\n","\n","## 2. Concepto: Transfer Learning en NLP\n","\n","### Paradigma tradicional (lo que hicimos en notebooks anteriores):\n","\n","```\n","1. Recolectar dataset etiquetado\n","2. Inicializar pesos aleatoriamente\n","3. Entrenar desde cero\n","4. Evaluar\n","```\n","\n","**Problemas:**\n","- Requiere muchos datos (>10k muestras)\n","- Tiempo de entrenamiento largo\n","- Recursos computacionales (GPU)\n","- El modelo aprende lenguaje desde cero\n","\n","### Paradigma moderno: Transfer Learning\n","\n","```\n","1. Tomar modelo pre-entrenado en corpus masivo (Wikipedia, Common Crawl)\n","2. Adaptar a tu tarea espec√≠fica (fine-tuning)\n","3. Requiere pocos datos (<1k muestras)\n","4. Entrenamiento r√°pido\n","```\n","\n","**Ventajas:**\n","- El modelo ya \"entiende\" lenguaje\n","- Solo ajusta para tu tarea espec√≠fica\n","- Funciona con datasets peque√±os\n","- Estado del arte con menos esfuerzo\n","\n","### Analog√≠a\n","\n","**Entrenar desde cero:** Como ense√±ar a leer a alguien desde alfabeto y luego pedirle que clasifique sentimientos\n","\n","**Transfer learning:** Como pedirle a alguien que ya lee espa√±ol que aprenda a distinguir rese√±as positivas/negativas (mucho m√°s r√°pido)\n","\n","### Dos fases del Transfer Learning\n","\n","**Pre-entrenamiento (ya hecho por otros):**\n","- Corpus masivo: Wikipedia (3B palabras), Common Crawl (500B palabras)\n","- Tarea auto-supervisada: Masked Language Modeling (BERT), Next Token Prediction (GPT)\n","- Meses de entrenamiento en clusters de GPUs\n","- Costo: $100k - $1M USD\n","\n","**Fine-tuning (lo que nosotros haremos):**\n","- Dataset espec√≠fico: Nuestras rese√±as etiquetadas\n","- Tarea supervisada: Clasificaci√≥n, NER, QA, etc.\n","- Horas/d√≠as en una GPU\n","- Costo: $10 - $100 USD"]},{"cell_type":"markdown","metadata":{"id":"t3E3wCS5b2sQ"},"source":["---\n","\n","## 3. HuggingFace Pipelines: La Forma M√°s Simple\n","\n","Los **pipelines** son abstracciones de alto nivel que encapsulan:\n","1. Tokenizaci√≥n (texto ‚Üí tokens)\n","2. Modelo (tokens ‚Üí predicciones)\n","3. Post-procesamiento (predicciones ‚Üí formato legible)\n","\n","### Pipelines disponibles:\n","\n","- `sentiment-analysis`: Clasificaci√≥n de sentimientos\n","- `ner`: Named Entity Recognition\n","- `question-answering`: Responder preguntas sobre un contexto\n","- `text-generation`: Generar texto (GPT)\n","- `translation`: Traducci√≥n\n","- `summarization`: Resumen de texto\n","- `fill-mask`: Completar texto enmascarado\n","- Y muchos m√°s...\n","\n","### Sintaxis b√°sica:\n","\n","```python\n","# Opci√≥n 1: Modelo por defecto (ingl√©s)\n","classifier = pipeline(\"sentiment-analysis\")\n","\n","# Opci√≥n 2: Modelo espec√≠fico\n","classifier = pipeline(\"sentiment-analysis\", model=\"finiteautomata/beto-sentiment-analysis\")\n","```"]},{"cell_type":"markdown","metadata":{"id":"4X1C3Nwqb2sS"},"source":["---\n","\n","## 4. Cargando BETO: BERT en Espa√±ol\n","\n","### ¬øQu√© es BETO?\n","\n","**BETO** (Spanish BERT) es un modelo transformer pre-entrenado en espa√±ol:\n","- Basado en BERT (Bidirectional Encoder Representations from Transformers)\n","- Pre-entrenado en Wikipedia en espa√±ol\n","- 110M par√°metros\n","- Desarrollado por Universidad de Chile\n","\n","### Variante que usaremos:\n","\n","`finiteautomata/beto-sentiment-analysis`:\n","- BETO fine-tuneado para an√°lisis de sentimientos\n","- Entrenado en tweets en espa√±ol\n","- 3 clases: POS (positivo), NEU (neutral), NEG (negativo)\n","- Estado del arte para espa√±ol"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKmQlxhMb2sT"},"outputs":[],"source":["print(\"Cargando modelo BETO...\")\n","print(\"La primera vez descargar√° ~400MB (modelo + tokenizer).\")\n","print(\"Esto puede tardar 1-2 minutos.\\n\")\n","\n","# Creamos el pipeline de sentiment analysis con BETO\n","# model: Especificamos el modelo del HuggingFace Model Hub\n","clasificador = pipeline(\n","    \"sentiment-analysis\",\n","    model=\"finiteautomata/beto-sentiment-analysis\"\n",")\n","\n","print(\"Modelo BETO cargado correctamente.\")\n","print(\"\\nInformaci√≥n del modelo:\")\n","print(f\"  Nombre: finiteautomata/beto-sentiment-analysis\")\n","print(f\"  Base: BETO (Spanish BERT)\")\n","print(f\"  Tarea: Sentiment Analysis\")\n","print(f\"  Idioma: Espa√±ol\")\n","print(f\"  Clases: POS, NEU, NEG\")"]},{"cell_type":"markdown","metadata":{"id":"_Jc6Mfjyb2sV"},"source":["---\n","\n","## 5. Primera Predicci√≥n: ¬°3 L√≠neas de C√≥digo!\n","\n","Vamos a clasificar una frase. Observ√° lo simple que es comparado con todo lo que programamos antes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXOJduVlb2sW"},"outputs":[],"source":["# Una sola frase de ejemplo\n","texto = \"Si queres morirte de calor, el lugar est√° b√°rbaro, muy recomendable.\"\n","\n","# Predicci√≥n (una l√≠nea)\n","resultado = clasificador(texto)\n","\n","# Mostramos el resultado\n","print(\"=\"*70)\n","print(\"PREDICCI√ìN CON TRANSFORMERS\")\n","print(\"=\"*70)\n","print(f\"\\nTexto: '{texto}'\")\n","print(f\"\\nResultado:\")\n","print(f\"  Sentimiento: {resultado[0]['label']}\")\n","print(f\"  Confianza: {resultado[0]['score']:.2%}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"Observaci√≥n: ¬°Esto fue mucho m√°s simple que todos los notebooks\")\n","print(\"anteriores juntos! El modelo ya viene entrenado y listo para usar.\")"]},{"cell_type":"markdown","metadata":{"id":"URpVOn2Nb2sX"},"source":["---\n","\n","## 6. Corpus de Prueba: Espa√±ol Rioplatense\n","\n","Usamos el mismo corpus de los notebooks anteriores para comparar resultados."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dz7rYyXmb2sY"},"outputs":[],"source":["# Corpus en espa√±ol rioplatense con expresiones coloquiales\n","frases = [\n","    # Positivas\n","    \"La verdad, este lugar est√° b√°rbaro. Muy recomendable.\",\n","    \"Qu√© buena onda la atenci√≥n, volver√≠a sin dudarlo.\",\n","    \"Me encant√≥ la comida, aunque la m√∫sica estaba muy fuerte.\",\n","    \"Todo excelente. Atenci√≥n de diez.\",\n","    \"Muy conforme con el resultado final.\",\n","    \"Super√≥ mis expectativas, gracias.\",\n","    \"El mejor asado que prob√© en mucho tiempo.\",\n","    \"Excelente relaci√≥n precio-calidad, muy recomendable.\",\n","    \"La atenci√≥n fue impecable, muy atentos.\",\n","    \"Me gust√≥ mucho el ambiente tranquilo.\",\n","\n","    # Negativas\n","    \"Una porquer√≠a de servicio, nunca m√°s vuelvo.\",\n","    \"El env√≠o fue lento y el producto lleg√≥ da√±ado. Qu√© desastre.\",\n","    \"Qu√© estafa, me arrepiento de haber comprado.\",\n","    \"No me gust√≥ para nada la experiencia.\",\n","    \"No lo recomiendo, mala calidad.\",\n","    \"Mal√≠sima atenci√≥n, el mozo ten√≠a mala onda.\",\n","    \"Tardaron dos horas en entregar, lleg√≥ todo fr√≠o.\",\n","    \"Me cobraron de m√°s y encima se hicieron los giles.\",\n","    \"La carne estaba pasada, casi no se pod√≠a comer.\",\n","    \"P√©sima experiencia, no vuelvo m√°s.\",\n","\n","    # Adicionales con expresiones argentinas\n","    \"Zafa, pero nada especial.\",\n","    \"Est√° piola el lugar, volver√≠a.\",\n","    \"Qu√© garr√≥n, tardaron una banda.\",\n","    \"Re copado todo, la rompieron.\",\n","    \"Un bodrio total, no vayan.\"\n","]\n","\n","print(f\"Corpus: {len(frases)} frases en espa√±ol rioplatense\")\n","print(f\"\\nEjemplos:\")\n","print(f\"  [Positiva] {frases[0]}\")\n","print(f\"  [Negativa] {frases[10]}\")\n","print(f\"  [Coloquial] {frases[20]}\")"]},{"cell_type":"markdown","metadata":{"id":"qX4u3R1Pb2sZ"},"source":["---\n","\n","## 7. Clasificaci√≥n Masiva con Pipelines\n","\n","Los pipelines pueden procesar listas de textos de forma eficiente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXTjPl71b2sa"},"outputs":[],"source":["print(\"Clasificando todas las frases con BETO...\\n\")\n","\n","# Clasificamos todas las frases de una vez\n","# El pipeline autom√°ticamente procesa en batches\n","resultados = clasificador(frases)\n","\n","print(\"=\"*70)\n","print(\"RESULTADOS DE CLASIFICACI√ìN\")\n","print(\"=\"*70)\n","\n","# Mostramos resultados detallados\n","for i, (frase, resultado) in enumerate(zip(frases, resultados), 1):\n","    label = resultado['label']\n","    score = resultado['score']\n","\n","    # Truncamos frase si es muy larga\n","    frase_corta = frase if len(frase) <= 55 else frase[:52] + \"...\"\n","\n","    # Formato de salida\n","    print(f\"\\n{i:2d}. '{frase_corta}'\")\n","    print(f\"    ‚Üí {label} (confianza: {score:.2%})\")\n","\n","print(\"\\n\" + \"=\"*70)"]},{"cell_type":"markdown","metadata":{"id":"450-ga4-b2sa"},"source":["---\n","\n","## 8. An√°lisis de Resultados\n","\n","Analicemos qu√© tan bien BETO maneja expresiones coloquiales argentinas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mkbABQjb2sa"},"outputs":[],"source":["# Extraemos las frases con expresiones muy argentinas\n","frases_argentinas = [\n","    (\"Zafa, pero nada especial.\", \"NEU o POS\"),  # Esperado\n","    (\"Est√° piola el lugar, volver√≠a.\", \"POS\"),\n","    (\"Qu√© garr√≥n, tardaron una banda.\", \"NEG\"),\n","    (\"Re copado todo, la rompieron.\", \"POS\"),\n","    (\"Un bodrio total, no vayan.\", \"NEG\"),\n","    (\"Qu√© buena onda la atenci√≥n.\", \"POS\"),\n","    (\"Mal√≠sima atenci√≥n, mala onda.\", \"NEG\"),\n","]\n","\n","print(\"=\"*70)\n","print(\"AN√ÅLISIS: EXPRESIONES ARGENTINAS\")\n","print(\"=\"*70)\n","print(\"\\n¬øBETO entiende jerga argentina?\\n\")\n","\n","# Clasificamos estas frases\n","resultados_arg = clasificador([frase for frase, _ in frases_argentinas])\n","\n","aciertos = 0\n","for (frase, esperado), resultado in zip(frases_argentinas, resultados_arg):\n","    predicho = resultado['label']\n","    score = resultado['score']\n","\n","    # Verificamos si coincide\n","    correcto = predicho in esperado\n","    marca = \"‚úì\" if correcto else \"?\"\n","    if correcto:\n","        aciertos += 1\n","\n","    print(f\"{marca} '{frase}'\")\n","    print(f\"  Esperado: {esperado} | Predicho: {predicho} ({score:.2%})\\n\")\n","\n","print(\"=\"*70)\n","print(f\"Aciertos: {aciertos}/{len(frases_argentinas)}\")\n","print(\"\\nObservaci√≥n: BETO fue entrenado en tweets en espa√±ol, por lo que\")\n","print(\"maneja bien expresiones coloquiales. Sin embargo, jerga muy espec√≠fica\")\n","print(\"de Argentina puede ser un desaf√≠o. Fine-tuning con datos locales mejorar√≠a esto.\")"]},{"cell_type":"markdown","metadata":{"id":"TCvrpJT7b2sb"},"source":["---\n","\n","## 9. Comparaci√≥n con Modelos Anteriores\n","\n","Comparemos conceptualmente lo que ganamos con transformers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SikQbD7sb2sc"},"outputs":[],"source":["print(\"=\"*70)\n","print(\"COMPARACI√ìN: MODELOS CONSTRUIDOS VS. TRANSFORMERS\")\n","print(\"=\"*70)\n","\n","comparacion = [\n","    (\"L√≠neas de c√≥digo\", \"~200-300 l√≠neas\", \"3 l√≠neas\"),\n","    (\"Tiempo de desarrollo\", \"Horas/d√≠as\", \"Minutos\"),\n","    (\"Datos requeridos\", \">1000 muestras\", \"0 (modelo ya entrenado)\"),\n","    (\"Tiempo de entrenamiento\", \"Minutos/horas\", \"0 (ya entrenado)\"),\n","    (\"Comprensi√≥n ling√º√≠stica\", \"Limitada\", \"Profunda (pre-entrenado)\"),\n","    (\"Manejo de jerga\", \"Depende del vocabulario\", \"Robusto\"),\n","    (\"Contexto\", \"Limitado (BoW/LSTM)\", \"Bidireccional (BERT)\"),\n","    (\"Par√°metros\", \"Decenas/cientos\", \"110 millones\"),\n","    (\"Accuracy esperado\", \"70-85%\", \"90-95%\"),\n","    (\"Fine-tuning\", \"N/A\", \"Posible con pocos datos\"),\n","    (\"Multiling√ºe\", \"Solo espa√±ol\", \"Soporte nativo\"),\n","    (\"Interpretabilidad\", \"Alta (pesos directos)\", \"Media (attention)\"),\n","    (\"Recursos computacionales\", \"CPU suficiente\", \"GPU recomendada\"),\n","    (\"Tama√±o del modelo\", \"KB\", \"400+ MB\"),\n","]\n","\n","print(f\"\\n{'Aspecto':<30} | {'Nuestros Modelos':<25} | {'Transformers (BETO)':<25}\")\n","print(\"-\"*85)\n","for aspecto, nuestros, transformers in comparacion:\n","    print(f\"{aspecto:<30} | {nuestros:<25} | {transformers:<25}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"CONCLUSI√ìN\")\n","print(\"=\"*70)\n","print(\"\\nLo que construimos en notebooks anteriores fue FUNDAMENTAL para:\")\n","print(\"  1. Entender c√≥mo funcionan las redes neuronales\")\n","print(\"  2. Apreciar la complejidad que HuggingFace abstrae\")\n","print(\"  3. Saber qu√© est√° pasando 'bajo el cap√≥'\")\n","print(\"  4. Debugging cuando algo falla\")\n","print(\"\\nPero para producci√≥n y proyectos reales:\")\n","print(\"  ‚Üí SIEMPRE usar transfer learning con modelos pre-entrenados\")\n","print(\"  ‚Üí HuggingFace es el est√°ndar de la industria\")\n","print(\"  ‚Üí No reinventar la rueda (a√±os-persona de investigaci√≥n)\")"]},{"cell_type":"markdown","metadata":{"id":"pkJsAYyMb2sc"},"source":["---\n","\n","## 10. Explorando Otros Modelos en Espa√±ol\n","\n","HuggingFace tiene m√∫ltiples modelos para espa√±ol. Veamos algunos."]},{"cell_type":"markdown","metadata":{"id":"WW-sXs3db2sd"},"source":["### 10.1. RoBERTuito: Especializado en Twitter Espa√±ol\n","\n","**RoBERTuito** es un modelo basado en RoBERTa entrenado en tweets en espa√±ol:\n","- Mejor manejo de lenguaje coloquial\n","- Comprende jerga, emojis, hashtags\n","- Ideal para redes sociales"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAX0T0D8b2sd"},"outputs":[],"source":["print(\"Cargando RoBERTuito...\\n\")\n","\n","# Cargamos pipeline con RoBERTuito\n","clasificador_twitter = pipeline(\n","    \"sentiment-analysis\",\n","    model=\"pysentimiento/robertuito-sentiment-analysis\"\n",")\n","\n","print(\"RoBERTuito cargado.\")\n","print(\"\\nProbando con frases estilo Twitter/redes sociales:\\n\")\n","\n","# Frases tipo tweet\n","tweets = [\n","    \"Jajaja re copado el lugar üòÇüëå\",\n","    \"Nooo qu√© garr√≥n mal servicio üò§\",\n","    \"10/10 recomendad√≠simo üî•\",\n","    \"Naaa una estafa total üëéüëé\",\n","]\n","\n","resultados_twitter = clasificador_twitter(tweets)\n","\n","for tweet, resultado in zip(tweets, resultados_twitter):\n","    print(f\"Tweet: '{tweet}'\")\n","    print(f\"  ‚Üí {resultado['label']} ({resultado['score']:.2%})\\n\")"]},{"cell_type":"markdown","metadata":{"id":"r8M4ldOEb2sd"},"source":["### 10.2. Comparaci√≥n BETO vs. RoBERTuito\n","\n","Comparemos ambos modelos en las mismas frases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJr46VSxb2se"},"outputs":[],"source":["print(\"=\"*70)\n","print(\"COMPARACI√ìN: BETO vs. RoBERTuito\")\n","print(\"=\"*70)\n","\n","frases_test = [\n","    \"Est√° piola el lugar, re tranquilo.\",\n","    \"Qu√© bodrio, nunca m√°s vuelvo.\",\n","    \"La comida estaba bien, nada del otro mundo.\",\n","]\n","\n","resultados_beto = clasificador(frases_test)\n","resultados_robertuito = clasificador_twitter(frases_test)\n","\n","for i, frase in enumerate(frases_test):\n","    print(f\"\\nFrase: '{frase}'\")\n","    print(f\"  BETO:       {resultados_beto[i]['label']:8s} ({resultados_beto[i]['score']:.2%})\")\n","    print(f\"  RoBERTuito: {resultados_robertuito[i]['label']:8s} ({resultados_robertuito[i]['score']:.2%})\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"Observaciones:\")\n","print(\"  - BETO: M√°s formal, entrenado en Wikipedia\")\n","print(\"  - RoBERTuito: Mejor con jerga y lenguaje informal\")\n","print(\"  - Elecci√≥n depende del dominio de tu aplicaci√≥n\")"]},{"cell_type":"markdown","metadata":{"id":"651q2FiQb2sg"},"source":["---\n","\n","## 11. Casos Dif√≠ciles: L√≠mites de los Modelos\n","\n","Incluso los transformers tienen l√≠mites. Veamos casos desafiantes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UfHIK-X3b2sg"},"outputs":[],"source":["print(\"=\"*70)\n","print(\"CASOS DIF√çCILES: IRON√çA, SARCASMO, AMBIG√úEDAD\")\n","print(\"=\"*70)\n","\n","casos_dificiles = [\n","    # Iron√≠a\n","    \"Buen√≠simo, justo lo que necesitaba, tardar 3 horas.\",\n","\n","    # Sarcasmo\n","    \"Claro, excelente idea cobrarme el doble.\",\n","\n","    # Sentimiento mixto\n","    \"La comida excelente pero el servicio p√©simo.\",\n","\n","    # Ambig√ºedad\n","    \"Interesante experiencia.\",\n","\n","    # Negaci√≥n doble\n","    \"No estuvo nada mal.\",\n","]\n","\n","print(\"\\nProbando BETO con casos desafiantes:\\n\")\n","\n","resultados_dificiles = clasificador(casos_dificiles)\n","\n","for caso, resultado in zip(casos_dificiles, resultados_dificiles):\n","    print(f\"Frase: '{caso}'\")\n","    print(f\"  BETO dice: {resultado['label']} ({resultado['score']:.2%})\")\n","    print(f\"  Nota: ¬øCaptur√≥ el matiz?\\n\")\n","\n","print(\"=\"*70)\n","print(\"Observaciones:\")\n","print(\"  - Iron√≠a/sarcasmo: Muy dif√≠cil incluso para humanos sin contexto\")\n","print(\"  - Sentimiento mixto: Los modelos devuelven una sola clase\")\n","print(\"  - Ambig√ºedad: Score bajo indica incertidumbre del modelo\")\n","print(\"\\nSoluciones:\")\n","print(\"  ‚Üí Fine-tuning con datos espec√≠ficos del dominio\")\n","print(\"  ‚Üí Modelos multiclase para sentimientos mixtos\")\n","print(\"  ‚Üí Features adicionales (emojis, contexto conversacional)\")"]},{"cell_type":"markdown","metadata":{"id":"_friITEcb2sg"},"source":["---\n","\n","## 12. Pr√≥ximos Pasos: Fine-tuning\n","\n","Lo que vimos en este notebook es **inferencia** (usar modelo ya entrenado). El siguiente paso es **fine-tuning**: adaptar el modelo a tu dataset espec√≠fico."]},{"cell_type":"markdown","metadata":{"id":"qFYayL9Zb2sh"},"source":["### ¬øCu√°ndo hacer fine-tuning?\n","\n","**Usar modelo out-of-the-box:**\n","- Tarea general (sentiment analysis est√°ndar)\n","- No ten√©s datos etiquetados\n","- Prototipado r√°pido\n","- Precisi√≥n suficiente (>85%)\n","\n","**Hacer fine-tuning:**\n","- Dominio muy espec√≠fico (medicina, legal, finanzas)\n","- Jerga particular (ej: argentinismos)\n","- Necesit√°s >90% accuracy\n","- Ten√©s dataset etiquetado (>500 muestras)\n","\n","### Proceso de fine-tuning (pr√≥ximas semanas del curso):\n","\n","```python\n","from transformers import AutoModelForSequenceClassification, Trainer\n","\n","# 1. Cargar modelo pre-entrenado\n","modelo = AutoModelForSequenceClassification.from_pretrained(\"dccuchile/bert-base-spanish-wwm-cased\")\n","\n","# 2. Preparar dataset\n","train_dataset = tokenize_and_encode(train_texts, train_labels)\n","eval_dataset = tokenize_and_encode(eval_texts, eval_labels)\n","\n","# 3. Configurar entrenamiento\n","trainer = Trainer(\n","    model=modelo,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n",")\n","\n","# 4. Fine-tune\n","trainer.train()\n","```\n","\n","**Esto lo ver√°n en detalle en las pr√≥ximas semanas del curso.**"]},{"cell_type":"markdown","metadata":{"id":"84D182s-b2sh"},"source":["---\n","\n","## Gu√≠a Te√≥rico-Conceptual\n","\n","### 1. Arquitectura Transformer: Conceptos Clave\n","\n","**Attention Mechanism (Self-Attention):**\n","\n","La innovaci√≥n central de los transformers. Permite que cada palabra \"preste atenci√≥n\" a todas las dem√°s:\n","\n","```\n","Frase: \"El banco est√° cerrado\"\n","\n","Self-attention permite que \"banco\" mire:\n","  - \"El\" ‚Üí Art√≠culo (bajo peso)\n","  - \"est√°\" ‚Üí Verbo estado (medio peso)\n","  - \"cerrado\" ‚Üí ¬°Alto peso! Disambigua \"banco\" (instituci√≥n vs. asiento)\n","```\n","\n","**F√≥rmula simplificada:**\n","$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n","\n","Donde:\n","- **Q (Query)**: \"¬øQu√© busco?\"\n","- **K (Key)**: \"¬øQu√© ofrezco?\"\n","- **V (Value)**: \"¬øQu√© informaci√≥n tengo?\"\n","\n","**Multi-Head Attention:**\n","- M√∫ltiples attention en paralelo\n","- Cada \"cabeza\" aprende patrones diferentes\n","- BERT tiene 12 capas √ó 12 cabezas = 144 atenciones\n","\n","**Ventajas sobre LSTM:**\n","1. **Paralelizaci√≥n**: Todas las palabras se procesan simult√°neamente\n","2. **Dependencias largas**: Cualquier palabra puede atender a cualquier otra\n","3. **Interpretabilidad**: Los pesos de attention son visualizables\n","\n","### 2. BERT vs. GPT: Encoder vs. Decoder\n","\n","| Aspecto | BERT | GPT |\n","|---------|------|-----|\n","| Arquitectura | Encoder (bidirectional) | Decoder (unidirectional) |\n","| Lectura | Lee toda la oraci√≥n | Lee izquierda a derecha |\n","| Pre-entrenamiento | Masked LM | Next token prediction |\n","| Mejor para | Clasificaci√≥n, NER, QA | Generaci√≥n de texto |\n","| Ejemplos | BERT, RoBERTa, BETO | GPT-2, GPT-3, GPT-4 |\n","\n","**Masked Language Modeling (BERT):**\n","```\n","Input:  \"El [MASK] est√° cerrado\"\n","Output: \"banco\" (con probabilidad 0.8)\n","```\n","\n","**Next Token Prediction (GPT):**\n","```\n","Input:  \"El banco est√°\"\n","Output: \"cerrado\" (con probabilidad 0.6)\n","```\n","\n","### 3. Tokenizaci√≥n en Transformers\n","\n","Los transformers usan **sub-word tokenization** (WordPiece, BPE):\n","\n","**Ventajas sobre tokenizaci√≥n por palabra:**\n","```\n","Palabra completa:\n","\"recomendabil√≠simo\" ‚Üí <UNK> (desconocida)\n","\n","WordPiece:\n","\"recomendabil√≠simo\" ‚Üí [\"recomienda\", \"##bili\", \"##simo\"]\n","```\n","\n","Esto permite:\n","- Vocabulario finito (30k tokens)\n","- Manejar palabras nuevas\n","- Capturar morfolog√≠a (prefijos, sufijos)\n","\n","### 4. Pre-entrenamiento: La Fase Cara\n","\n","**Corpus de pre-entrenamiento:**\n","- BERT (ingl√©s): Wikipedia (2.5B palabras) + BookCorpus (800M palabras)\n","- BETO (espa√±ol): Wikipedia espa√±ol (3B palabras)\n","- GPT-3: Common Crawl (570GB de texto)\n","\n","**Recursos computacionales:**\n","- BERT: 4 d√≠as en 16 TPUs (~$7k)\n","- GPT-3: Semanas en clusters masivos (~$12M)\n","\n","**Por esto NO entrenamos desde cero:**\n","- Costo prohibitivo\n","- Expertise t√©cnico alto\n","- Modelos ya disponibles\n","\n","### 5. Fine-tuning: La Fase Accesible\n","\n","**Proceso:**\n","1. **Congelar capas inferiores**: Mantener conocimiento ling√º√≠stico general\n","2. **Entrenar capas superiores**: Adaptar a tarea espec√≠fica\n","3. **Pocos epochs**: 2-5 t√≠picamente\n","4. **Learning rate bajo**: 1e-5 a 5e-5\n","\n","**Datos necesarios:**\n","- Clasificaci√≥n: 500-1000 muestras\n","- NER: 1000-5000 entidades\n","- QA: 500-2000 pares pregunta-respuesta\n","\n","**Tiempo:**\n","- GPU T4 (Google Colab): 30-60 minutos\n","- GPU A100: 5-10 minutos\n","\n","### 6. HuggingFace Model Hub\n","\n","**Estad√≠sticas (2025):**\n","- 500,000+ modelos\n","- 100+ idiomas\n","- Todas las arquitecturas: BERT, GPT, T5, LLaMA, etc.\n","\n","**Modelos destacados para espa√±ol:**\n","1. **BETO** (`dccuchile/bert-base-spanish-wwm-cased`)\n","   - BERT base en espa√±ol\n","   - 110M par√°metros\n","   - General purpose\n","\n","2. **RoBERTuito** (`pysentimiento/robertuito-*`)\n","   - RoBERTa en tweets espa√±ol\n","   - Jerga, emojis, hashtags\n","   - Mejor para redes sociales\n","\n","3. **MarIA** (`PlanTL-GOB-ES/roberta-base-bne`)\n","   - RoBERTa espa√±ol\n","   - Corpus BNE (Biblioteca Nacional)\n","   - Muy robusto\n","\n","4. **mBERT** (`bert-base-multilingual-cased`)\n","   - 104 idiomas incluyendo espa√±ol\n","   - √ötil para tareas multiling√ºes\n","\n","### 7. Limitaciones de Transformers\n","\n","**1. Longitud de secuencia:**\n","- BERT: M√°ximo 512 tokens\n","- Documentos largos requieren truncamiento o chunking\n","- Soluciones: Longformer, BigBird (hasta 4096 tokens)\n","\n","**2. Recursos computacionales:**\n","- Modelos grandes (400MB - 2GB)\n","- Inferencia m√°s lenta que modelos simples\n","- GPU recomendada para fine-tuning\n","\n","**3. Interpretabilidad:**\n","- 110M par√°metros son dif√≠ciles de interpretar\n","- Attention weights ayudan pero no explican todo\n","- \"Caja menos negra\" que antes, pero a√∫n opaco\n","\n","**4. Sesgos del pre-entrenamiento:**\n","- Heredan sesgos del corpus (Wikipedia, internet)\n","- Requiere cuidado en aplicaciones sensibles\n","- Fine-tuning puede amplificar sesgos\n","\n","**5. Overfitting en fine-tuning:**\n","- Con pocos datos, puede sobreajustar\n","- Requiere regularizaci√≥n, early stopping\n","- Data augmentation ayuda"]},{"cell_type":"markdown","metadata":{"id":"o6E85KlPb2si"},"source":["---\n","\n","## Preguntas y Respuestas para Estudio\n","\n","### Preguntas Conceptuales\n","\n","**1. ¬øQu√© es transfer learning en NLP y por qu√© es tan importante?**\n","\n","*Respuesta:* Transfer learning es usar un modelo pre-entrenado en un corpus masivo (Wikipedia, Common Crawl) y adaptarlo a una tarea espec√≠fica con pocos datos. Es importante porque:\n","1. El modelo ya \"entiende\" lenguaje (gram√°tica, sem√°ntica)\n","2. Solo necesit√°s 500-1000 muestras en vez de millones\n","3. Ahorras meses de entrenamiento y miles de d√≥lares\n","4. Obten√©s estado del arte sin expertise en deep learning\n","\n","**2. ¬øCu√°l es la principal innovaci√≥n de los transformers sobre LSTM?**\n","\n","*Respuesta:* **Self-attention mechanism**. En lugar de procesar secuencialmente (palabra por palabra), los transformers permiten que todas las palabras se \"vean\" entre s√≠ simult√°neamente. Esto:\n","- Permite paralelizaci√≥n (mucho m√°s r√°pido)\n","- Captura dependencias largas mejor\n","- Es m√°s interpretable (visualizar attention weights)\n","\n","**3. ¬øQu√© es un pipeline en HuggingFace y qu√© problema resuelve?**\n","\n","*Respuesta:* Un pipeline es una abstracci√≥n de alto nivel que encapsula:\n","1. **Tokenizaci√≥n**: Texto ‚Üí tokens\n","2. **Modelo**: Tokens ‚Üí embeddings ‚Üí predicciones\n","3. **Post-procesamiento**: Predicciones ‚Üí formato legible\n","\n","Resuelve el problema de complejidad: en lugar de manejar manualmente tokenizers, modelos y configuraciones, el pipeline lo hace en una l√≠nea de c√≥digo.\n","\n","**4. ¬øPor qu√© BERT es \"bidirectional\" y GPT es \"unidirectional\"?**\n","\n","*Respuesta:*\n","- **BERT**: Lee toda la oraci√≥n en ambas direcciones. En \"El banco est√° cerrado\", \"banco\" ve tanto \"El\" (izquierda) como \"cerrado\" (derecha)\n","- **GPT**: Solo lee de izquierda a derecha. En \"El banco est√°\", solo puede usar \"El banco\" para predecir \"est√°\"\n","\n","Por eso BERT es mejor para clasificaci√≥n/comprensi√≥n y GPT para generaci√≥n.\n","\n","**5. ¬øQu√© es Masked Language Modeling y por qu√© es efectivo para pre-entrenamiento?**\n","\n","*Respuesta:* MLM oculta aleatoriamente el 15% de las palabras y el modelo debe predecirlas: \"El [MASK] est√° cerrado\" ‚Üí \"banco\". Es efectivo porque:\n","- Fuerza al modelo a entender contexto bidireccional\n","- Es auto-supervisado (no requiere etiquetas humanas)\n","- Aprende representaciones profundas del lenguaje\n","\n","### Preguntas T√©cnicas\n","\n","**6. En el c√≥digo, ¬øqu√© hace exactamente `pipeline(\"sentiment-analysis\", model=\"...\")` internamente?**\n","\n","*Respuesta:*\n","1. Descarga el modelo y tokenizer desde HuggingFace Hub (si no est√° en cach√©)\n","2. Carga el modelo pre-entrenado en memoria\n","3. Configura el tokenizer apropiado (WordPiece para BERT)\n","4. Crea un pipeline que encadena: tokenizaci√≥n ‚Üí modelo ‚Üí argmax ‚Üí label\n","\n","**7. ¬øPor qu√© los modelos transformer son tan grandes (400MB+)?**\n","\n","*Respuesta:*\n","- BERT base: 110M par√°metros √ó 4 bytes (float32) = 440MB\n","- Cada par√°metro es un peso de la red neuronal\n","- 12 capas √ó 12 attention heads √ó embeddings √ó FFN = muchos par√°metros\n","\n","Comparado con nuestro MLP (280 par√°metros = 1KB), es ~400,000 veces m√°s grande.\n","\n","**8. ¬øQu√© significa el \"score\" en el resultado del pipeline?**\n","\n","*Respuesta:* Es la probabilidad (softmax) que el modelo asigna a esa clase:\n","- score=0.95 ‚Üí 95% seguro de la predicci√≥n\n","- score=0.55 ‚Üí Apenas seguro (predicci√≥n incierta)\n","\n","√ötil para:\n","- Filtrar predicciones inciertas (threshold > 0.7)\n","- Priorizar casos para revisi√≥n humana\n","\n","**9. ¬øCu√°l es la diferencia entre `model` y `pipeline` en HuggingFace?**\n","\n","*Respuesta:*\n","- **Pipeline**: API de alto nivel, maneja todo autom√°ticamente, f√°cil de usar\n","- **Model**: API de bajo nivel, control total, requiere manejar tokenizaci√≥n manualmente\n","\n","```python\n","# Pipeline (simple)\n","resultado = pipeline(\"sentiment-analysis\")(\"texto\")\n","\n","# Model (control manual)\n","tokenizer = AutoTokenizer.from_pretrained(\"beto\")\n","model = AutoModel.from_pretrained(\"beto\")\n","inputs = tokenizer(\"texto\", return_tensors=\"pt\")\n","outputs = model(**inputs)\n","```\n","\n","**10. ¬øPor qu√© BETO y RoBERTuito dan resultados diferentes en la misma frase?**\n","\n","*Respuesta:*\n","1. **Corpus de pre-entrenamiento diferente**:\n","   - BETO: Wikipedia (formal)\n","   - RoBERTuito: Twitter (informal)\n","2. **Fine-tuning diferente**: Diferentes datos de sentiment analysis\n","3. **Vocabulario**: RoBERTuito conoce mejor jerga, emojis\n","\n","La elecci√≥n depende del dominio de tu aplicaci√≥n.\n","\n","### Preguntas de Aplicaci√≥n\n","\n","**11. Ten√©s 200 rese√±as etiquetadas de un restaurante. ¬øUsar√≠as el modelo as-is o har√≠as fine-tuning?**\n","\n","*Respuesta:* **Depende:**\n","- **As-is si**: El modelo out-of-the-box ya da >85% accuracy, el dominio es general\n","- **Fine-tuning si**:\n","  - Necesit√°s >90% accuracy\n","  - Hay vocabulario muy espec√≠fico del restaurante\n","  - 200 muestras son borderline (m√≠nimo recomendado es 500), pero pod√©s intentar con data augmentation\n","\n","**12. ¬øC√≥mo manejar√≠as un documento de 2000 palabras con BERT (l√≠mite: 512 tokens)?**\n","\n","*Respuesta:* Opciones:\n","1. **Truncamiento**: Tomar primeros 512 tokens (pierde info del final)\n","2. **Chunking + agregaci√≥n**: Dividir en chunks de 512, clasificar cada uno, agregar (voting, promedio)\n","3. **Extractivo**: Identificar secci√≥n clave (ej: resumen, intro) y clasificar eso\n","4. **Modelo especializado**: Usar Longformer o BigBird (hasta 4096 tokens)\n","\n","**13. En producci√≥n, necesit√°s clasificar 10,000 textos/segundo. ¬øTransformers son apropiados?**\n","\n","*Respuesta:* Probablemente **no** para latencia ultra-baja:\n","- BERT inference: ~50-100ms/texto en GPU\n","- 10k/seg = 0.1ms/texto (100x m√°s r√°pido requerido)\n","\n","**Soluciones:**\n","1. **Distillation**: DistilBERT (60% m√°s r√°pido, 97% accuracy)\n","2. **Quantization**: INT8 en vez de FP32 (4x m√°s r√°pido)\n","3. **Caching**: Cachear predicciones para textos comunes\n","4. **Ensemble**: Modelo simple (Naive Bayes) para mayor√≠a, BERT solo para casos inciertos\n","\n","**14. ¬øC√≥mo usar√≠as HuggingFace para un chatbot que genera respuestas?**\n","\n","*Respuesta:*\n","```python\n","# Usar modelo generativo (GPT)\n","from transformers import pipeline\n","\n","generador = pipeline(\"text-generation\", model=\"gpt2\")\n","\n","respuesta = generador(\n","    \"Usuario: Hola, ¬øc√≥mo est√°s?\\nBot:\",\n","    max_length=50,\n","    num_return_sequences=1\n",")\n","```\n","\n","Para espa√±ol, usar modelos como `DeepESP/gpt2-spanish`.\n","\n","**15. Dise√±√° un sistema de moderaci√≥n de contenido con transformers.**\n","\n","*Respuesta:*\n","```python\n","# Pipeline multimodelo\n","\n","# 1. Clasificaci√≥n de toxicidad\n","toxicidad = pipeline(\"text-classification\", model=\"modelo-toxicidad-es\")\n","\n","# 2. Detecci√≥n de spam\n","spam = pipeline(\"text-classification\", model=\"modelo-spam-es\")\n","\n","# 3. NER para detectar info personal\n","ner = pipeline(\"ner\", model=\"modelo-ner-es\")\n","\n","def moderar(texto):\n","    # An√°lisis paralelo\n","    es_toxico = toxicidad(texto)[0]['score'] > 0.7\n","    es_spam = spam(texto)[0]['score'] > 0.8\n","    tiene_pii = any(ent['entity'] == 'PER' for ent in ner(texto))\n","    \n","    if es_toxico or es_spam:\n","        return \"RECHAZAR\"\n","    elif tiene_pii:\n","        return \"ADVERTIR\"\n","    else:\n","        return \"APROBAR\"\n","```"]},{"cell_type":"markdown","metadata":{"id":"i6YNhdgpb2sk"},"source":["---\n","\n","## Ejercicios Propuestos\n","\n","### Ejercicio 1: Explorar el Model Hub\n","Visita https://huggingface.co/models y busca modelos para:\n","- Sentiment analysis en espa√±ol\n","- NER en espa√±ol\n","- Traducci√≥n es‚Üíen\n","\n","Prob√° al menos 3 modelos diferentes y compar√° resultados.\n","\n","### Ejercicio 2: An√°lisis de Confianza\n","Filtr√° las predicciones del modelo para mostrar solo aquellas con score > 0.8. ¬øCu√°ntas frases quedan? ¬øLas predicciones de baja confianza tienen algo en com√∫n?\n","\n","### Ejercicio 3: Comparaci√≥n Cuantitativa\n","Eval√∫a BETO vs. RoBERTuito en un conjunto de 50 frases etiquetadas manualmente:\n","- Calcula accuracy, precision, recall\n","- Identifica en qu√© tipo de frases cada modelo es mejor\n","\n","### Ejercicio 4: NER con Transformers\n","Usa un pipeline de NER para extraer entidades de texto:\n","```python\n","ner = pipeline(\"ner\", model=\"mrm8488/bert-spanish-cased-finetuned-ner\")\n","```\n","Prob√° con textos sobre geograf√≠a, personas famosas, organizaciones.\n","\n","### Ejercicio 5: L√≠mites del Modelo\n","Crea 10 frases dise√±adas para \"enga√±ar\" al modelo (iron√≠a, sarcasmo, negaciones dobles). ¬øQu√© porcentaje clasifica incorrectamente? ¬øVes patrones en los errores?\n","\n","---\n","\n","## Conclusi√≥n\n","\n","Este notebook marca el **fin del m√≥dulo introductorio** y el **inicio del programa principal** del curso.\n","\n","### Lo que aprendimos:\n","\n","1. **Transfer learning**: El paradigma moderno de NLP\n","2. **HuggingFace Transformers**: La librer√≠a est√°ndar de la industria\n","3. **Pipelines**: Abstracciones de alto nivel para inferencia\n","4. **Modelos en espa√±ol**: BETO, RoBERTuito, MarIA\n","5. **Limitaciones**: Casos dif√≠ciles, sesgos, recursos\n","\n","### El viaje hasta aqu√≠:\n","\n","```\n","Notebook 1-2: Baselines cl√°sicos (sklearn, Naive Bayes)\n","     ‚Üì\n","Notebook 3: Fundamentos (Perceptr√≥n desde cero)\n","     ‚Üì\n","Notebook 4: Redes profundas (MLP con PyTorch)\n","     ‚Üì\n","Notebook 5: Secuencias (LSTM con Keras)\n","     ‚Üì\n","Notebook 6: Estado del arte (Transformers con HuggingFace) ‚Üê EST√ÅS AQU√ç\n","```\n","\n","### ¬øQu√© sigue en el curso?\n","\n","**Semanas 1-3: Fundamentos de transformers**\n","- Arquitectura en detalle\n","- Tokenizaci√≥n avanzada\n","- Preparaci√≥n de datos\n","\n","**Semanas 4-5: Fine-tuning**\n","- Clasificaci√≥n binaria y multiclase\n","- Optimizaci√≥n de hiperpar√°metros\n","- Evaluaci√≥n rigurosa\n","\n","**Semana 6: NER y token classification**\n","\n","**Semanas 7-8: Modelos generativos**\n","- GPT para generaci√≥n\n","- Summarization\n","- Question Answering\n","\n","**Semanas 9-11: Proyecto integrador**\n","\n","### Reflexi√≥n final\n","\n","Los 5 notebooks anteriores fueron **fundamentales** para entender c√≥mo funcionan las redes neuronales. Ahora que vimos la potencia de HuggingFace, apreci√°s:\n","\n","- El valor de abstracciones bien dise√±adas\n","- Por qu√© transfer learning revolucion√≥ NLP\n","- Que \"hacer deep learning\" hoy es usar herramientas correctamente, no programar todo desde cero\n","\n","Pero tambi√©n entend√©s **qu√© est√° pasando bajo el cap√≥**, lo cual te diferencia de alguien que solo sabe usar pipelines sin comprensi√≥n profunda.\n","\n","**Pr√≥ximo paso:** El Notebook 7 (Embeddings Sem√°nticos) es material complementario para proyectos avanzados. Luego contin√∫a con el programa principal del curso.\n","\n","---\n","\n","*Este material fue desarrollado con fines educativos para la Tecnicatura en Ciencia de Datos del IFTS.*"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}