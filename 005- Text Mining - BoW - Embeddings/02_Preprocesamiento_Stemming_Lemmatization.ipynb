{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_NHYb-fQu3N"
      },
      "source": [
        "# Cuaderno Clase PLN - Preprocesamiento Avanzado\n",
        "\n",
        "**Objetivos:**\n",
        "*   Repasar la limpieza básica de texto.\n",
        "*   Entender y aplicar Stemming (NLTK).\n",
        "*   Entender y aplicar Lematización (spaCy).\n",
        "*   Comparar los resultados de ambas técnicas.\n",
        "*   Reflexionar sobre el impacto del preprocesamiento.\n",
        "\n",
        "**Agenda:**\n",
        "\n",
        "1.  Instalaciones e Importaciones\n",
        "2.  Repaso: Limpieza básica y Tokenización\n",
        "3.  El problema de las variantes de palabras\n",
        "4.  Stemming con NLTK\n",
        "5.  Lematización con spaCy\n",
        "6.  Comparación Stemming vs. Lematización\n",
        "7.  Micro-Laboratorio (Ejercicio Práctico)\n",
        "8.  Brainstorming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu4TosgXReRP"
      },
      "source": [
        "# 1. Instalaciones e Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CyxzGvIRdxg"
      },
      "outputs": [],
      "source": [
        "# Instalar librerías (si no están ya en Colab)\n",
        "!pip install nltk spacy > /dev/null\n",
        "!python -m spacy download es_core_news_sm > /dev/null # Modelo pequeño de español para spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNOnXGQsRrII"
      },
      "outputs": [],
      "source": [
        "# Importar librerías\n",
        "import nltk\n",
        "import spacy\n",
        "import re # Para expresiones regulares (limpieza)\n",
        "import string # Para signos de puntuación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "176_JS8VR2FR",
        "outputId": "efa7668f-6902-479f-b9ca-2ba539a728ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oqkr1o2SCab",
        "outputId": "dcd5757a-0825-4fb3-8585-d3fe77a8b533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de spaCy 'es_core_news_sm' cargado.\n"
          ]
        }
      ],
      "source": [
        "# Cargar modelo de spaCy en español\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "print(\"Modelo de spaCy 'es_core_news_sm' cargado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHvRsNlySKbH"
      },
      "outputs": [],
      "source": [
        "# Cargar stopwords en español de NLTK\n",
        "stopwords_es = nltk.corpus.stopwords.words('spanish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yq3GXkdSNqh",
        "outputId": "f0657d2d-92e1-41aa-c226-db900135915f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ejemplo de stopwords en español (NLTK): ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con']...\n"
          ]
        }
      ],
      "source": [
        "# Añadir algunas stopwords comunes si es necesario (opcional)\n",
        "#stopwords_es.extend(['tan', 'van', 'ser', 'haber', 'ir'])\n",
        "print(f\"\\nEjemplo de stopwords en español (NLTK): {stopwords_es[:15]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P1d3ILUlAC-"
      },
      "source": [
        "# 2. Repaso: Limpieza básica y Tokenización\n",
        "\n",
        "Recordemos los pasos comunes que ya vimos:\n",
        "*   **Pasar a minúsculas:** Para tratar \"Hola\" y \"hola\" igual.\n",
        "*   **Quitar números:** A menudo no aportan significado general.\n",
        "*   **Quitar signos de puntuación:** Como ',', '.', '!', '?'.\n",
        "*   **Quitar stopwords:** Palabras muy comunes (\"el\", \"la\", \"de\", \"que\", \"y\"...) que aparecen mucho pero no suelen distinguir el tema del texto.\n",
        "*   **Tokenización:** Dividir el texto en unidades (palabras o \"tokens\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbQyV_bJlKHe"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de texto\n",
        "texto_ejemplo = \"Los niños corrían rápidamente por el parque, jugando y riendo. ¡Qué día más lindo!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DONcWdH2lNpj"
      },
      "outputs": [],
      "source": [
        "# Función simple de limpieza y tokenización (usando NLTK para stopwords y tokenización)\n",
        "def limpiar_tokenizar_basico(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Quitar números (usando expresiones regulares)\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación\n",
        "  texto = texto.translate(str.maketrans('', '', string.punctuation + '¡¿'))\n",
        "  # 4. Quitar espacios extra\n",
        "  texto = texto.strip()\n",
        "  # 5. Tokenizar\n",
        "  tokens = nltk.word_tokenize(texto, language='spanish')\n",
        "  # 6. Quitar stopwords\n",
        "  tokens_limpios = [palabra for palabra in tokens if palabra not in stopwords_es]\n",
        "  return tokens_limpios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV-6TnQMlxTj",
        "outputId": "7afa118c-dccd-40d5-e64e-9918b2a421fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Los niños corrían rápidamente por el parque, jugando y riendo. ¡Qué día más lindo!\n",
            "Tokens después de limpieza básica y quitar stopwords (NLTK): ['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']\n"
          ]
        }
      ],
      "source": [
        "# Aplicar la función\n",
        "tokens_basicos = limpiar_tokenizar_basico(texto_ejemplo)\n",
        "print(\"Texto original:\", texto_ejemplo)\n",
        "print(\"Tokens después de limpieza básica y quitar stopwords (NLTK):\", tokens_basicos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcrJG67umD0T"
      },
      "source": [
        "# 3. El problema de las variantes de palabras\n",
        "\n",
        "Observen los tokens resultantes: `['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']`.\n",
        "\n",
        "Tenemos \"corrían\", \"jugando\", \"riendo\". Si tuviéramos otro texto con \"correr\", \"juega\", \"reír\", serían tokens diferentes.\n",
        "\n",
        "**¿No sería útil agrupar las palabras que comparten una raíz o significado base?**\n",
        "\n",
        "*   **corrían, correr, corremos, corrió -> CORRER**\n",
        "*   **jugando, juega, jugamos -> JUGAR**\n",
        "\n",
        "Esto ayuda a:\n",
        "*   Reducir el tamaño del vocabulario (menos columnas en BoW/TF-IDF).\n",
        "*   Agrupar conceptos similares.\n",
        "\n",
        "Dos técnicas principales para esto: **Stemming** y **Lematización**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1n0lY4-mQLN"
      },
      "source": [
        "# 4. Stemming con NLTK\n",
        "\n",
        "*   **¿Qué es?** Un proceso **heurístico** (basado en reglas simples) para cortar el final de las palabras y obtener su \"raíz\" o \"stem\".\n",
        "*   **No siempre produce una palabra real** del diccionario.\n",
        "*   **Ventajas:** Rápido, simple, reduce mucho el vocabulario.\n",
        "*   **Desventajas:** A veces \"corta\" demasiado o agrupa palabras incorrectamente. No considera el contexto gramatical.\n",
        "*   **Herramienta:** NLTK tiene `SnowballStemmer` para español."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCrfk44pmEUx"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Crear un stemmer para español\n",
        "stemmer = SnowballStemmer('spanish')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P_DAqeImfYk",
        "outputId": "b8c6cf83-00c8-4be8-9a5e-cb3c7c46634e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens originales (limpios): ['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']\n",
            "Stems resultantes (NLTK):  ['niñ', 'corr', 'rapid', 'parqu', 'jug', 'riend', 'dia', 'lind']\n"
          ]
        }
      ],
      "source": [
        "# Vamos a aplicar stemming a los tokens que obtuvimos antes (después de limpieza básica)\n",
        "stems_nltk = [stemmer.stem(token) for token in tokens_basicos]\n",
        "\n",
        "print(\"Tokens originales (limpios):\", tokens_basicos)\n",
        "print(\"Stems resultantes (NLTK): \", stems_nltk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLFpz2X_mbJK",
        "outputId": "7e53b2f0-a890-4554-aedc-02b8a11894b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming para ['correr', 'corría', 'corriendo', 'corredor', 'corredores']: ['corr', 'corr', 'corr', 'corredor', 'corredor']\n"
          ]
        }
      ],
      "source": [
        "# Probemos con otras palabras relacionadas\n",
        "palabras_relacionadas = ['correr', 'corría', 'corriendo', 'corredor', 'corredores']\n",
        "stems_relacionadas = [stemmer.stem(p) for p in palabras_relacionadas]\n",
        "print(f\"\\nStemming para {palabras_relacionadas}: {stems_relacionadas}\") # Notar que agrupa bien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HopByEBZmlkU",
        "outputId": "d1be423d-15b6-4868-ab7d-a9228ea30789"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming para ['computadora', 'computación', 'computar']: ['comput', 'comput', 'comput']\n"
          ]
        }
      ],
      "source": [
        "palabras_problematicas = ['computadora', 'computación', 'computar']\n",
        "stems_problematicos = [stemmer.stem(p) for p in palabras_problematicas]\n",
        "print(f\"Stemming para {palabras_problematicas}: {stems_problematicos}\") # Funciona razonable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyNFRHInmm7u",
        "outputId": "85651e23-c7cf-426f-98b3-db017c8ca8f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming para ['universidad', 'universo']: ['univers', 'univers']\n"
          ]
        }
      ],
      "source": [
        "palabras_problematicas_2 = ['universidad', 'universo']\n",
        "stems_problematicos_2 = [stemmer.stem(p) for p in palabras_problematicas_2]\n",
        "print(f\"Stemming para {palabras_problematicas_2}: {stems_problematicos_2}\") # ¡Ojo! Puede agrupar de más"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0JXR2rRnIa-"
      },
      "source": [
        "# 5. Lematización con spaCy\n",
        "\n",
        "*   **¿Qué es?** Un proceso más **lingüístico**, basado en diccionarios y análisis morfológico, para encontrar la forma canónica o de diccionario de una palabra (su \"lema\").\n",
        "*   **Produce palabras reales**.\n",
        "*   **Ventajas:** Más preciso conceptualmente, mejor para análisis semántico.\n",
        "*   **Desventajas:** Más lento computacionalmente, requiere modelos lingüísticos (como los de spaCy).\n",
        "*   **Herramienta:** spaCy lo hace automáticamente al procesar el texto con un modelo cargado (`nlp()`). El lema está en el atributo `token.lemma_`. spaCy también identifica stopwords (`token.is_stop`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RVojUlInWZ8"
      },
      "outputs": [],
      "source": [
        "# Usaremos spaCy directamente sobre el texto original limpio (sin quitar stopwords aún)\n",
        "# porque spaCy necesita el contexto para lematizar bien.\n",
        "\n",
        "def limpiar_texto_spacy(texto):\n",
        "  # 1. Minúsculas\n",
        "  texto = texto.lower()\n",
        "  # 2. Quitar números\n",
        "  texto = re.sub(r'\\d+', '', texto)\n",
        "  # 3. Quitar puntuación (dejamos espacios)\n",
        "  texto = texto.translate(str.maketrans(string.punctuation + '¡¿', ' ' * len(string.punctuation + '¡¿')))\n",
        "  # 4. Quitar espacios extra\n",
        "  texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "  return texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4lS9v95ndWF",
        "outputId": "a73fd9ec-88d8-4711-956f-60b77da06e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto limpio para spaCy: los niños corrían rápidamente por el parque jugando y riendo qué día más lindo\n"
          ]
        }
      ],
      "source": [
        "texto_limpio_spacy = limpiar_texto_spacy(texto_ejemplo)\n",
        "print(\"Texto limpio para spaCy:\", texto_limpio_spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "448foMWQnitz"
      },
      "outputs": [],
      "source": [
        "# Procesar el texto limpio con spaCy\n",
        "doc = nlp(texto_limpio_spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz4UMjZJnkaq",
        "outputId": "edd6f246-8496-4ede-e69b-986f080bf695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemas resultantes (spaCy, filtrando stopwords y no alfabéticos): ['niño', 'correr', 'rápidamente', 'parque', 'jugar', 'reir', 'lindo']\n"
          ]
        }
      ],
      "source": [
        "# Obtener los lemas, filtrando stopwords y tokens no alfabéticos\n",
        "lemas_spacy = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "\n",
        "print(\"\\nLemas resultantes (spaCy, filtrando stopwords y no alfabéticos):\", lemas_spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_COplXgnugY",
        "outputId": "6a2b575a-a337-437d-9cbe-477a1d774756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemas para correr corría corriendo corredor corredores: ['correr', 'correr', 'correr', 'corredor', 'corredor']\n"
          ]
        }
      ],
      "source": [
        "# Veamos los lemas de las palabras relacionadas\n",
        "doc_relacionadas = nlp(\"correr corría corriendo corredor corredores\")\n",
        "lemas_relacionadas_spacy = [token.lemma_ for token in doc_relacionadas]\n",
        "print(f\"\\nLemas para {' '.join([t.text for t in doc_relacionadas])}: {lemas_relacionadas_spacy}\") # ¡Excelente! \"corredor\" es distinto de \"correr\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC98_51nnJhi",
        "outputId": "e63ae8da-632b-4565-f8f0-e35f7f9ad44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemas para computadora computación computar: ['computadoro', 'computación', 'computar']\n"
          ]
        }
      ],
      "source": [
        "doc_problematicas = nlp(\"computadora computación computar\")\n",
        "lemas_problematicas_spacy = [token.lemma_ for token in doc_problematicas]\n",
        "print(f\"Lemas para {' '.join([t.text for t in doc_problematicas])}: {lemas_problematicas_spacy}\") # \"computación\" se mantiene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1NJHViin78w",
        "outputId": "0fd2b11f-b662-40ee-ec51-62ff79ff9d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemas para universidad universo: ['universidad', 'universo']\n"
          ]
        }
      ],
      "source": [
        "doc_problematicas_2 = nlp(\"universidad universo\")\n",
        "lemas_problematicas_2_spacy = [token.lemma_ for token in doc_problematicas_2]\n",
        "print(f\"Lemas para {' '.join([t.text for t in doc_problematicas_2])}: {lemas_problematicas_2_spacy}\") # Correctamente separados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Py5ywppoQ-_"
      },
      "source": [
        "# 6. Comparación Stemming vs. Lematización\n",
        "\n",
        "Veamos lado a lado los resultados para nuestro texto de ejemplo:\n",
        "\n",
        "*   **Tokens originales (limpios):** `['niños', 'corrían', 'rápidamente', 'parque', 'jugando', 'riendo', 'día', 'lindo']`\n",
        "*   **Stems (NLTK):** `['niñ', 'corr', 'rapid', 'parqu', 'jug', 'riend', 'dia', 'lind']`\n",
        "*   **Lemas (spaCy):** `['niño', 'correr', 'rápidamente', 'parque', 'jugar', 'reír', 'día', 'lindo']`\n",
        "\n",
        "**Observaciones:**\n",
        "*   Los lemas son palabras reales, los stems no siempre.\n",
        "*   La lematización parece capturar mejor la forma base (\"correr\", \"jugar\", \"reír\").\n",
        "*   El stemming es más agresivo (\"niñ\", \"corr\", \"rapid\").\n",
        "*   Ambos acortan \"día\" (sin tilde) de forma similar en este caso (NLTK por stem, spaCy porque el modelo puede no tener la tilde en su lema base).\n",
        "\n",
        "**¿Cuándo usar cuál?**\n",
        "*   **Stemming:** Cuando la velocidad es crucial y no importa tanto la interpretabilidad (ej: recuperación de información a gran escala).\n",
        "*   **Lematización:** Cuando la precisión semántica y la interpretabilidad son importantes (ej: análisis de sentimiento, clasificación de temas, chatbots). **Generalmente preferido si los recursos computacionales lo permiten.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMhtHWA5oovZ"
      },
      "source": [
        "# 7. Micro-Laboratorio (Ejercicio Práctico)\n",
        "\n",
        "**Consigna:**\n",
        "\n",
        "Dado el siguiente conjunto de frases (reviews de películas):\n",
        "1.  Definir una función `preprocesar_nltk(texto)` que:\n",
        "    *   Limpie el texto (minúsculas, números, puntuación).\n",
        "    *   Tokenice.\n",
        "    *   Quite stopwords (usando la lista de NLTK).\n",
        "    *   Aplique Stemming (con `SnowballStemmer`).\n",
        "    *   Devuelva la lista de stems.\n",
        "2.  Definir una función `preprocesar_spacy(texto)` que:\n",
        "    *   Limpie el texto (minúsculas, números, puntuación - cuidado con no quitar espacios necesarios para spaCy).\n",
        "    *   Procese el texto con `nlp()`.\n",
        "    *   Devuelva la lista de lemas de los tokens que no sean stopwords (`token.is_stop`) y sean alfabéticos (`token.is_alpha`).\n",
        "3.  Aplicar ambas funciones a cada frase del dataset `reviews`.\n",
        "4.  Imprimir los resultados de ambas funciones para cada frase, uno debajo del otro, para poder comparar.\n",
        "5.  **Observar:** ¿Qué diferencias notables encuentran? ¿En qué casos un método parece funcionar \"mejor\" que el otro?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57JZ3yh6oSDV"
      },
      "outputs": [],
      "source": [
        "# Dataset para el ejercicio\n",
        "reviews = [\n",
        "    \"Una película emocionante con actuaciones brillantes. ¡Me encantó!\",\n",
        "    \"Muy aburrida y lenta. El guión era predecible y los actores no convencían.\",\n",
        "    \"Los efectos especiales fueron impresionantes, pero la historia dejaba mucho que desear.\",\n",
        "    \"¡Qué gran comedia! Me reí sin parar durante toda la película.\",\n",
        "    \"Un documental necesario que aborda temas importantes con profundidad y sensibilidad.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av-E8AxOpFUP"
      },
      "source": [
        "# 8. Brainstorming\n",
        "\n",
        "Ahora que conocemos estas técnicas, pensemos:\n",
        "\n",
        "**¿Cómo podemos preprocesar el texto de manera que se eviten sesgos y discriminaciones?**\n",
        "\n",
        "*   ¿Qué pasa si la lista de `stopwords` que usamos (sea de NLTK o spaCy) quita palabras importantes para un grupo minoritario o en un contexto específico (ej: jerga, términos culturales)?\n",
        "*   ¿Los stemmers o lematizadores funcionan igual de bien con diferentes dialectos del español o con lenguaje inclusivo? (Probablemente no, los modelos estándar están entrenados en textos más \"formales\").\n",
        "*   Si quitamos nombres propios o entidades, ¿podríamos estar eliminando información crucial sobre representación?\n",
        "*   Al elegir agresividad (stemming) vs. precisión (lematización), ¿podríamos afectar diferencialmente el análisis de textos de distintos grupos?\n",
        "*   ¿Qué responsabilidad tenemos al elegir y aplicar estas técnicas? ¿Deberíamos documentar siempre nuestras decisiones de preprocesamiento?\n",
        "\n",
        "**(Discusión en grupo)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKBXPxy-pImh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GUÍA DE ESTUDIO - PREPROCESAMIENTO AVANZADO\n",
        "\n",
        "## Preguntas y Respuestas Clave\n",
        "\n",
        "### **Conceptos Fundamentales**\n",
        "\n",
        "**P: ¿Cuál es la diferencia principal entre stemming y lematización?**  \n",
        "R: Stemming corta palabras usando reglas heurísticas (rápido, impreciso). Lematización encuentra la forma de diccionario usando análisis lingüístico (lento, preciso).\n",
        "\n",
        "**P: ¿Por qué necesitamos reducir variantes de palabras?**  \n",
        "R: Para agrupar conceptos similares (\"correr\", \"corriendo\", \"corrió\") y reducir el tamaño del vocabulario sin perder significado esencial.\n",
        "\n",
        "**P: ¿Qué produce mejor resultados para español: stemming o lematización?**  \n",
        "R: Lematización, porque el español es morfológicamente rico y los lemas mantienen interpretabilidad.\n",
        "\n",
        "### **Implementación Técnica**\n",
        "\n",
        "**P: ¿Cuándo usar SnowballStemmer de NLTK?**  \n",
        "R: Cuando necesitas velocidad extrema, tienes recursos limitados, o haces análisis exploratorio inicial.\n",
        "\n",
        "**P: ¿Por qué spaCy necesita contexto para lematizar?**  \n",
        "R: Porque la misma palabra puede tener diferentes lemas según su función gramatical en la oración.\n",
        "\n",
        "**P: ¿Qué ventaja tiene procesar texto completo vs tokens individuales?**  \n",
        "R: El contexto permite al modelo distinguir homónimos y asignar categorías gramaticales correctas.\n",
        "\n",
        "### **Casos Prácticos**\n",
        "\n",
        "**P: ¿Qué problemas puede causar stemming agresivo?**  \n",
        "R: Agrupa palabras no relacionadas (\"universidad\" y \"universo\" → \"univers\") o produce raíces no interpretables.\n",
        "\n",
        "**P: ¿Cuándo preferirías stemming sobre lematización?**  \n",
        "R: En sistemas de búsqueda masiva, cuando la interpretabilidad no es crítica, o con recursos computacionales muy limitados.\n",
        "\n",
        "### **Consideraciones para Español Argentino**\n",
        "\n",
        "**P: ¿Cómo afecta el lunfardo al preprocesamiento?**  \n",
        "R: Los modelos estándar pueden no reconocer argentinismos, requiriendo vocabularios personalizados o modelos entrenados localmente.\n",
        "\n",
        "**P: ¿Qué hacer con lenguaje inclusivo (\"estudiantx\")?**  \n",
        "R: Considerar reglas de preprocesamiento específicas o modelos actualizados que incluyan estas variantes.\n",
        "\n",
        "## Puntos Clave para Recordar\n",
        "\n",
        "1. **Para español: lematización > stemming** por riqueza morfológica\n",
        "2. **spaCy integra múltiples funciones** (lematización + stopwords + POS tags)\n",
        "3. **Contexto es crucial** para preprocesamiento preciso\n",
        "4. **Stemming: velocidad. Lematización: precisión**\n",
        "5. **Decisiones de preprocesamiento afectan** todos los análisis posteriores\n",
        "\n",
        "## Errores Comunes a Evitar\n",
        "\n",
        "- Usar stemming sin evaluar impacto en interpretabilidad\n",
        "- Procesar tokens individuales cuando se necesita contexto\n",
        "- Ignorar dialectos locales en modelos de preprocesamiento\n",
        "- No documentar decisiones de preprocesamiento\n",
        "- Aplicar misma técnica sin considerar tipo de texto\n",
        "\n",
        "## Conexión con Próxima Clase\n",
        "\n",
        "Un texto bien preprocesado es la base para embeddings de calidad. La próxima clase mostrará cómo estos tokens lematizados se convierten en **vectores semánticos** que capturan significado.\n",
        "\n",
        "---\n",
        "*Consejo: Experimenta con tu propio texto en español. ¿Qué diferencias observas entre stems y lemas? ¿Cuál preserva mejor el significado?*"
      ],
      "metadata": {
        "id": "WuwklYPzV01n"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}