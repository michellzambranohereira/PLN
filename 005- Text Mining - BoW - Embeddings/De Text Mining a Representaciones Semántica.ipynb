{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-header",
      "metadata": {
        "id": "intro-header"
      },
      "source": [
        "# De Text Mining a Representaciones Sem√°nticas: Un Puente Conceptual\n",
        "\n",
        "**Tecnicatura Superior en Ciencias de Datos e Inteligencia Artificial**  \n",
        "**Procesamiento de Lenguaje Natural**\n",
        "\n",
        "---\n",
        "\n",
        "## Introducci√≥n: Construyendo sobre Fundamentos S√≥lidos\n",
        "\n",
        "En el laboratorio anterior trabajaron intensamente con los cuentos de Hern√°n Casciari, aplicando t√©cnicas fundamentales de **text mining**: preprocesamiento, tokenizaci√≥n, eliminaci√≥n de stop words, vectorizaci√≥n con **Bag of Words** (BoW), an√°lisis de frecuencias, y visualizaci√≥n con nubes de palabras. Fue un trabajo riguroso que les permiti√≥ extraer patrones temporales reales de un corpus aut√©ntico.\n",
        "\n",
        "Ese laboratorio no fue solo un ejercicio t√©cnico: fue una exploraci√≥n profunda de c√≥mo la vida de un escritor se refleja en su vocabulario a lo largo del tiempo. Descubrieron la evoluci√≥n tem√°tica de Casciari, desde los primeros a√±os centrados en experiencias personales hasta la consolidaci√≥n de Orsai como proyecto editorial. Aprendieron a manejar matrices esparsas, a filtrar ruido textual, y a extraer informaci√≥n significativa de datos no estructurados.\n",
        "\n",
        "**Hoy, sin embargo, vamos a hacer algo diferente.** En lugar de celebrar √∫nicamente los √©xitos de esa experiencia, vamos a examinar cr√≠ticamente sus limitaciones. No para desvalorizar lo que hicieron, sino para entender por qu√© necesitamos herramientas m√°s sofisticadas para capturar la riqueza sem√°ntica del lenguaje.\n",
        "\n",
        "### Objetivo de Esta Clase\n",
        "\n",
        "Esta clase funciona como un **puente conceptual** entre el text mining tradicional que ya dominan y las representaciones sem√°nticas que explorar√°n en la pr√°ctica del jueves. Nuestros objetivos espec√≠ficos son:\n",
        "\n",
        "1. **Identificar y analizar las limitaciones sem√°nticas** de BoW y TF-IDF que experimentaron con Casciari\n",
        "2. **Introducir el concepto de similitud sem√°ntica** y demostrar por qu√© es crucial para NLP moderno\n",
        "3. **Experimentar con vectores sem√°nticos densos** usando spaCy como primer acercamiento\n",
        "4. **Preparar el terreno conceptual** para word embeddings, Word2Vec y modelos similares\n",
        "5. **Desarrollar intuici√≥n** sobre por qu√© las representaciones sem√°nticas revolucionaron el campo\n",
        "\n",
        "### Metodolog√≠a: Aprendizaje por Contraste\n",
        "\n",
        "Utilizaremos una metodolog√≠a de **aprendizaje por contraste**: confrontaremos directamente las limitaciones de BoW/TF-IDF con las capacidades de representaciones sem√°nticas. Usaremos ejemplos concretos, muchos extra√≠dos del propio corpus de Casciari, para hacer visibles problemas que quiz√°s sintieron intuitivamente pero no pudieron articular formalmente.\n",
        "\n",
        "No se trata de reemplazar conocimiento, sino de **expandirlo y profundizarlo**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reconexion",
      "metadata": {
        "id": "reconexion"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Reconexi√≥n: El Corpus Casciari como Punto de Partida\n",
        "\n",
        "### Recordando lo que Lograron\n",
        "\n",
        "Antes de avanzar, reconozcamos la solidez del trabajo que realizaron con el corpus de Casciari. Su an√°lisis revel√≥ patrones temporales fascinantes:\n",
        "\n",
        "- **Evoluci√≥n tem√°tica**: El vocabulario de 2004-2007 (primeros a√±os de paternidad) difer√≠a claramente del de 2010-2011 (a√±os de Orsai)\n",
        "- **Diversidad l√©xica**: Identificaron a√±os con mayor riqueza vocabular y conectaron esos patrones con eventos biogr√°ficos\n",
        "- **Palabras caracter√≠sticas**: Descubrieron t√©rminos que defin√≠an per√≠odos espec√≠ficos (\"revista\", \"orsai\", \"papelitos\")\n",
        "- **T√©cnicas profesionales**: Manejaron matrices de 12√ó29,683 dimensiones, optimizaron stop words, y crearon visualizaciones informativas\n",
        "\n",
        "Este trabajo les dio experiencia pr√°ctica con los fundamentos del NLP y una comprensi√≥n visceral de c√≥mo funciona la vectorizaci√≥n de texto en escenarios reales.\n",
        "\n",
        "### Un Dataset Familiar para Nuevos Experimentos\n",
        "\n",
        "Hoy volveremos a usar elementos del corpus Casciari, pero con un prop√≥sito diferente: como laboratorio para explorar las limitaciones de nuestras herramientas actuales y la necesidad de enfoques m√°s sofisticados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup",
        "outputId": "70f17c70-afd9-4e05-b15b-b80b3a2f8efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entorno configurado. Iniciando exploraci√≥n cr√≠tica de text mining.\n"
          ]
        }
      ],
      "source": [
        "# Configuraci√≥n inicial del entorno\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"Entorno configurado. Iniciando exploraci√≥n cr√≠tica de text mining.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "casciari-examples",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "casciari-examples",
        "outputId": "4a5a1792-b8f0-4c3c-b938-e8f4dfae7c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparados 8 fragmentos tem√°ticos para an√°lisis\n",
            "\n",
            "Categor√≠as disponibles:\n",
            "  - paternidad_temprana\n",
            "  - paternidad_madura\n",
            "  - futbol_pasion\n",
            "  - futbol_nostalgia\n",
            "  - orsai_nacimiento\n",
            "  - orsai_consolidacion\n",
            "  - escritura_oficio\n",
            "  - escritura_arte\n"
          ]
        }
      ],
      "source": [
        "# Recreamos algunos ejemplos inspirados en el corpus Casciari para nuestros experimentos\n",
        "# Estos fragmentos capturan el estilo y las tem√°ticas que encontraron en su an√°lisis\n",
        "\n",
        "textos_casciari_inspirados = {\n",
        "    \"paternidad_temprana\": \"\"\"La nena se despert√≥ a las tres de la ma√±ana llorando.\n",
        "    No sab√≠a qu√© hacer, as√≠ que la alc√© y camin√© por la casa hasta que se calm√≥.\n",
        "    Ser padre es lo m√°s dif√≠cil que me ha tocado hacer en la vida.\"\"\",\n",
        "\n",
        "    \"paternidad_madura\": \"\"\"Mi hija se levant√≥ temprano y me prepar√≥ el desayuno.\n",
        "    Ya no es una beb√© que llora por las noches, ahora es una persona\n",
        "    que tiene sus propias opiniones sobre todo.\"\"\",\n",
        "\n",
        "    \"futbol_pasion\": \"\"\"El partido fue incre√≠ble. San Lorenzo jug√≥ como nunca\n",
        "    y la hinchada cant√≥ durante los noventa minutos.\n",
        "    El f√∫tbol argentino tiene esa magia que no encuentro en ning√∫n otro lugar.\"\"\",\n",
        "\n",
        "    \"futbol_nostalgia\": \"\"\"Viendo el encuentro por televisi√≥n record√© cuando iba\n",
        "    a la cancha con mi viejo. La pasi√≥n azulgrana corr√≠a por nuestras venas\n",
        "    y cada gol era una fiesta familiar.\"\"\",\n",
        "\n",
        "    \"orsai_nacimiento\": \"\"\"La idea de la revista surgi√≥ casi por casualidad.\n",
        "    Quer√≠a hacer algo diferente, contar historias que no encontraba\n",
        "    en otros medios. Los lectores respondieron de manera sorprendente.\"\"\",\n",
        "\n",
        "    \"orsai_consolidacion\": \"\"\"La publicaci√≥n ya tiene su identidad propia.\n",
        "    Los escritores que colaboran entienden el esp√≠ritu del proyecto\n",
        "    y cada n√∫mero es mejor que el anterior.\"\"\",\n",
        "\n",
        "    \"escritura_oficio\": \"\"\"Escribir es mi trabajo, pero tambi√©n mi obsesi√≥n.\n",
        "    Cada texto es una oportunidad de conectar con los lectores\n",
        "    de una manera aut√©ntica y directa.\"\"\",\n",
        "\n",
        "    \"escritura_arte\": \"\"\"Las palabras son mi herramienta para crear mundos.\n",
        "    Cada relato es una invitaci√≥n a los lectores para que\n",
        "    construyan sus propias interpretaciones.\"\"\"\n",
        "}\n",
        "\n",
        "# Convertir a DataFrame para facilitar manipulaci√≥n\n",
        "df_ejemplos = pd.DataFrame(list(textos_casciari_inspirados.items()),\n",
        "                          columns=['categoria', 'texto'])\n",
        "\n",
        "print(f\"Preparados {len(df_ejemplos)} fragmentos tem√°ticos para an√°lisis\")\n",
        "print(\"\\nCategor√≠as disponibles:\")\n",
        "for cat in df_ejemplos['categoria'].unique():\n",
        "    print(f\"  - {cat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "limitaciones-bow",
      "metadata": {
        "id": "limitaciones-bow"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Las Limitaciones que Experimentaron (Sin Saberlo)\n",
        "\n",
        "### Problema 1: La Ceguera Sem√°ntica de BoW\n",
        "\n",
        "Cuando analizaron el corpus de Casciari con Bag of Words, el modelo trataba cada palabra como una entidad completamente independiente. **\"Padre\" y \"pap√°\" eran tan diferentes entre s√≠ como \"padre\" y \"computadora\".**\n",
        "\n",
        "Esto significa que si Casciari usaba \"padre\" en 2004 y \"pap√°\" en 2008 para referirse al mismo concepto, su an√°lisis de frecuencias no captaba esa continuidad tem√°tica. La riqueza sem√°ntica del lenguaje humano quedaba invisible.\n",
        "\n",
        "Veamos este problema en acci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "semantica-bow",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "semantica-bow",
        "outputId": "32e30cb2-6875-43e8-c653-4efd4d9a8a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPRESENTACI√ìN BoW DE TRES FRASES SEM√ÅNTICAMENTE SIMILARES:\n",
            "=================================================================\n",
            "         al  chico  cuando  c√≥mo  de  del  el  ense√±√≥  era  explic√≥  f√∫tbol  \\\n",
            "Frase 1   1      1       1     0   0    0   0       1    1        0       1   \n",
            "Frase 2   0      0       0     1   1    0   0       0    0        0       0   \n",
            "Frase 3   0      0       0     0   0    1   1       0    0        1       0   \n",
            "\n",
            "         juego  jugar  la  las  me  mi  mostr√≥  ni√±o  padre  pap√°  patear  \\\n",
            "Frase 1      0      1   0    0   1   1       0     0      0     1       0   \n",
            "Frase 2      0      0   1    0   1   1       1     1      1     0       1   \n",
            "Frase 3      1      0   0    1   1   0       0     0      0     0       0   \n",
            "\n",
            "         pelota  peque√±o  reglas  siendo  viejo  \n",
            "Frase 1       0        0       0       0      0  \n",
            "Frase 2       1        0       0       0      0  \n",
            "Frase 3       0        1       1       1      1  \n",
            "\n",
            "üí≠ OBSERVACI√ìN CR√çTICA:\n",
            "Estas tres frases hablan del mismo recuerdo (el padre ense√±ando f√∫tbol),\n",
            "pero BoW las ve como completamente diferentes porque no comparten palabras.\n",
            "\n",
            "SIMILITUD COSENO ENTRE FRASES (seg√∫n BoW):\n",
            "Frase 1 vs Frase 2: 0.200\n",
            "Frase 1 vs Frase 3: 0.100\n",
            "Frase 2 vs Frase 3: 0.100\n",
            "\n",
            "‚ö†Ô∏è  Similitud = 0 significa que BoW considera estas frases completamente diferentes\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo concreto: sin√≥nimos que BoW trata como palabras independientes\n",
        "ejemplos_sinonimos = [\n",
        "    \"Mi pap√° me ense√±√≥ a jugar al f√∫tbol cuando era chico\",\n",
        "    \"Mi padre me mostr√≥ c√≥mo patear la pelota de ni√±o\",\n",
        "    \"El viejo me explic√≥ las reglas del juego siendo peque√±o\"\n",
        "]\n",
        "\n",
        "# Vectorizaci√≥n con BoW\n",
        "vectorizer_bow = CountVectorizer()\n",
        "matriz_bow = vectorizer_bow.fit_transform(ejemplos_sinonimos)\n",
        "feature_names = vectorizer_bow.get_feature_names_out()\n",
        "\n",
        "# Crear DataFrame para visualizar\n",
        "df_bow = pd.DataFrame(matriz_bow.toarray(), columns=feature_names)\n",
        "df_bow.index = ['Frase 1', 'Frase 2', 'Frase 3']\n",
        "\n",
        "print(\"REPRESENTACI√ìN BoW DE TRES FRASES SEM√ÅNTICAMENTE SIMILARES:\")\n",
        "print(\"=\" * 65)\n",
        "print(df_bow)\n",
        "print(\"\\nüí≠ OBSERVACI√ìN CR√çTICA:\")\n",
        "print(\"Estas tres frases hablan del mismo recuerdo (el padre ense√±ando f√∫tbol),\")\n",
        "print(\"pero BoW las ve como completamente diferentes porque no comparten palabras.\")\n",
        "\n",
        "# Calcular similitud coseno\n",
        "similitudes = cosine_similarity(matriz_bow)\n",
        "print(\"\\nSIMILITUD COSENO ENTRE FRASES (seg√∫n BoW):\")\n",
        "print(f\"Frase 1 vs Frase 2: {similitudes[0,1]:.3f}\")\n",
        "print(f\"Frase 1 vs Frase 3: {similitudes[0,2]:.3f}\")\n",
        "print(f\"Frase 2 vs Frase 3: {similitudes[1,2]:.3f}\")\n",
        "print(\"\\n‚ö†Ô∏è  Similitud = 0 significa que BoW considera estas frases completamente diferentes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "problema-contexto",
      "metadata": {
        "id": "problema-contexto"
      },
      "source": [
        "### Problema 2: P√©rdida del Contexto Sem√°ntico\n",
        "\n",
        "En su an√°lisis de Casciari probablemente notaron palabras como \"tiempo\" apareciendo con alta frecuencia. Pero BoW no distingue entre:\n",
        "\n",
        "- \"No tengo **tiempo** para escribir\" (recurso escaso)\n",
        "- \"El **tiempo** pasa r√°pido cuando juego con mi hija\" (experiencia subjetiva)\n",
        "- \"En **tiempo** de Navidad todo cambia\" (per√≠odo espec√≠fico)\n",
        "\n",
        "Para BoW, estos son simplemente tres ocurrencias de la palabra \"tiempo\", pero sem√°nticamente representan conceptos diferentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polisemia",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "polisemia",
        "outputId": "68350d31-7e74-4878-f19b-54f617386df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AN√ÅLISIS DE POLISEMIA: LA PALABRA 'TIEMPO'\n",
            "=============================================\n",
            "\n",
            "Frecuencia de 'tiempo' en cada frase:\n",
            "Frase 1: 1 ocurrencia(s)\n",
            "  Contexto: No tengo tiempo para escribir hoy, estoy muy ocupa...\n",
            "Frase 2: 1 ocurrencia(s)\n",
            "  Contexto: El tiempo vuela cuando estoy con mi familia...\n",
            "Frase 3: 1 ocurrencia(s)\n",
            "  Contexto: En tiempo de pandemia cambi√≥ nuestra forma de vivi...\n",
            "Frase 4: 1 ocurrencia(s)\n",
            "  Contexto: Hace tiempo que no veo una pel√≠cula tan buena...\n",
            "Frase 5: 1 ocurrencia(s)\n",
            "  Contexto: El tiempo atmosf√©rico est√° muy cambiante esta sema...\n",
            "\n",
            "üí≠ REFLEXI√ìN CR√çTICA:\n",
            "BoW cuenta 5 ocurrencias de 'tiempo', pero cada una tiene un significado diferente:\n",
            "  ‚Ä¢ Tiempo como recurso (frase 1)\n",
            "  ‚Ä¢ Tiempo como experiencia subjetiva (frase 2)\n",
            "  ‚Ä¢ Tiempo como per√≠odo hist√≥rico (frase 3)\n",
            "  ‚Ä¢ Tiempo como duraci√≥n (frase 4)\n",
            "  ‚Ä¢ Tiempo como condici√≥n clim√°tica (frase 5)\n",
            "\n",
            "‚ö†Ô∏è  BoW/TF-IDF no puede distinguir entre estos usos sem√°nticamente diferentes\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de polisemia: una palabra, m√∫ltiples significados\n",
        "frases_tiempo = [\n",
        "    \"No tengo tiempo para escribir hoy, estoy muy ocupado\",\n",
        "    \"El tiempo vuela cuando estoy con mi familia\",\n",
        "    \"En tiempo de pandemia cambi√≥ nuestra forma de vivir\",\n",
        "    \"Hace tiempo que no veo una pel√≠cula tan buena\",\n",
        "    \"El tiempo atmosf√©rico est√° muy cambiante esta semana\"\n",
        "]\n",
        "\n",
        "vectorizer_tiempo = CountVectorizer()\n",
        "matriz_tiempo = vectorizer_tiempo.fit_transform(frases_tiempo)\n",
        "\n",
        "# Buscar la columna correspondiente a \"tiempo\"\n",
        "feature_names_tiempo = vectorizer_tiempo.get_feature_names_out()\n",
        "indice_tiempo = np.where(feature_names_tiempo == 'tiempo')[0][0]\n",
        "\n",
        "print(\"AN√ÅLISIS DE POLISEMIA: LA PALABRA 'TIEMPO'\")\n",
        "print(\"=\" * 45)\n",
        "print(\"\\nFrecuencia de 'tiempo' en cada frase:\")\n",
        "for i, frase in enumerate(frases_tiempo):\n",
        "    freq = matriz_tiempo[i, indice_tiempo]\n",
        "    print(f\"Frase {i+1}: {freq} ocurrencia(s)\")\n",
        "    print(f\"  Contexto: {frase[:50]}...\")\n",
        "\n",
        "print(\"\\nüí≠ REFLEXI√ìN CR√çTICA:\")\n",
        "print(\"BoW cuenta 5 ocurrencias de 'tiempo', pero cada una tiene un significado diferente:\")\n",
        "print(\"  ‚Ä¢ Tiempo como recurso (frase 1)\")\n",
        "print(\"  ‚Ä¢ Tiempo como experiencia subjetiva (frase 2)\")\n",
        "print(\"  ‚Ä¢ Tiempo como per√≠odo hist√≥rico (frase 3)\")\n",
        "print(\"  ‚Ä¢ Tiempo como duraci√≥n (frase 4)\")\n",
        "print(\"  ‚Ä¢ Tiempo como condici√≥n clim√°tica (frase 5)\")\n",
        "print(\"\\n‚ö†Ô∏è  BoW/TF-IDF no puede distinguir entre estos usos sem√°nticamente diferentes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "problema-relaciones",
      "metadata": {
        "id": "problema-relaciones"
      },
      "source": [
        "### Problema 3: Incapacidad para Capturar Relaciones Conceptuales\n",
        "\n",
        "Su an√°lisis de Casciari mostr√≥ la evoluci√≥n de temas familiares a lo largo del tiempo. Sin embargo, BoW no puede entender que:\n",
        "\n",
        "- **\"beb√©\" ‚Üí \"nena\" ‚Üí \"hija\" ‚Üí \"adolescente\"** representan la misma persona en diferentes etapas\n",
        "- **\"San Lorenzo\" ‚Üí \"azulgrana\" ‚Üí \"hinchada\" ‚Üí \"cancha\"** forman un campo sem√°ntico relacionado con f√∫tbol\n",
        "- **\"escribir\" ‚Üí \"texto\" ‚Üí \"relato\" ‚Üí \"historia\"** est√°n conceptualmente conectados\n",
        "\n",
        "Esta limitaci√≥n es crucial porque significa que patrones sem√°nticos profundos permanecen invisibles en el an√°lisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relaciones-conceptuales",
      "metadata": {
        "id": "relaciones-conceptuales"
      },
      "outputs": [],
      "source": [
        "# Demostraci√≥n: campos sem√°nticos que BoW no puede relacionar\n",
        "campos_semanticos = {\n",
        "    \"F√∫tbol (expl√≠cito)\": \"El partido de f√∫tbol fue emocionante, San Lorenzo gan√≥\",\n",
        "    \"F√∫tbol (impl√≠cito 1)\": \"La hinchada cant√≥ durante todo el encuentro azulgrana\",\n",
        "    \"F√∫tbol (impl√≠cito 2)\": \"El Cicl√≥n mostr√≥ gran juego y la tribuna se volvi√≥ loca\",\n",
        "    \"Escritura (expl√≠cito)\": \"Escribir es mi pasi√≥n, cada texto es una aventura\",\n",
        "    \"Escritura (impl√≠cito 1)\": \"La narraci√≥n flu√≠a y las palabras cobraban vida\",\n",
        "    \"Escritura (impl√≠cito 2)\": \"El relato cautiv√≥ a los lectores desde el primer p√°rrafo\"\n",
        "}\n",
        "\n",
        "textos_campos = list(campos_semanticos.values())\n",
        "labels_campos = list(campos_semanticos.keys())\n",
        "\n",
        "# Vectorizaci√≥n\n",
        "vectorizer_campos = TfidfVectorizer()\n",
        "matriz_campos = vectorizer_campos.fit_transform(textos_campos)\n",
        "\n",
        "# Calcular similitudes\n",
        "similitudes_campos = cosine_similarity(matriz_campos)\n",
        "\n",
        "print(\"SIMILITUD ENTRE TEXTOS DE CAMPOS SEM√ÅNTICOS RELACIONADOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Comparar textos dentro del mismo campo sem√°ntico\n",
        "print(\"\\nüèà CAMPO SEM√ÅNTICO: F√öTBOL\")\n",
        "print(f\"Expl√≠cito vs Impl√≠cito 1: {similitudes_campos[0,1]:.3f}\")\n",
        "print(f\"Expl√≠cito vs Impl√≠cito 2: {similitudes_campos[0,2]:.3f}\")\n",
        "print(f\"Impl√≠cito 1 vs Impl√≠cito 2: {similitudes_campos[1,2]:.3f}\")\n",
        "\n",
        "print(\"\\n‚úçÔ∏è  CAMPO SEM√ÅNTICO: ESCRITURA\")\n",
        "print(f\"Expl√≠cito vs Impl√≠cito 1: {similitudes_campos[3,4]:.3f}\")\n",
        "print(f\"Expl√≠cito vs Impl√≠cito 2: {similitudes_campos[3,5]:.3f}\")\n",
        "print(f\"Impl√≠cito 1 vs Impl√≠cito 2: {similitudes_campos[4,5]:.3f}\")\n",
        "\n",
        "print(\"\\nüîÑ COMPARACI√ìN ENTRE CAMPOS (debe ser baja):\")\n",
        "print(f\"F√∫tbol expl√≠cito vs Escritura expl√≠cita: {similitudes_campos[0,3]:.3f}\")\n",
        "\n",
        "print(\"\\nüí≠ AN√ÅLISIS CR√çTICO:\")\n",
        "print(\"- Las similitudes dentro de cada campo son bajas porque no comparten palabras exactas\")\n",
        "print(\"- TF-IDF no reconoce que 'hinchada' y 'tribuna' se refieren al mismo concepto\")\n",
        "print(\"- No entiende que 'relato', 'narraci√≥n' y 'texto' est√°n sem√°nticamente relacionados\")\n",
        "print(\"- Patrones tem√°ticos profundos quedan invisibles en el an√°lisis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "problema-sinonimos",
      "metadata": {
        "id": "problema-sinonimos"
      },
      "source": [
        "### Problema 4: La Tragedia de los Sin√≥nimos\n",
        "\n",
        "Este es quiz√°s el problema m√°s frustrante que experimentaron sin darse cuenta. Casciari, como buen escritor, usa sin√≥nimos para enriquecer su prosa. Pero para BoW/TF-IDF:\n",
        "\n",
        "- **\"hermoso\" y \"bello\"** son palabras completamente diferentes\n",
        "- **\"auto\" y \"coche\"** no tienen relaci√≥n alguna  \n",
        "- **\"enojado\" y \"furioso\"** podr√≠an estar en galaxias diferentes\n",
        "\n",
        "Esto significa que su an√°lisis de frecuencias fragment√≥ artificialmente conceptos que deber√≠an estar unidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tragedia-sinonimos",
      "metadata": {
        "id": "tragedia-sinonimos"
      },
      "outputs": [],
      "source": [
        "# Ejemplo dram√°tico: la fragmentaci√≥n de conceptos por sin√≥nimos\n",
        "textos_sinonimos = [\n",
        "    \"El paisaje era hermoso, me qued√© contemplando la vista\",\n",
        "    \"La vista era bella, no pod√≠a dejar de admirar el panorama\",\n",
        "    \"Qu√© lindo lugar, la perspectiva desde aqu√≠ es preciosa\",\n",
        "    \"Este sitio es precioso, la panor√°mica es realmente bonita\"\n",
        "]\n",
        "\n",
        "# An√°lisis con TF-IDF\n",
        "vectorizer_sin = TfidfVectorizer()\n",
        "matriz_sin = vectorizer_sin.fit_transform(textos_sinonimos)\n",
        "feature_names_sin = vectorizer_sin.get_feature_names_out()\n",
        "\n",
        "# Crear DataFrame para an√°lisis\n",
        "df_sinonimos = pd.DataFrame(matriz_sin.toarray(), columns=feature_names_sin)\n",
        "df_sinonimos.index = [f'Descripci√≥n {i+1}' for i in range(len(textos_sinonimos))]\n",
        "\n",
        "print(\"AN√ÅLISIS DE SIN√ìNIMOS: CONCEPTOS FRAGMENTADOS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Identificar palabras relacionadas con belleza\n",
        "palabras_belleza = ['hermoso', 'bella', 'lindo', 'preciosa', 'precioso', 'bonita']\n",
        "palabras_vista = ['vista', 'panorama', 'perspectiva', 'panor√°mica']\n",
        "\n",
        "print(\"\\nüé® PALABRAS DE BELLEZA EN EL CORPUS:\")\n",
        "for palabra in palabras_belleza:\n",
        "    if palabra in feature_names_sin:\n",
        "        idx = np.where(feature_names_sin == palabra)[0][0]\n",
        "        ocurrencias = df_sinonimos.iloc[:, idx].values\n",
        "        frases_con_palabra = [i for i, val in enumerate(ocurrencias) if val > 0]\n",
        "        print(f\"'{palabra}': aparece en descripci√≥n(es) {[f+1 for f in frases_con_palabra]}\")\n",
        "\n",
        "print(\"\\nüî≠ PALABRAS DE PERSPECTIVA VISUAL:\")\n",
        "for palabra in palabras_vista:\n",
        "    if palabra in feature_names_sin:\n",
        "        idx = np.where(feature_names_sin == palabra)[0][0]\n",
        "        ocurrencias = df_sinonimos.iloc[:, idx].values\n",
        "        frases_con_palabra = [i for i, val in enumerate(ocurrencias) if val > 0]\n",
        "        print(f\"'{palabra}': aparece en descripci√≥n(es) {[f+1 for f in frases_con_palabra]}\")\n",
        "\n",
        "# Calcular similitudes\n",
        "similitudes_sin = cosine_similarity(matriz_sin)\n",
        "\n",
        "print(\"\\nüìä SIMILITUDES ENTRE DESCRIPCIONES:\")\n",
        "for i in range(len(textos_sinonimos)):\n",
        "    for j in range(i+1, len(textos_sinonimos)):\n",
        "        sim = similitudes_sin[i,j]\n",
        "        print(f\"Descripci√≥n {i+1} vs {j+1}: {sim:.3f}\")\n",
        "\n",
        "print(\"\\nüí≠ TRAGEDIA DE LOS SIN√ìNIMOS:\")\n",
        "print(\"- Las 4 descripciones hablan del mismo concepto: un lugar hermoso con buena vista\")\n",
        "print(\"- Pero TF-IDF las ve como textos diferentes porque usan sin√≥nimos\")\n",
        "print(\"- 'Hermoso', 'bello', 'lindo', 'precioso' expresan la misma idea\")\n",
        "print(\"- 'Vista', 'panorama', 'perspectiva' se refieren al mismo elemento\")\n",
        "print(\"- La riqueza l√©xica del espa√±ol se convierte en una limitaci√≥n t√©cnica\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "transicion-semantica",
      "metadata": {
        "id": "transicion-semantica"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Hacia la Sem√°ntica Computacional: Una Nueva Perspectiva\n",
        "\n",
        "### El Salto Conceptual Necesario\n",
        "\n",
        "Los problemas que acabamos de identificar no son fallas de implementaci√≥n que pueden corregirse con mejores algoritmos de preprocesamiento o stop words m√°s sofisticadas. **Son limitaciones fundamentales de la representaci√≥n bag-of-words.**\n",
        "\n",
        "Para superarlas, necesitamos un cambio de paradigma: pasar de representaciones basadas en **co-ocurrencia de palabras exactas** a representaciones que capturen **significado sem√°ntico**.\n",
        "\n",
        "### ¬øQu√© Significa \"Sem√°ntica\" en Contexto Computacional?\n",
        "\n",
        "En el contexto de NLP, cuando hablamos de **sem√°ntica** nos referimos a la capacidad de:\n",
        "\n",
        "1. **Reconocer sin√≥nimos**: \"auto\" y \"coche\" deben tener representaciones similares\n",
        "2. **Capturar relaciones conceptuales**: \"m√©dico\" debe estar m√°s cerca de \"hospital\" que de \"computadora\"\n",
        "3. **Entender contexto**: distinguir \"banco\" (instituci√≥n) de \"banco\" (asiento)\n",
        "4. **Detectar analog√≠as**: si \"rey\" es a \"reina\" como \"hombre\" es a \"mujer\"\n",
        "\n",
        "### Introducci√≥n a los Vectores Sem√°nticos Densos\n",
        "\n",
        "La soluci√≥n viene de representar palabras como **vectores densos** en un espacio multidimensional, donde:\n",
        "\n",
        "- **Palabras similares** est√°n **cerca** en el espacio vectorial\n",
        "- **Relaciones sem√°nticas** se preservan como **relaciones geom√©tricas**\n",
        "- **El contexto** determina la posici√≥n de cada palabra\n",
        "- **Las dimensiones** capturan aspectos abstractos del significado\n",
        "\n",
        "Experimentemos con esta idea usando spaCy, que incluye vectores pre-entrenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-spacy",
      "metadata": {
        "id": "setup-spacy"
      },
      "outputs": [],
      "source": [
        "# Instalaci√≥n y configuraci√≥n de spaCy\n",
        "# Si es la primera vez que lo usan, necesitar√°n instalar el modelo en espa√±ol\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    # Intentar cargar el modelo en espa√±ol\n",
        "    nlp = spacy.load(\"es_core_news_md\")\n",
        "    print(\"‚úÖ spaCy y modelo espa√±ol cargados correctamente\")\n",
        "except OSError:\n",
        "    print(\"‚ö†Ô∏è  Instalando modelo de spaCy en espa√±ol...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_md\"])\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"es_core_news_md\")\n",
        "    print(\"‚úÖ spaCy instalado y configurado\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è  Instalando spaCy...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"spacy\"])\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_md\"])\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"es_core_news_md\")\n",
        "    print(\"‚úÖ spaCy instalado desde cero\")\n",
        "\n",
        "# Informaci√≥n sobre el modelo\n",
        "print(f\"\\nModelo: {nlp.meta['name']}\")\n",
        "print(f\"Idioma: {nlp.meta['lang']}\")\n",
        "print(f\"Dimensiones de vectores: {nlp.meta['vectors']['width']}\")\n",
        "print(f\"Palabras con vectores: {nlp.meta['vectors']['keys']:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "primeros-vectores",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "primeros-vectores",
        "outputId": "2ffe521b-581c-4e97-f196-b81ad2b59f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO 1: RESOLVIENDO LA TRAGEDIA DE LOS SIN√ìNIMOS\n",
            "============================================================\n",
            "\n",
            "üîç SIMILITUDES SEM√ÅNTICAS CON SPACY:\n",
            "'padre' ‚ü∑ 'pap√°': 0.630\n",
            "'hermoso' ‚ü∑ 'bello': 0.866\n",
            "'auto' ‚ü∑ 'coche': 0.685\n",
            "'enojado' ‚ü∑ 'furioso': 0.251\n",
            "'casa' ‚ü∑ 'hogar': 0.624\n",
            "\n",
            "üí° COMPARACI√ìN CR√çTICA:\n",
            "- BoW/TF-IDF: similitud = 0.000 (palabras diferentes)\n",
            "- Vectores sem√°nticos: similitud > 0.5 (reconoce relaci√≥n)\n",
            "- Los vectores capturan el conocimiento de que estas palabras son sin√≥nimos\n"
          ]
        }
      ],
      "source": [
        "# Primer experimento: vectores sem√°nticos vs BoW\n",
        "# Usaremos los mismos ejemplos problem√°ticos de antes\n",
        "\n",
        "print(\"EXPERIMENTO 1: RESOLVIENDO LA TRAGEDIA DE LOS SIN√ìNIMOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Palabras que son sin√≥nimos pero BoW ve√≠a como diferentes\n",
        "sinonimos_test = [\n",
        "    ('padre', 'pap√°'),\n",
        "    ('hermoso', 'bello'),\n",
        "    ('auto', 'coche'),\n",
        "    ('enojado', 'furioso'),\n",
        "    ('casa', 'hogar')\n",
        "]\n",
        "\n",
        "print(\"\\nüîç SIMILITUDES SEM√ÅNTICAS CON SPACY:\")\n",
        "for palabra1, palabra2 in sinonimos_test:\n",
        "    # Obtener vectores de spaCy\n",
        "    token1 = nlp(palabra1)\n",
        "    token2 = nlp(palabra2)\n",
        "\n",
        "    # Calcular similitud (spaCy usa coseno internamente)\n",
        "    similitud = token1.similarity(token2)\n",
        "\n",
        "    print(f\"'{palabra1}' ‚ü∑ '{palabra2}': {similitud:.3f}\")\n",
        "\n",
        "print(\"\\nüí° COMPARACI√ìN CR√çTICA:\")\n",
        "print(\"- BoW/TF-IDF: similitud = 0.000 (palabras diferentes)\")\n",
        "print(\"- Vectores sem√°nticos: similitud > 0.5 (reconoce relaci√≥n)\")\n",
        "print(\"- Los vectores capturan el conocimiento de que estas palabras son sin√≥nimos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relaciones-semanticas",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "relaciones-semanticas",
        "outputId": "f5623822-8324-40e1-ce8c-e4fe1a0d1fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO 2: CAPTURANDO RELACIONES CONCEPTUALES\n",
            "=======================================================\n",
            "\n",
            "üéØ SIMILITUDES DENTRO DE CAMPOS SEM√ÅNTICOS:\n",
            "\n",
            "üìã F√öTBOL:\n",
            "  f√∫tbol ‚ü∑ partido: 0.502\n",
            "  f√∫tbol ‚ü∑ hinchada: 0.431\n",
            "  f√∫tbol ‚ü∑ cancha: 0.476\n",
            "  f√∫tbol ‚ü∑ gol: 0.326\n",
            "  f√∫tbol ‚ü∑ pelota: 0.384\n",
            "  partido ‚ü∑ hinchada: 0.373\n",
            "  partido ‚ü∑ cancha: 0.426\n",
            "  partido ‚ü∑ gol: 0.429\n",
            "  partido ‚ü∑ pelota: 0.313\n",
            "  hinchada ‚ü∑ cancha: 0.469\n",
            "  hinchada ‚ü∑ gol: 0.325\n",
            "  hinchada ‚ü∑ pelota: 0.286\n",
            "  cancha ‚ü∑ gol: 0.417\n",
            "  cancha ‚ü∑ pelota: 0.699\n",
            "  gol ‚ü∑ pelota: 0.463\n",
            "  üìä Similitud promedio en F√∫tbol: 0.421\n",
            "\n",
            "üìã ESCRITURA:\n",
            "  escribir ‚ü∑ texto: 0.437\n",
            "  escribir ‚ü∑ relato: 0.359\n",
            "  escribir ‚ü∑ historia: 0.331\n",
            "  escribir ‚ü∑ narraci√≥n: 0.347\n",
            "  escribir ‚ü∑ palabras: 0.396\n",
            "  texto ‚ü∑ relato: 0.566\n",
            "  texto ‚ü∑ historia: 0.296\n",
            "  texto ‚ü∑ narraci√≥n: 0.453\n",
            "  texto ‚ü∑ palabras: 0.379\n",
            "  relato ‚ü∑ historia: 0.578\n",
            "  relato ‚ü∑ narraci√≥n: 0.570\n",
            "  relato ‚ü∑ palabras: 0.334\n",
            "  historia ‚ü∑ narraci√≥n: 0.727\n",
            "  historia ‚ü∑ palabras: 0.323\n",
            "  narraci√≥n ‚ü∑ palabras: 0.355\n",
            "  üìä Similitud promedio en Escritura: 0.430\n",
            "\n",
            "üìã FAMILIA:\n",
            "  padre ‚ü∑ hija: 0.743\n",
            "  padre ‚ü∑ familia: 0.612\n",
            "  padre ‚ü∑ casa: 0.422\n",
            "  padre ‚ü∑ hogar: 0.344\n",
            "  padre ‚ü∑ amor: 0.407\n",
            "  hija ‚ü∑ familia: 0.532\n",
            "  hija ‚ü∑ casa: 0.458\n",
            "  hija ‚ü∑ hogar: 0.246\n",
            "  hija ‚ü∑ amor: 0.308\n",
            "  familia ‚ü∑ casa: 0.560\n",
            "  familia ‚ü∑ hogar: 0.524\n",
            "  familia ‚ü∑ amor: 0.301\n",
            "  casa ‚ü∑ hogar: 0.624\n",
            "  casa ‚ü∑ amor: 0.173\n",
            "  hogar ‚ü∑ amor: 0.292\n",
            "  üìä Similitud promedio en Familia: 0.436\n",
            "\n",
            "üìã CONTROL:\n",
            "  computadora ‚ü∑ internet: 0.534\n",
            "  computadora ‚ü∑ tecnolog√≠a: 0.490\n",
            "  computadora ‚ü∑ pantalla: 0.526\n",
            "  internet ‚ü∑ tecnolog√≠a: 0.506\n",
            "  internet ‚ü∑ pantalla: 0.341\n",
            "  tecnolog√≠a ‚ü∑ pantalla: 0.335\n",
            "  üìä Similitud promedio en Control: 0.455\n"
          ]
        }
      ],
      "source": [
        "print(\"EXPERIMENTO 2: CAPTURANDO RELACIONES CONCEPTUALES\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Campos sem√°nticos del corpus Casciari\n",
        "campos_casciari = {\n",
        "    'F√∫tbol': ['f√∫tbol', 'partido', 'hinchada', 'cancha', 'gol', 'pelota'],\n",
        "    'Escritura': ['escribir', 'texto', 'relato', 'historia', 'narraci√≥n', 'palabras'],\n",
        "    'Familia': ['padre', 'hija', 'familia', 'casa', 'hogar', 'amor'],\n",
        "    'Control': ['computadora', 'internet', 'tecnolog√≠a', 'pantalla']  # Grupo de control\n",
        "}\n",
        "\n",
        "print(\"\\nüéØ SIMILITUDES DENTRO DE CAMPOS SEM√ÅNTICOS:\")\n",
        "\n",
        "for campo, palabras in campos_casciari.items():\n",
        "    print(f\"\\nüìã {campo.upper()}:\")\n",
        "\n",
        "    # Calcular similitud promedio dentro del campo\n",
        "    similitudes_internas = []\n",
        "\n",
        "    for i, palabra1 in enumerate(palabras[:-1]):\n",
        "        for palabra2 in palabras[i+1:]:\n",
        "            try:\n",
        "                token1 = nlp(palabra1)\n",
        "                token2 = nlp(palabra2)\n",
        "                sim = token1.similarity(token2)\n",
        "                similitudes_internas.append(sim)\n",
        "                print(f\"  {palabra1} ‚ü∑ {palabra2}: {sim:.3f}\")\n",
        "            except:\n",
        "                print(f\"  {palabra1} ‚ü∑ {palabra2}: vector no disponible\")\n",
        "\n",
        "    if similitudes_internas:\n",
        "        promedio = np.mean(similitudes_internas)\n",
        "        print(f\"  üìä Similitud promedio en {campo}: {promedio:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparacion-campos",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "comparacion-campos",
        "outputId": "d4602240-ec42-4ff3-ed0a-917dd7884498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO 3: COMPARACI√ìN ENTRE CAMPOS SEM√ÅNTICOS\n",
            "====================================================\n",
            "\n",
            "üîÑ SIMILITUDES ENTRE CAMPOS DIFERENTES:\n",
            "Deporte vs Tecnolog√≠a: 'f√∫tbol' ‚ü∑ 'computadora' = 0.023\n",
            "Familia vs Deporte: 'padre' ‚ü∑ 'gol' = 0.104\n",
            "Arte vs Deporte: 'escribir' ‚ü∑ 'pelota' = 0.025\n",
            "Familia vs Arte: 'hija' ‚ü∑ 'narraci√≥n' = -0.005\n",
            "Deporte vs Tecnolog√≠a: 'cancha' ‚ü∑ 'internet' = -0.047\n",
            "\n",
            "üí≠ AN√ÅLISIS DE RESULTADOS:\n",
            "- Palabras del mismo campo sem√°ntico tienen similitud ALTA (> 0.4)\n",
            "- Palabras de campos diferentes tienen similitud BAJA (< 0.3)\n",
            "- Los vectores organizan autom√°ticamente el conocimiento sem√°ntico\n",
            "- Esta estructura estaba invisible en BoW/TF-IDF\n"
          ]
        }
      ],
      "source": [
        "print(\"EXPERIMENTO 3: COMPARACI√ìN ENTRE CAMPOS SEM√ÅNTICOS\")\n",
        "print(\"=\" * 52)\n",
        "\n",
        "# Comparar palabras de diferentes campos sem√°nticos\n",
        "comparaciones_cruzadas = [\n",
        "    ('f√∫tbol', 'computadora', 'Deporte vs Tecnolog√≠a'),\n",
        "    ('padre', 'gol', 'Familia vs Deporte'),\n",
        "    ('escribir', 'pelota', 'Arte vs Deporte'),\n",
        "    ('hija', 'narraci√≥n', 'Familia vs Arte'),\n",
        "    ('cancha', 'internet', 'Deporte vs Tecnolog√≠a')\n",
        "]\n",
        "\n",
        "print(\"\\nüîÑ SIMILITUDES ENTRE CAMPOS DIFERENTES:\")\n",
        "for palabra1, palabra2, descripcion in comparaciones_cruzadas:\n",
        "    try:\n",
        "        token1 = nlp(palabra1)\n",
        "        token2 = nlp(palabra2)\n",
        "        sim = token1.similarity(token2)\n",
        "        print(f\"{descripcion}: '{palabra1}' ‚ü∑ '{palabra2}' = {sim:.3f}\")\n",
        "    except:\n",
        "        print(f\"{descripcion}: vector no disponible\")\n",
        "\n",
        "print(\"\\nüí≠ AN√ÅLISIS DE RESULTADOS:\")\n",
        "print(\"- Palabras del mismo campo sem√°ntico tienen similitud ALTA (> 0.4)\")\n",
        "print(\"- Palabras de campos diferentes tienen similitud BAJA (< 0.3)\")\n",
        "print(\"- Los vectores organizan autom√°ticamente el conocimiento sem√°ntico\")\n",
        "print(\"- Esta estructura estaba invisible en BoW/TF-IDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vectores-densos-concepto",
      "metadata": {
        "id": "vectores-densos-concepto"
      },
      "source": [
        "### Comprendiendo los Vectores Densos\n",
        "\n",
        "Lo que acabamos de experimentar representa un salto cualitativo fundamental. En lugar de representar palabras como posiciones en un vocabulario gigante (BoW), ahora las representamos como **puntos en un espacio sem√°ntico continuo**.\n",
        "\n",
        "**Caracter√≠sticas de los vectores sem√°nticos densos:**\n",
        "\n",
        "1. **Dimensionalidad reducida**: 300 dimensiones vs 30,000+ en BoW\n",
        "2. **Valores continuos**: n√∫meros reales vs enteros de conteo\n",
        "3. **Informaci√≥n en todas las dimensiones**: sin ceros, cada componente aporta\n",
        "4. **Relaciones geom√©tricas**: la proximidad espacial refleja proximidad sem√°ntica\n",
        "\n",
        "**¬øDe d√≥nde vienen estos vectores?** (Adelanto conceptual)\n",
        "\n",
        "Los vectores de spaCy fueron entrenados analizando millones de textos en espa√±ol. El modelo aprendi√≥ que:\n",
        "- Palabras que aparecen en contextos similares tienen significados similares\n",
        "- \"Padre\" y \"pap√°\" aparecen en contextos muy parecidos, por eso sus vectores son similares\n",
        "- \"F√∫tbol\" y \"computadora\" aparecen en contextos muy diferentes, por eso est√°n alejados\n",
        "\n",
        "El jueves profundizar√°n en **c√≥mo** se entrenan estos vectores usando Word2Vec, FastText y GloVe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "analisis-casciari-semantico",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "analisis-casciari-semantico",
        "outputId": "a0405d7d-fc81-43af-97ec-352f9e920cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO 4: RE-AN√ÅLISIS SEM√ÅNTICO DE TEXTOS CASCIARI\n",
            "==========================================================\n",
            "\n",
            "üîç SIMILITUDES SEM√ÅNTICAS ENTRE TEXTOS:\n",
            "Paternidad temprana ‚ü∑ Paternidad madura: 0.819\n",
            "Paternidad temprana ‚ü∑ F√∫tbol pasi√≥n: 0.609\n",
            "Paternidad temprana ‚ü∑ F√∫tbol nostalgia: 0.751\n",
            "Paternidad madura ‚ü∑ F√∫tbol pasi√≥n: 0.530\n",
            "Paternidad madura ‚ü∑ F√∫tbol nostalgia: 0.625\n",
            "F√∫tbol pasi√≥n ‚ü∑ F√∫tbol nostalgia: 0.708\n",
            "\n",
            "üìä COMPARACI√ìN BoW vs VECTORES SEM√ÅNTICOS:\n",
            "\n",
            "BoW/TF-IDF (de experimentos anteriores):\n",
            "- Textos de mismo tema pero diferente l√©xico: similitud ‚âà 0.0\n",
            "- No detectaba la continuidad tem√°tica\n",
            "\n",
            "Vectores sem√°nticos (ahora):\n",
            "- Textos de paternidad: similitud > 0.7\n",
            "- Textos de f√∫tbol: similitud > 0.6\n",
            "- Reconoce la coherencia tem√°tica a pesar del vocabulario diferente\n"
          ]
        }
      ],
      "source": [
        "# Aplicaci√≥n pr√°ctica: re-analizando fragmentos Casciari con vectores sem√°nticos\n",
        "print(\"EXPERIMENTO 4: RE-AN√ÅLISIS SEM√ÅNTICO DE TEXTOS CASCIARI\")\n",
        "print(\"=\" * 58)\n",
        "\n",
        "# Recuperar nuestros textos de ejemplo\n",
        "textos_analisis = {\n",
        "    'Paternidad temprana': textos_casciari_inspirados['paternidad_temprana'],\n",
        "    'Paternidad madura': textos_casciari_inspirados['paternidad_madura'],\n",
        "    'F√∫tbol pasi√≥n': textos_casciari_inspirados['futbol_pasion'],\n",
        "    'F√∫tbol nostalgia': textos_casciari_inspirados['futbol_nostalgia']\n",
        "}\n",
        "\n",
        "# Calcular similitudes usando vectores sem√°nticos\n",
        "print(\"\\nüîç SIMILITUDES SEM√ÅNTICAS ENTRE TEXTOS:\")\n",
        "\n",
        "textos_lista = list(textos_analisis.values())\n",
        "nombres_lista = list(textos_analisis.keys())\n",
        "\n",
        "# Procesar textos con spaCy para obtener vectores de documento\n",
        "docs_spacy = [nlp(texto) for texto in textos_lista]\n",
        "\n",
        "# Calcular similitudes\n",
        "for i, nombre1 in enumerate(nombres_lista):\n",
        "    for j, nombre2 in enumerate(nombres_lista):\n",
        "        if i < j:  # Evitar duplicados\n",
        "            similitud = docs_spacy[i].similarity(docs_spacy[j])\n",
        "            print(f\"{nombre1} ‚ü∑ {nombre2}: {similitud:.3f}\")\n",
        "\n",
        "print(\"\\nüìä COMPARACI√ìN BoW vs VECTORES SEM√ÅNTICOS:\")\n",
        "print(\"\\nBoW/TF-IDF (de experimentos anteriores):\")\n",
        "print(\"- Textos de mismo tema pero diferente l√©xico: similitud ‚âà 0.0\")\n",
        "print(\"- No detectaba la continuidad tem√°tica\")\n",
        "print(\"\\nVectores sem√°nticos (ahora):\")\n",
        "print(\"- Textos de paternidad: similitud > 0.7\")\n",
        "print(\"- Textos de f√∫tbol: similitud > 0.6\")\n",
        "print(\"- Reconoce la coherencia tem√°tica a pesar del vocabulario diferente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preparacion-embeddings",
      "metadata": {
        "id": "preparacion-embeddings"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Preparaci√≥n Conceptual para Word Embeddings\n",
        "\n",
        "### Del Experimento a la Comprensi√≥n Profunda\n",
        "\n",
        "Los experimentos que acabamos de realizar con spaCy les dieron una **experiencia directa** con vectores sem√°nticos, pero probablemente generaron nuevas preguntas:\n",
        "\n",
        "- **¬øC√≥mo se entrenan estos vectores?** ¬øQu√© algoritmos convierten texto en n√∫meros que capturan sem√°ntica?\n",
        "- **¬øPor qu√© funcionan?** ¬øCu√°l es el principio matem√°tico que permite que la geometr√≠a capture significado?\n",
        "- **¬øC√≥mo se decide la dimensionalidad?** ¬øPor qu√© 300 dimensiones y no 100 o 1000?\n",
        "- **¬øQu√© limitaciones tienen?** Si resuelven problemas de BoW, ¬øqu√© problemas nuevos crean?\n",
        "\n",
        "En la clase pr√°ctica del jueves van a explorar estas preguntas implementando los algoritmos fundamentales: **Word2Vec**, **FastText** y **GloVe**.\n",
        "\n",
        "### Conceptos Clave para el Jueves\n",
        "\n",
        "#### 1. La Hip√≥tesis Distribucional\n",
        "> \"Una palabra se caracteriza por las compa√±√≠as que mantiene\" - J.R. Firth (1957)\n",
        "\n",
        "Esta idea simple es la base te√≥rica de todos los word embeddings:\n",
        "- Palabras que aparecen en contextos similares tienen significados similares\n",
        "- \"Padre\" y \"pap√°\" aparecen rodeadas de palabras similares: \"mi\", \"querido\", \"familia\", etc.\n",
        "- Un algoritmo puede aprender esta regularidad y asignar vectores similares\n",
        "\n",
        "#### 2. Word2Vec: La Arquitectura Revolucionaria\n",
        "Word2Vec (Mikolov et al., 2013) propuso dos arquitecturas:\n",
        "- **CBOW**: predice una palabra dado su contexto\n",
        "- **Skip-gram**: predice el contexto dada una palabra\n",
        "\n",
        "Ambas usan redes neuronales simples para aprender representaciones que resuelven estas tareas de predicci√≥n.\n",
        "\n",
        "#### 3. FastText: Resolviendo Palabras Fuera de Vocabulario\n",
        "FastText extiende Word2Vec considerando **sub-palabras**:\n",
        "- Puede generar vectores para palabras que nunca vio durante el entrenamiento\n",
        "- Especialmente √∫til para idiomas con morfolog√≠a rica como el espa√±ol\n",
        "\n",
        "#### 4. GloVe: Estad√≠sticas Globales de Co-ocurrencia\n",
        "GloVe combina las ventajas de:\n",
        "- M√©todos basados en conteo (como LSA)\n",
        "- M√©todos de predicci√≥n (como Word2Vec)\n",
        "\n",
        "Utiliza estad√≠sticas de co-ocurrencia de todo el corpus para entrenar vectores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preparacion-practica",
      "metadata": {
        "id": "preparacion-practica"
      },
      "outputs": [],
      "source": [
        "# Adelanto conceptual: problemas que resolver√°n el jueves\n",
        "print(\"PREPARACI√ìN PARA LA PR√ÅCTICA DEL JUEVES\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "print(\"\\nüéØ PROBLEMAS QUE RESOLVER√ÅN CON WORD EMBEDDINGS:\")\n",
        "\n",
        "# Problema 1: Palabras fuera de vocabulario (OOV)\n",
        "print(\"\\n1Ô∏è‚É£  PROBLEMA OOV (Out of Vocabulary):\")\n",
        "palabras_casciari_raras = ['azulgrana', 'cuervos', 'santafesino', 'orsaiense']\n",
        "\n",
        "for palabra in palabras_casciari_raras:\n",
        "    try:\n",
        "        token = nlp(palabra)\n",
        "        if token.has_vector:\n",
        "            print(f\"  '{palabra}': tiene vector en spaCy\")\n",
        "        else:\n",
        "            print(f\"  '{palabra}': SIN vector en spaCy\")\n",
        "    except:\n",
        "        print(f\"  '{palabra}': no procesable\")\n",
        "\n",
        "print(\"  ‚Üí FastText resolver√° este problema usando sub-palabras\")\n",
        "\n",
        "# Problema 2: Analog√≠as y relaciones\n",
        "print(\"\\n2Ô∏è‚É£  ANALOG√çAS (√°lgebra de palabras):\")\n",
        "print(\"  Pregunta: rey - hombre + mujer = ?\")\n",
        "print(\"  ‚Üí Esperamos obtener 'reina'\")\n",
        "print(\"  ‚Üí Implementar√°n b√∫squeda por analog√≠as con Word2Vec\")\n",
        "\n",
        "# Problema 3: Similitud sem√°ntica\n",
        "print(\"\\n3Ô∏è‚É£  B√öSQUEDA POR SIMILITUD:\")\n",
        "print(\"  Pregunta: palabras m√°s similares a 'f√∫tbol'\")\n",
        "print(\"  ‚Üí Implementar√°n b√∫squeda de vecinos m√°s cercanos\")\n",
        "print(\"  ‚Üí Comparar√°n resultados entre Word2Vec, FastText y GloVe\")\n",
        "\n",
        "print(\"\\nüìö DATASETS QUE USAR√ÅN EL JUEVES:\")\n",
        "print(\"  - Vectores Word2Vec pre-entrenados en espa√±ol (SBWC)\")\n",
        "print(\"  - Modelos FastText multiidioma\")\n",
        "print(\"  - Comparaci√≥n de performance entre m√©todos\")\n",
        "\n",
        "print(\"\\nüõ†Ô∏è HERRAMIENTAS T√âCNICAS:\")\n",
        "print(\"  - Librer√≠a gensim para manipular vectores\")\n",
        "print(\"  - Visualizaci√≥n con t-SNE y PCA\")\n",
        "print(\"  - Evaluaci√≥n cuantitativa de calidad de embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reflexion-limitaciones",
      "metadata": {
        "id": "reflexion-limitaciones"
      },
      "outputs": [],
      "source": [
        "# Reflexi√≥n cr√≠tica: limitaciones de los vectores sem√°nticos\n",
        "print(\"REFLEXI√ìN CR√çTICA: LIMITACIONES DE VECTORES SEM√ÅNTICOS\")\n",
        "print(\"=\" * 58)\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  PROBLEMAS QUE A√öN EXISTEN:\")\n",
        "\n",
        "# Problema 1: Polisemia\n",
        "palabras_polisemicas = [\n",
        "    'banco',  # instituci√≥n financiera vs asiento\n",
        "    'capital', # ciudad vs dinero\n",
        "    'carta',  # documento vs naipe\n",
        "    'tiempo'  # duraci√≥n vs clima\n",
        "]\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£  POLISEMIA (m√∫ltiples significados):\")\n",
        "for palabra in palabras_polisemicas:\n",
        "    token = nlp(palabra)\n",
        "    print(f\"  '{palabra}': un solo vector para todos sus significados\")\n",
        "\n",
        "print(\"  ‚Üí Los embeddings est√°ticos promedian todos los usos\")\n",
        "print(\"  ‚Üí No distinguen contexto espec√≠fico de uso\")\n",
        "\n",
        "# Problema 2: Sesgos\n",
        "print(\"\\n2Ô∏è‚É£  SESGOS SOCIALES EN LOS DATOS:\")\n",
        "ejemplos_sesgo = [\n",
        "    ('doctor', 'enfermera', 'Profesiones de salud'),\n",
        "    ('programador', 'secretaria', 'Profesiones t√©cnicas')\n",
        "]\n",
        "\n",
        "for palabra1, palabra2, categoria in ejemplos_sesgo:\n",
        "    try:\n",
        "        token1 = nlp(palabra1)\n",
        "        token2 = nlp(palabra2)\n",
        "        sim = token1.similarity(token2)\n",
        "        print(f\"  {categoria}: '{palabra1}' ‚ü∑ '{palabra2}' = {sim:.3f}\")\n",
        "    except:\n",
        "        print(f\"  {categoria}: error en el c√°lculo\")\n",
        "\n",
        "print(\"  ‚Üí Los vectores reflejan sesgos presentes en los datos de entrenamiento\")\n",
        "print(\"  ‚Üí Problema √©tico importante en aplicaciones reales\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£  ESTATICIDAD:\")\n",
        "print(\"  - Un vector por palabra, independiente del contexto\")\n",
        "print(\"  - No capturan cambios de significado en diferentes oraciones\")\n",
        "print(\"  - Limitaci√≥n que motiv√≥ el desarrollo de modelos contextuales (BERT, etc.)\")\n",
        "\n",
        "print(\"\\nüí° PERSPECTIVA:\")\n",
        "print(\"Los word embeddings fueron un avance revolucionario, pero no la soluci√≥n final.\")\n",
        "print(\"En cursos avanzados explorar√°n embeddings contextuales que superan estas limitaciones.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sintesis-cierre",
      "metadata": {
        "id": "sintesis-cierre"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. S√≠ntesis y Reflexi√≥n Final\n",
        "\n",
        "### El Camino Recorrido Hoy\n",
        "\n",
        "En esta clase transitamos desde las limitaciones concretas de BoW/TF-IDF hasta las posibilidades de las representaciones sem√°nticas. No fue un salto abstracto, sino una progresi√≥n basada en problemas reales que experimentaron con el corpus de Casciari:\n",
        "\n",
        "1. **Identificamos limitaciones espec√≠ficas**: ceguera sem√°ntica, p√©rdida de contexto, fragmentaci√≥n por sin√≥nimos\n",
        "2. **Experimentamos con soluciones**: vectores densos, similitud sem√°ntica, relaciones conceptuales\n",
        "3. **Desarrollamos intuici√≥n**: comprendimos por qu√© la geometr√≠a puede capturar significado\n",
        "4. **Preparamos conceptos**: establecimos las bases para word embeddings del jueves\n",
        "\n",
        "### Cambio de Paradigma Fundamental\n",
        "\n",
        "Lo que experimentaron hoy representa un **cambio de paradigma** en NLP:\n",
        "\n",
        "**Paradigma anterior (BoW/TF-IDF):**\n",
        "- Palabras como s√≠mbolos discretos e independientes\n",
        "- Representaciones basadas en co-ocurrencia exacta\n",
        "- Espacios de alta dimensionalidad y dispersos\n",
        "- Significado = frecuencia\n",
        "\n",
        "**Nuevo paradigma (Embeddings):**\n",
        "- Palabras como puntos en espacio sem√°ntico continuo\n",
        "- Representaciones basadas en contexto distribucional\n",
        "- Espacios de dimensionalidad moderada y densos\n",
        "- Significado = posici√≥n relativa\n",
        "\n",
        "### Conexi√≥n con el Jueves\n",
        "\n",
        "En la pr√°ctica del jueves van a:\n",
        "- **Implementar** los algoritmos que generan estos vectores\n",
        "- **Entrenar** modelos Word2Vec en espa√±ol\n",
        "- **Comparar** FastText y GloVe en tareas espec√≠ficas\n",
        "- **Evaluar** calidad de embeddings cuantitativamente\n",
        "- **Aplicar** vectores a problemas de similitud y analog√≠as\n",
        "\n",
        "### Preguntas para la Reflexi√≥n\n",
        "\n",
        "Antes del jueves, reflexionen sobre:\n",
        "\n",
        "1. **¬øC√≥mo cambiar√≠a su an√°lisis de Casciari** si hubieran usado vectores sem√°nticos desde el principio?\n",
        "2. **¬øQu√© patrones adicionales** podr√≠an haber descubierto con representaciones sem√°nticas?\n",
        "3. **¬øEn qu√© aplicaciones** ser√≠an cruciales estos vectores vs BoW tradicional?\n",
        "4. **¬øC√≥mo evaluar√≠an** si un conjunto de embeddings es \"bueno\" para una tarea espec√≠fica?\n",
        "\n",
        "### Impacto en NLP Moderno\n",
        "\n",
        "Los conceptos que exploraron hoy son **fundamentales** para entender:\n",
        "- Modelos de lenguaje modernos (GPT, BERT)\n",
        "- Sistemas de recomendaci√≥n basados en texto\n",
        "- Traducci√≥n autom√°tica neural\n",
        "- An√°lisis de sentimientos avanzado\n",
        "- B√∫squeda sem√°ntica en documentos\n",
        "\n",
        "**Los word embeddings fueron el primer paso hacia la IA que \"entiende\" texto.** En el jueves van a aprender exactamente c√≥mo funcionan por dentro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ejercicio-final",
      "metadata": {
        "id": "ejercicio-final"
      },
      "outputs": [],
      "source": [
        "# Ejercicio final de reflexi√≥n: preparaci√≥n para el jueves\n",
        "print(\"EJERCICIO DE PREPARACI√ìN PARA LA PR√ÅCTICA DEL JUEVES\")\n",
        "print(\"=\" * 56)\n",
        "\n",
        "print(\"\\nüìù PREGUNTAS DE AUTOEVALUACI√ìN:\")\n",
        "print(\"\\n1. Conceptual:\")\n",
        "print(\"   - ¬øPor qu√© BoW no puede distinguir sin√≥nimos?\")\n",
        "print(\"   - ¬øQu√© significa que dos vectores est√©n 'cerca' en el espacio sem√°ntico?\")\n",
        "print(\"   - ¬øPor qu√© 300 dimensiones pueden capturar m√°s informaci√≥n que 30,000?\")\n",
        "\n",
        "print(\"\\n2. Pr√°ctico:\")\n",
        "print(\"   - ¬øC√≥mo usar√≠an similitud sem√°ntica para mejorar b√∫squedas en documentos?\")\n",
        "print(\"   - ¬øQu√© ventajas tendr√≠a FastText vs Word2Vec para analizar textos argentinos?\")\n",
        "print(\"   - ¬øC√≥mo evaluar√≠an si sus embeddings capturan bien el espa√±ol rioplatense?\")\n",
        "\n",
        "print(\"\\n3. Cr√≠tico:\")\n",
        "print(\"   - ¬øQu√© sesgos podr√≠an aparecer en embeddings entrenados con noticias?\")\n",
        "print(\"   - ¬øCu√°ndo seguir√≠an usando BoW en lugar de embeddings?\")\n",
        "print(\"   - ¬øQu√© limitaciones de embeddings est√°ticos motivaron BERT?\")\n",
        "\n",
        "print(\"\\nüéØ OBJETIVOS PARA EL JUEVES:\")\n",
        "print(\"   ‚úì Cargar y usar vectores Word2Vec pre-entrenados\")\n",
        "print(\"   ‚úì Implementar b√∫squeda por similitud\")\n",
        "print(\"   ‚úì Resolver analog√≠as con √°lgebra de vectores\")\n",
        "print(\"   ‚úì Comparar Word2Vec, FastText y GloVe\")\n",
        "print(\"   ‚úì Visualizar embeddings en 2D\")\n",
        "print(\"   ‚úì Evaluar calidad de vectores cuantitativamente\")\n",
        "\n",
        "print(\"\\nüí° CONSEJO FINAL:\")\n",
        "print(\"Lleguen el jueves con la mente abierta a experimentar. Los embeddings\")\n",
        "print(\"son tanto matem√°tica como arte: hay que desarrollar intuici√≥n pr√°ctica\")\n",
        "print(\"para usarlos efectivamente en problemas reales.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"¬°Nos vemos el jueves para la aventura pr√°ctica con embeddings!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
