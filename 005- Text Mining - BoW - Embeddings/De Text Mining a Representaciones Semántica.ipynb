{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-header",
      "metadata": {
        "id": "intro-header"
      },
      "source": [
        "# De Text Mining a Representaciones Semánticas: Un Puente Conceptual\n",
        "\n",
        "**Tecnicatura Superior en Ciencias de Datos e Inteligencia Artificial**  \n",
        "**Procesamiento de Lenguaje Natural**\n",
        "\n",
        "---\n",
        "\n",
        "## Introducción: Construyendo sobre Fundamentos Sólidos\n",
        "\n",
        "En el laboratorio anterior trabajaron intensamente con los cuentos de Hernán Casciari, aplicando técnicas fundamentales de **text mining**: preprocesamiento, tokenización, eliminación de stop words, vectorización con **Bag of Words** (BoW), análisis de frecuencias, y visualización con nubes de palabras. Fue un trabajo riguroso que les permitió extraer patrones temporales reales de un corpus auténtico.\n",
        "\n",
        "Ese laboratorio no fue solo un ejercicio técnico: fue una exploración profunda de cómo la vida de un escritor se refleja en su vocabulario a lo largo del tiempo. Descubrieron la evolución temática de Casciari, desde los primeros años centrados en experiencias personales hasta la consolidación de Orsai como proyecto editorial. Aprendieron a manejar matrices esparsas, a filtrar ruido textual, y a extraer información significativa de datos no estructurados.\n",
        "\n",
        "**Hoy, sin embargo, vamos a hacer algo diferente.** En lugar de celebrar únicamente los éxitos de esa experiencia, vamos a examinar críticamente sus limitaciones. No para desvalorizar lo que hicieron, sino para entender por qué necesitamos herramientas más sofisticadas para capturar la riqueza semántica del lenguaje.\n",
        "\n",
        "### Objetivo de Esta Clase\n",
        "\n",
        "Esta clase funciona como un **puente conceptual** entre el text mining tradicional que ya dominan y las representaciones semánticas que explorarán en la práctica del jueves. Nuestros objetivos específicos son:\n",
        "\n",
        "1. **Identificar y analizar las limitaciones semánticas** de BoW y TF-IDF que experimentaron con Casciari\n",
        "2. **Introducir el concepto de similitud semántica** y demostrar por qué es crucial para NLP moderno\n",
        "3. **Experimentar con vectores semánticos densos** usando spaCy como primer acercamiento\n",
        "4. **Preparar el terreno conceptual** para word embeddings, Word2Vec y modelos similares\n",
        "5. **Desarrollar intuición** sobre por qué las representaciones semánticas revolucionaron el campo\n",
        "\n",
        "### Metodología: Aprendizaje por Contraste\n",
        "\n",
        "Utilizaremos una metodología de **aprendizaje por contraste**: confrontaremos directamente las limitaciones de BoW/TF-IDF con las capacidades de representaciones semánticas. Usaremos ejemplos concretos, muchos extraídos del propio corpus de Casciari, para hacer visibles problemas que quizás sintieron intuitivamente pero no pudieron articular formalmente.\n",
        "\n",
        "No se trata de reemplazar conocimiento, sino de **expandirlo y profundizarlo**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reconexion",
      "metadata": {
        "id": "reconexion"
      },
      "source": [
        "---\n",
        "\n",
        "## 1. Reconexión: El Corpus Casciari como Punto de Partida\n",
        "\n",
        "### Recordando lo que Lograron\n",
        "\n",
        "Antes de avanzar, reconozcamos la solidez del trabajo que realizaron con el corpus de Casciari. Su análisis reveló patrones temporales fascinantes:\n",
        "\n",
        "- **Evolución temática**: El vocabulario de 2004-2007 (primeros años de paternidad) difería claramente del de 2010-2011 (años de Orsai)\n",
        "- **Diversidad léxica**: Identificaron años con mayor riqueza vocabular y conectaron esos patrones con eventos biográficos\n",
        "- **Palabras características**: Descubrieron términos que definían períodos específicos (\"revista\", \"orsai\", \"papelitos\")\n",
        "- **Técnicas profesionales**: Manejaron matrices de 12×29,683 dimensiones, optimizaron stop words, y crearon visualizaciones informativas\n",
        "\n",
        "Este trabajo les dio experiencia práctica con los fundamentos del NLP y una comprensión visceral de cómo funciona la vectorización de texto en escenarios reales.\n",
        "\n",
        "### Un Dataset Familiar para Nuevos Experimentos\n",
        "\n",
        "Hoy volveremos a usar elementos del corpus Casciari, pero con un propósito diferente: como laboratorio para explorar las limitaciones de nuestras herramientas actuales y la necesidad de enfoques más sofisticados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "setup",
        "outputId": "70f17c70-afd9-4e05-b15b-b80b3a2f8efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entorno configurado. Iniciando exploración crítica de text mining.\n"
          ]
        }
      ],
      "source": [
        "# Configuración inicial del entorno\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"Entorno configurado. Iniciando exploración crítica de text mining.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "casciari-examples",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "casciari-examples",
        "outputId": "4a5a1792-b8f0-4c3c-b938-e8f4dfae7c0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparados 8 fragmentos temáticos para análisis\n",
            "\n",
            "Categorías disponibles:\n",
            "  - paternidad_temprana\n",
            "  - paternidad_madura\n",
            "  - futbol_pasion\n",
            "  - futbol_nostalgia\n",
            "  - orsai_nacimiento\n",
            "  - orsai_consolidacion\n",
            "  - escritura_oficio\n",
            "  - escritura_arte\n"
          ]
        }
      ],
      "source": [
        "# Recreamos algunos ejemplos inspirados en el corpus Casciari para nuestros experimentos\n",
        "# Estos fragmentos capturan el estilo y las temáticas que encontraron en su análisis\n",
        "\n",
        "textos_casciari_inspirados = {\n",
        "    \"paternidad_temprana\": \"\"\"La nena se despertó a las tres de la mañana llorando.\n",
        "    No sabía qué hacer, así que la alcé y caminé por la casa hasta que se calmó.\n",
        "    Ser padre es lo más difícil que me ha tocado hacer en la vida.\"\"\",\n",
        "\n",
        "    \"paternidad_madura\": \"\"\"Mi hija se levantó temprano y me preparó el desayuno.\n",
        "    Ya no es una bebé que llora por las noches, ahora es una persona\n",
        "    que tiene sus propias opiniones sobre todo.\"\"\",\n",
        "\n",
        "    \"futbol_pasion\": \"\"\"El partido fue increíble. San Lorenzo jugó como nunca\n",
        "    y la hinchada cantó durante los noventa minutos.\n",
        "    El fútbol argentino tiene esa magia que no encuentro en ningún otro lugar.\"\"\",\n",
        "\n",
        "    \"futbol_nostalgia\": \"\"\"Viendo el encuentro por televisión recordé cuando iba\n",
        "    a la cancha con mi viejo. La pasión azulgrana corría por nuestras venas\n",
        "    y cada gol era una fiesta familiar.\"\"\",\n",
        "\n",
        "    \"orsai_nacimiento\": \"\"\"La idea de la revista surgió casi por casualidad.\n",
        "    Quería hacer algo diferente, contar historias que no encontraba\n",
        "    en otros medios. Los lectores respondieron de manera sorprendente.\"\"\",\n",
        "\n",
        "    \"orsai_consolidacion\": \"\"\"La publicación ya tiene su identidad propia.\n",
        "    Los escritores que colaboran entienden el espíritu del proyecto\n",
        "    y cada número es mejor que el anterior.\"\"\",\n",
        "\n",
        "    \"escritura_oficio\": \"\"\"Escribir es mi trabajo, pero también mi obsesión.\n",
        "    Cada texto es una oportunidad de conectar con los lectores\n",
        "    de una manera auténtica y directa.\"\"\",\n",
        "\n",
        "    \"escritura_arte\": \"\"\"Las palabras son mi herramienta para crear mundos.\n",
        "    Cada relato es una invitación a los lectores para que\n",
        "    construyan sus propias interpretaciones.\"\"\"\n",
        "}\n",
        "\n",
        "# Convertir a DataFrame para facilitar manipulación\n",
        "df_ejemplos = pd.DataFrame(list(textos_casciari_inspirados.items()),\n",
        "                          columns=['categoria', 'texto'])\n",
        "\n",
        "print(f\"Preparados {len(df_ejemplos)} fragmentos temáticos para análisis\")\n",
        "print(\"\\nCategorías disponibles:\")\n",
        "for cat in df_ejemplos['categoria'].unique():\n",
        "    print(f\"  - {cat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "limitaciones-bow",
      "metadata": {
        "id": "limitaciones-bow"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Las Limitaciones que Experimentaron (Sin Saberlo)\n",
        "\n",
        "### Problema 1: La Ceguera Semántica de BoW\n",
        "\n",
        "Cuando analizaron el corpus de Casciari con Bag of Words, el modelo trataba cada palabra como una entidad completamente independiente. **\"Padre\" y \"papá\" eran tan diferentes entre sí como \"padre\" y \"computadora\".**\n",
        "\n",
        "Esto significa que si Casciari usaba \"padre\" en 2004 y \"papá\" en 2008 para referirse al mismo concepto, su análisis de frecuencias no captaba esa continuidad temática. La riqueza semántica del lenguaje humano quedaba invisible.\n",
        "\n",
        "Veamos este problema en acción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "semantica-bow",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "semantica-bow",
        "outputId": "32e30cb2-6875-43e8-c653-4efd4d9a8a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPRESENTACIÓN BoW DE TRES FRASES SEMÁNTICAMENTE SIMILARES:\n",
            "=================================================================\n",
            "         al  chico  cuando  cómo  de  del  el  enseñó  era  explicó  fútbol  \\\n",
            "Frase 1   1      1       1     0   0    0   0       1    1        0       1   \n",
            "Frase 2   0      0       0     1   1    0   0       0    0        0       0   \n",
            "Frase 3   0      0       0     0   0    1   1       0    0        1       0   \n",
            "\n",
            "         juego  jugar  la  las  me  mi  mostró  niño  padre  papá  patear  \\\n",
            "Frase 1      0      1   0    0   1   1       0     0      0     1       0   \n",
            "Frase 2      0      0   1    0   1   1       1     1      1     0       1   \n",
            "Frase 3      1      0   0    1   1   0       0     0      0     0       0   \n",
            "\n",
            "         pelota  pequeño  reglas  siendo  viejo  \n",
            "Frase 1       0        0       0       0      0  \n",
            "Frase 2       1        0       0       0      0  \n",
            "Frase 3       0        1       1       1      1  \n",
            "\n",
            "💭 OBSERVACIÓN CRÍTICA:\n",
            "Estas tres frases hablan del mismo recuerdo (el padre enseñando fútbol),\n",
            "pero BoW las ve como completamente diferentes porque no comparten palabras.\n",
            "\n",
            "SIMILITUD COSENO ENTRE FRASES (según BoW):\n",
            "Frase 1 vs Frase 2: 0.200\n",
            "Frase 1 vs Frase 3: 0.100\n",
            "Frase 2 vs Frase 3: 0.100\n",
            "\n",
            "⚠️  Similitud = 0 significa que BoW considera estas frases completamente diferentes\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo concreto: sinónimos que BoW trata como palabras independientes\n",
        "ejemplos_sinonimos = [\n",
        "    \"Mi papá me enseñó a jugar al fútbol cuando era chico\",\n",
        "    \"Mi padre me mostró cómo patear la pelota de niño\",\n",
        "    \"El viejo me explicó las reglas del juego siendo pequeño\"\n",
        "]\n",
        "\n",
        "# Vectorización con BoW\n",
        "vectorizer_bow = CountVectorizer()\n",
        "matriz_bow = vectorizer_bow.fit_transform(ejemplos_sinonimos)\n",
        "feature_names = vectorizer_bow.get_feature_names_out()\n",
        "\n",
        "# Crear DataFrame para visualizar\n",
        "df_bow = pd.DataFrame(matriz_bow.toarray(), columns=feature_names)\n",
        "df_bow.index = ['Frase 1', 'Frase 2', 'Frase 3']\n",
        "\n",
        "print(\"REPRESENTACIÓN BoW DE TRES FRASES SEMÁNTICAMENTE SIMILARES:\")\n",
        "print(\"=\" * 65)\n",
        "print(df_bow)\n",
        "print(\"\\n💭 OBSERVACIÓN CRÍTICA:\")\n",
        "print(\"Estas tres frases hablan del mismo recuerdo (el padre enseñando fútbol),\")\n",
        "print(\"pero BoW las ve como completamente diferentes porque no comparten palabras.\")\n",
        "\n",
        "# Calcular similitud coseno\n",
        "similitudes = cosine_similarity(matriz_bow)\n",
        "print(\"\\nSIMILITUD COSENO ENTRE FRASES (según BoW):\")\n",
        "print(f\"Frase 1 vs Frase 2: {similitudes[0,1]:.3f}\")\n",
        "print(f\"Frase 1 vs Frase 3: {similitudes[0,2]:.3f}\")\n",
        "print(f\"Frase 2 vs Frase 3: {similitudes[1,2]:.3f}\")\n",
        "print(\"\\n⚠️  Similitud = 0 significa que BoW considera estas frases completamente diferentes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "problema-contexto",
      "metadata": {
        "id": "problema-contexto"
      },
      "source": [
        "### Problema 2: Pérdida del Contexto Semántico\n",
        "\n",
        "En su análisis de Casciari probablemente notaron palabras como \"tiempo\" apareciendo con alta frecuencia. Pero BoW no distingue entre:\n",
        "\n",
        "- \"No tengo **tiempo** para escribir\" (recurso escaso)\n",
        "- \"El **tiempo** pasa rápido cuando juego con mi hija\" (experiencia subjetiva)\n",
        "- \"En **tiempo** de Navidad todo cambia\" (período específico)\n",
        "\n",
        "Para BoW, estos son simplemente tres ocurrencias de la palabra \"tiempo\", pero semánticamente representan conceptos diferentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "polisemia",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "polisemia",
        "outputId": "68350d31-7e74-4878-f19b-54f617386df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANÁLISIS DE POLISEMIA: LA PALABRA 'TIEMPO'\n",
            "=============================================\n",
            "\n",
            "Frecuencia de 'tiempo' en cada frase:\n",
            "Frase 1: 1 ocurrencia(s)\n",
            "  Contexto: No tengo tiempo para escribir hoy, estoy muy ocupa...\n",
            "Frase 2: 1 ocurrencia(s)\n",
            "  Contexto: El tiempo vuela cuando estoy con mi familia...\n",
            "Frase 3: 1 ocurrencia(s)\n",
            "  Contexto: En tiempo de pandemia cambió nuestra forma de vivi...\n",
            "Frase 4: 1 ocurrencia(s)\n",
            "  Contexto: Hace tiempo que no veo una película tan buena...\n",
            "Frase 5: 1 ocurrencia(s)\n",
            "  Contexto: El tiempo atmosférico está muy cambiante esta sema...\n",
            "\n",
            "💭 REFLEXIÓN CRÍTICA:\n",
            "BoW cuenta 5 ocurrencias de 'tiempo', pero cada una tiene un significado diferente:\n",
            "  • Tiempo como recurso (frase 1)\n",
            "  • Tiempo como experiencia subjetiva (frase 2)\n",
            "  • Tiempo como período histórico (frase 3)\n",
            "  • Tiempo como duración (frase 4)\n",
            "  • Tiempo como condición climática (frase 5)\n",
            "\n",
            "⚠️  BoW/TF-IDF no puede distinguir entre estos usos semánticamente diferentes\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo de polisemia: una palabra, múltiples significados\n",
        "frases_tiempo = [\n",
        "    \"No tengo tiempo para escribir hoy, estoy muy ocupado\",\n",
        "    \"El tiempo vuela cuando estoy con mi familia\",\n",
        "    \"En tiempo de pandemia cambió nuestra forma de vivir\",\n",
        "    \"Hace tiempo que no veo una película tan buena\",\n",
        "    \"El tiempo atmosférico está muy cambiante esta semana\"\n",
        "]\n",
        "\n",
        "vectorizer_tiempo = CountVectorizer()\n",
        "matriz_tiempo = vectorizer_tiempo.fit_transform(frases_tiempo)\n",
        "\n",
        "# Buscar la columna correspondiente a \"tiempo\"\n",
        "feature_names_tiempo = vectorizer_tiempo.get_feature_names_out()\n",
        "indice_tiempo = np.where(feature_names_tiempo == 'tiempo')[0][0]\n",
        "\n",
        "print(\"ANÁLISIS DE POLISEMIA: LA PALABRA 'TIEMPO'\")\n",
        "print(\"=\" * 45)\n",
        "print(\"\\nFrecuencia de 'tiempo' en cada frase:\")\n",
        "for i, frase in enumerate(frases_tiempo):\n",
        "    freq = matriz_tiempo[i, indice_tiempo]\n",
        "    print(f\"Frase {i+1}: {freq} ocurrencia(s)\")\n",
        "    print(f\"  Contexto: {frase[:50]}...\")\n",
        "\n",
        "print(\"\\n💭 REFLEXIÓN CRÍTICA:\")\n",
        "print(\"BoW cuenta 5 ocurrencias de 'tiempo', pero cada una tiene un significado diferente:\")\n",
        "print(\"  • Tiempo como recurso (frase 1)\")\n",
        "print(\"  • Tiempo como experiencia subjetiva (frase 2)\")\n",
        "print(\"  • Tiempo como período histórico (frase 3)\")\n",
        "print(\"  • Tiempo como duración (frase 4)\")\n",
        "print(\"  • Tiempo como condición climática (frase 5)\")\n",
        "print(\"\\n⚠️  BoW/TF-IDF no puede distinguir entre estos usos semánticamente diferentes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "problema-relaciones",
      "metadata": {
        "id": "problema-relaciones"
      },
      "source": [
        "### Problema 3: Incapacidad para Capturar Relaciones Conceptuales\n",
        "\n",
        "Su análisis de Casciari mostró la evolución de temas familiares a lo largo del tiempo. Sin embargo, BoW no puede entender que:\n",
        "\n",
        "- **\"bebé\" → \"nena\" → \"hija\" → \"adolescente\"** representan la misma persona en diferentes etapas\n",
        "- **\"San Lorenzo\" → \"azulgrana\" → \"hinchada\" → \"cancha\"** forman un campo semántico relacionado con fútbol\n",
        "- **\"escribir\" → \"texto\" → \"relato\" → \"historia\"** están conceptualmente conectados\n",
        "\n",
        "Esta limitación es crucial porque significa que patrones semánticos profundos permanecen invisibles en el análisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relaciones-conceptuales",
      "metadata": {
        "id": "relaciones-conceptuales"
      },
      "outputs": [],
      "source": [
        "# Demostración: campos semánticos que BoW no puede relacionar\n",
        "campos_semanticos = {\n",
        "    \"Fútbol (explícito)\": \"El partido de fútbol fue emocionante, San Lorenzo ganó\",\n",
        "    \"Fútbol (implícito 1)\": \"La hinchada cantó durante todo el encuentro azulgrana\",\n",
        "    \"Fútbol (implícito 2)\": \"El Ciclón mostró gran juego y la tribuna se volvió loca\",\n",
        "    \"Escritura (explícito)\": \"Escribir es mi pasión, cada texto es una aventura\",\n",
        "    \"Escritura (implícito 1)\": \"La narración fluía y las palabras cobraban vida\",\n",
        "    \"Escritura (implícito 2)\": \"El relato cautivó a los lectores desde el primer párrafo\"\n",
        "}\n",
        "\n",
        "textos_campos = list(campos_semanticos.values())\n",
        "labels_campos = list(campos_semanticos.keys())\n",
        "\n",
        "# Vectorización\n",
        "vectorizer_campos = TfidfVectorizer()\n",
        "matriz_campos = vectorizer_campos.fit_transform(textos_campos)\n",
        "\n",
        "# Calcular similitudes\n",
        "similitudes_campos = cosine_similarity(matriz_campos)\n",
        "\n",
        "print(\"SIMILITUD ENTRE TEXTOS DE CAMPOS SEMÁNTICOS RELACIONADOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Comparar textos dentro del mismo campo semántico\n",
        "print(\"\\n🏈 CAMPO SEMÁNTICO: FÚTBOL\")\n",
        "print(f\"Explícito vs Implícito 1: {similitudes_campos[0,1]:.3f}\")\n",
        "print(f\"Explícito vs Implícito 2: {similitudes_campos[0,2]:.3f}\")\n",
        "print(f\"Implícito 1 vs Implícito 2: {similitudes_campos[1,2]:.3f}\")\n",
        "\n",
        "print(\"\\n✍️  CAMPO SEMÁNTICO: ESCRITURA\")\n",
        "print(f\"Explícito vs Implícito 1: {similitudes_campos[3,4]:.3f}\")\n",
        "print(f\"Explícito vs Implícito 2: {similitudes_campos[3,5]:.3f}\")\n",
        "print(f\"Implícito 1 vs Implícito 2: {similitudes_campos[4,5]:.3f}\")\n",
        "\n",
        "print(\"\\n🔄 COMPARACIÓN ENTRE CAMPOS (debe ser baja):\")\n",
        "print(f\"Fútbol explícito vs Escritura explícita: {similitudes_campos[0,3]:.3f}\")\n",
        "\n",
        "print(\"\\n💭 ANÁLISIS CRÍTICO:\")\n",
        "print(\"- Las similitudes dentro de cada campo son bajas porque no comparten palabras exactas\")\n",
        "print(\"- TF-IDF no reconoce que 'hinchada' y 'tribuna' se refieren al mismo concepto\")\n",
        "print(\"- No entiende que 'relato', 'narración' y 'texto' están semánticamente relacionados\")\n",
        "print(\"- Patrones temáticos profundos quedan invisibles en el análisis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "problema-sinonimos",
      "metadata": {
        "id": "problema-sinonimos"
      },
      "source": [
        "### Problema 4: La Tragedia de los Sinónimos\n",
        "\n",
        "Este es quizás el problema más frustrante que experimentaron sin darse cuenta. Casciari, como buen escritor, usa sinónimos para enriquecer su prosa. Pero para BoW/TF-IDF:\n",
        "\n",
        "- **\"hermoso\" y \"bello\"** son palabras completamente diferentes\n",
        "- **\"auto\" y \"coche\"** no tienen relación alguna  \n",
        "- **\"enojado\" y \"furioso\"** podrían estar en galaxias diferentes\n",
        "\n",
        "Esto significa que su análisis de frecuencias fragmentó artificialmente conceptos que deberían estar unidos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tragedia-sinonimos",
      "metadata": {
        "id": "tragedia-sinonimos"
      },
      "outputs": [],
      "source": [
        "# Ejemplo dramático: la fragmentación de conceptos por sinónimos\n",
        "textos_sinonimos = [\n",
        "    \"El paisaje era hermoso, me quedé contemplando la vista\",\n",
        "    \"La vista era bella, no podía dejar de admirar el panorama\",\n",
        "    \"Qué lindo lugar, la perspectiva desde aquí es preciosa\",\n",
        "    \"Este sitio es precioso, la panorámica es realmente bonita\"\n",
        "]\n",
        "\n",
        "# Análisis con TF-IDF\n",
        "vectorizer_sin = TfidfVectorizer()\n",
        "matriz_sin = vectorizer_sin.fit_transform(textos_sinonimos)\n",
        "feature_names_sin = vectorizer_sin.get_feature_names_out()\n",
        "\n",
        "# Crear DataFrame para análisis\n",
        "df_sinonimos = pd.DataFrame(matriz_sin.toarray(), columns=feature_names_sin)\n",
        "df_sinonimos.index = [f'Descripción {i+1}' for i in range(len(textos_sinonimos))]\n",
        "\n",
        "print(\"ANÁLISIS DE SINÓNIMOS: CONCEPTOS FRAGMENTADOS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Identificar palabras relacionadas con belleza\n",
        "palabras_belleza = ['hermoso', 'bella', 'lindo', 'preciosa', 'precioso', 'bonita']\n",
        "palabras_vista = ['vista', 'panorama', 'perspectiva', 'panorámica']\n",
        "\n",
        "print(\"\\n🎨 PALABRAS DE BELLEZA EN EL CORPUS:\")\n",
        "for palabra in palabras_belleza:\n",
        "    if palabra in feature_names_sin:\n",
        "        idx = np.where(feature_names_sin == palabra)[0][0]\n",
        "        ocurrencias = df_sinonimos.iloc[:, idx].values\n",
        "        frases_con_palabra = [i for i, val in enumerate(ocurrencias) if val > 0]\n",
        "        print(f\"'{palabra}': aparece en descripción(es) {[f+1 for f in frases_con_palabra]}\")\n",
        "\n",
        "print(\"\\n🔭 PALABRAS DE PERSPECTIVA VISUAL:\")\n",
        "for palabra in palabras_vista:\n",
        "    if palabra in feature_names_sin:\n",
        "        idx = np.where(feature_names_sin == palabra)[0][0]\n",
        "        ocurrencias = df_sinonimos.iloc[:, idx].values\n",
        "        frases_con_palabra = [i for i, val in enumerate(ocurrencias) if val > 0]\n",
        "        print(f\"'{palabra}': aparece en descripción(es) {[f+1 for f in frases_con_palabra]}\")\n",
        "\n",
        "# Calcular similitudes\n",
        "similitudes_sin = cosine_similarity(matriz_sin)\n",
        "\n",
        "print(\"\\n📊 SIMILITUDES ENTRE DESCRIPCIONES:\")\n",
        "for i in range(len(textos_sinonimos)):\n",
        "    for j in range(i+1, len(textos_sinonimos)):\n",
        "        sim = similitudes_sin[i,j]\n",
        "        print(f\"Descripción {i+1} vs {j+1}: {sim:.3f}\")\n",
        "\n",
        "print(\"\\n💭 TRAGEDIA DE LOS SINÓNIMOS:\")\n",
        "print(\"- Las 4 descripciones hablan del mismo concepto: un lugar hermoso con buena vista\")\n",
        "print(\"- Pero TF-IDF las ve como textos diferentes porque usan sinónimos\")\n",
        "print(\"- 'Hermoso', 'bello', 'lindo', 'precioso' expresan la misma idea\")\n",
        "print(\"- 'Vista', 'panorama', 'perspectiva' se refieren al mismo elemento\")\n",
        "print(\"- La riqueza léxica del español se convierte en una limitación técnica\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "transicion-semantica",
      "metadata": {
        "id": "transicion-semantica"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Hacia la Semántica Computacional: Una Nueva Perspectiva\n",
        "\n",
        "### El Salto Conceptual Necesario\n",
        "\n",
        "Los problemas que acabamos de identificar no son fallas de implementación que pueden corregirse con mejores algoritmos de preprocesamiento o stop words más sofisticadas. **Son limitaciones fundamentales de la representación bag-of-words.**\n",
        "\n",
        "Para superarlas, necesitamos un cambio de paradigma: pasar de representaciones basadas en **co-ocurrencia de palabras exactas** a representaciones que capturen **significado semántico**.\n",
        "\n",
        "### ¿Qué Significa \"Semántica\" en Contexto Computacional?\n",
        "\n",
        "En el contexto de NLP, cuando hablamos de **semántica** nos referimos a la capacidad de:\n",
        "\n",
        "1. **Reconocer sinónimos**: \"auto\" y \"coche\" deben tener representaciones similares\n",
        "2. **Capturar relaciones conceptuales**: \"médico\" debe estar más cerca de \"hospital\" que de \"computadora\"\n",
        "3. **Entender contexto**: distinguir \"banco\" (institución) de \"banco\" (asiento)\n",
        "4. **Detectar analogías**: si \"rey\" es a \"reina\" como \"hombre\" es a \"mujer\"\n",
        "\n",
        "### Introducción a los Vectores Semánticos Densos\n",
        "\n",
        "La solución viene de representar palabras como **vectores densos** en un espacio multidimensional, donde:\n",
        "\n",
        "- **Palabras similares** están **cerca** en el espacio vectorial\n",
        "- **Relaciones semánticas** se preservan como **relaciones geométricas**\n",
        "- **El contexto** determina la posición de cada palabra\n",
        "- **Las dimensiones** capturan aspectos abstractos del significado\n",
        "\n",
        "Experimentemos con esta idea usando spaCy, que incluye vectores pre-entrenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup-spacy",
      "metadata": {
        "id": "setup-spacy"
      },
      "outputs": [],
      "source": [
        "# Instalación y configuración de spaCy\n",
        "# Si es la primera vez que lo usan, necesitarán instalar el modelo en español\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import spacy\n",
        "    # Intentar cargar el modelo en español\n",
        "    nlp = spacy.load(\"es_core_news_md\")\n",
        "    print(\"✅ spaCy y modelo español cargados correctamente\")\n",
        "except OSError:\n",
        "    print(\"⚠️  Instalando modelo de spaCy en español...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_md\"])\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"es_core_news_md\")\n",
        "    print(\"✅ spaCy instalado y configurado\")\n",
        "except ImportError:\n",
        "    print(\"⚠️  Instalando spaCy...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"spacy\"])\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_md\"])\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"es_core_news_md\")\n",
        "    print(\"✅ spaCy instalado desde cero\")\n",
        "\n",
        "# Información sobre el modelo\n",
        "print(f\"\\nModelo: {nlp.meta['name']}\")\n",
        "print(f\"Idioma: {nlp.meta['lang']}\")\n",
        "print(f\"Dimensiones de vectores: {nlp.meta['vectors']['width']}\")\n",
        "print(f\"Palabras con vectores: {nlp.meta['vectors']['keys']:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "primeros-vectores",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "primeros-vectores",
        "outputId": "2ffe521b-581c-4e97-f196-b81ad2b59f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO 1: RESOLVIENDO LA TRAGEDIA DE LOS SINÓNIMOS\n",
            "============================================================\n",
            "\n",
            "🔍 SIMILITUDES SEMÁNTICAS CON SPACY:\n",
            "'padre' ⟷ 'papá': 0.630\n",
            "'hermoso' ⟷ 'bello': 0.866\n",
            "'auto' ⟷ 'coche': 0.685\n",
            "'enojado' ⟷ 'furioso': 0.251\n",
            "'casa' ⟷ 'hogar': 0.624\n",
            "\n",
            "💡 COMPARACIÓN CRÍTICA:\n",
            "- BoW/TF-IDF: similitud = 0.000 (palabras diferentes)\n",
            "- Vectores semánticos: similitud > 0.5 (reconoce relación)\n",
            "- Los vectores capturan el conocimiento de que estas palabras son sinónimos\n"
          ]
        }
      ],
      "source": [
        "# Primer experimento: vectores semánticos vs BoW\n",
        "# Usaremos los mismos ejemplos problemáticos de antes\n",
        "\n",
        "print(\"EXPERIMENTO 1: RESOLVIENDO LA TRAGEDIA DE LOS SINÓNIMOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Palabras que son sinónimos pero BoW veía como diferentes\n",
        "sinonimos_test = [\n",
        "    ('padre', 'papá'),\n",
        "    ('hermoso', 'bello'),\n",
        "    ('auto', 'coche'),\n",
        "    ('enojado', 'furioso'),\n",
        "    ('casa', 'hogar')\n",
        "]\n",
        "\n",
        "print(\"\\n🔍 SIMILITUDES SEMÁNTICAS CON SPACY:\")\n",
        "for palabra1, palabra2 in sinonimos_test:\n",
        "    # Obtener vectores de spaCy\n",
        "    token1 = nlp(palabra1)\n",
        "    token2 = nlp(palabra2)\n",
        "\n",
        "    # Calcular similitud (spaCy usa coseno internamente)\n",
        "    similitud = token1.similarity(token2)\n",
        "\n",
        "    print(f\"'{palabra1}' ⟷ '{palabra2}': {similitud:.3f}\")\n",
        "\n",
        "print(\"\\n💡 COMPARACIÓN CRÍTICA:\")\n",
        "print(\"- BoW/TF-IDF: similitud = 0.000 (palabras diferentes)\")\n",
        "print(\"- Vectores semánticos: similitud > 0.5 (reconoce relación)\")\n",
        "print(\"- Los vectores capturan el conocimiento de que estas palabras son sinónimos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "relaciones-semanticas",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "relaciones-semanticas",
        "outputId": "f5623822-8324-40e1-ce8c-e4fe1a0d1fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO 2: CAPTURANDO RELACIONES CONCEPTUALES\n",
            "=======================================================\n",
            "\n",
            "🎯 SIMILITUDES DENTRO DE CAMPOS SEMÁNTICOS:\n",
            "\n",
            "📋 FÚTBOL:\n",
            "  fútbol ⟷ partido: 0.502\n",
            "  fútbol ⟷ hinchada: 0.431\n",
            "  fútbol ⟷ cancha: 0.476\n",
            "  fútbol ⟷ gol: 0.326\n",
            "  fútbol ⟷ pelota: 0.384\n",
            "  partido ⟷ hinchada: 0.373\n",
            "  partido ⟷ cancha: 0.426\n",
            "  partido ⟷ gol: 0.429\n",
            "  partido ⟷ pelota: 0.313\n",
            "  hinchada ⟷ cancha: 0.469\n",
            "  hinchada ⟷ gol: 0.325\n",
            "  hinchada ⟷ pelota: 0.286\n",
            "  cancha ⟷ gol: 0.417\n",
            "  cancha ⟷ pelota: 0.699\n",
            "  gol ⟷ pelota: 0.463\n",
            "  📊 Similitud promedio en Fútbol: 0.421\n",
            "\n",
            "📋 ESCRITURA:\n",
            "  escribir ⟷ texto: 0.437\n",
            "  escribir ⟷ relato: 0.359\n",
            "  escribir ⟷ historia: 0.331\n",
            "  escribir ⟷ narración: 0.347\n",
            "  escribir ⟷ palabras: 0.396\n",
            "  texto ⟷ relato: 0.566\n",
            "  texto ⟷ historia: 0.296\n",
            "  texto ⟷ narración: 0.453\n",
            "  texto ⟷ palabras: 0.379\n",
            "  relato ⟷ historia: 0.578\n",
            "  relato ⟷ narración: 0.570\n",
            "  relato ⟷ palabras: 0.334\n",
            "  historia ⟷ narración: 0.727\n",
            "  historia ⟷ palabras: 0.323\n",
            "  narración ⟷ palabras: 0.355\n",
            "  📊 Similitud promedio en Escritura: 0.430\n",
            "\n",
            "📋 FAMILIA:\n",
            "  padre ⟷ hija: 0.743\n",
            "  padre ⟷ familia: 0.612\n",
            "  padre ⟷ casa: 0.422\n",
            "  padre ⟷ hogar: 0.344\n",
            "  padre ⟷ amor: 0.407\n",
            "  hija ⟷ familia: 0.532\n",
            "  hija ⟷ casa: 0.458\n",
            "  hija ⟷ hogar: 0.246\n",
            "  hija ⟷ amor: 0.308\n",
            "  familia ⟷ casa: 0.560\n",
            "  familia ⟷ hogar: 0.524\n",
            "  familia ⟷ amor: 0.301\n",
            "  casa ⟷ hogar: 0.624\n",
            "  casa ⟷ amor: 0.173\n",
            "  hogar ⟷ amor: 0.292\n",
            "  📊 Similitud promedio en Familia: 0.436\n",
            "\n",
            "📋 CONTROL:\n",
            "  computadora ⟷ internet: 0.534\n",
            "  computadora ⟷ tecnología: 0.490\n",
            "  computadora ⟷ pantalla: 0.526\n",
            "  internet ⟷ tecnología: 0.506\n",
            "  internet ⟷ pantalla: 0.341\n",
            "  tecnología ⟷ pantalla: 0.335\n",
            "  📊 Similitud promedio en Control: 0.455\n"
          ]
        }
      ],
      "source": [
        "print(\"EXPERIMENTO 2: CAPTURANDO RELACIONES CONCEPTUALES\")\n",
        "print(\"=\" * 55)\n",
        "\n",
        "# Campos semánticos del corpus Casciari\n",
        "campos_casciari = {\n",
        "    'Fútbol': ['fútbol', 'partido', 'hinchada', 'cancha', 'gol', 'pelota'],\n",
        "    'Escritura': ['escribir', 'texto', 'relato', 'historia', 'narración', 'palabras'],\n",
        "    'Familia': ['padre', 'hija', 'familia', 'casa', 'hogar', 'amor'],\n",
        "    'Control': ['computadora', 'internet', 'tecnología', 'pantalla']  # Grupo de control\n",
        "}\n",
        "\n",
        "print(\"\\n🎯 SIMILITUDES DENTRO DE CAMPOS SEMÁNTICOS:\")\n",
        "\n",
        "for campo, palabras in campos_casciari.items():\n",
        "    print(f\"\\n📋 {campo.upper()}:\")\n",
        "\n",
        "    # Calcular similitud promedio dentro del campo\n",
        "    similitudes_internas = []\n",
        "\n",
        "    for i, palabra1 in enumerate(palabras[:-1]):\n",
        "        for palabra2 in palabras[i+1:]:\n",
        "            try:\n",
        "                token1 = nlp(palabra1)\n",
        "                token2 = nlp(palabra2)\n",
        "                sim = token1.similarity(token2)\n",
        "                similitudes_internas.append(sim)\n",
        "                print(f\"  {palabra1} ⟷ {palabra2}: {sim:.3f}\")\n",
        "            except:\n",
        "                print(f\"  {palabra1} ⟷ {palabra2}: vector no disponible\")\n",
        "\n",
        "    if similitudes_internas:\n",
        "        promedio = np.mean(similitudes_internas)\n",
        "        print(f\"  📊 Similitud promedio en {campo}: {promedio:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparacion-campos",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "comparacion-campos",
        "outputId": "d4602240-ec42-4ff3-ed0a-917dd7884498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO 3: COMPARACIÓN ENTRE CAMPOS SEMÁNTICOS\n",
            "====================================================\n",
            "\n",
            "🔄 SIMILITUDES ENTRE CAMPOS DIFERENTES:\n",
            "Deporte vs Tecnología: 'fútbol' ⟷ 'computadora' = 0.023\n",
            "Familia vs Deporte: 'padre' ⟷ 'gol' = 0.104\n",
            "Arte vs Deporte: 'escribir' ⟷ 'pelota' = 0.025\n",
            "Familia vs Arte: 'hija' ⟷ 'narración' = -0.005\n",
            "Deporte vs Tecnología: 'cancha' ⟷ 'internet' = -0.047\n",
            "\n",
            "💭 ANÁLISIS DE RESULTADOS:\n",
            "- Palabras del mismo campo semántico tienen similitud ALTA (> 0.4)\n",
            "- Palabras de campos diferentes tienen similitud BAJA (< 0.3)\n",
            "- Los vectores organizan automáticamente el conocimiento semántico\n",
            "- Esta estructura estaba invisible en BoW/TF-IDF\n"
          ]
        }
      ],
      "source": [
        "print(\"EXPERIMENTO 3: COMPARACIÓN ENTRE CAMPOS SEMÁNTICOS\")\n",
        "print(\"=\" * 52)\n",
        "\n",
        "# Comparar palabras de diferentes campos semánticos\n",
        "comparaciones_cruzadas = [\n",
        "    ('fútbol', 'computadora', 'Deporte vs Tecnología'),\n",
        "    ('padre', 'gol', 'Familia vs Deporte'),\n",
        "    ('escribir', 'pelota', 'Arte vs Deporte'),\n",
        "    ('hija', 'narración', 'Familia vs Arte'),\n",
        "    ('cancha', 'internet', 'Deporte vs Tecnología')\n",
        "]\n",
        "\n",
        "print(\"\\n🔄 SIMILITUDES ENTRE CAMPOS DIFERENTES:\")\n",
        "for palabra1, palabra2, descripcion in comparaciones_cruzadas:\n",
        "    try:\n",
        "        token1 = nlp(palabra1)\n",
        "        token2 = nlp(palabra2)\n",
        "        sim = token1.similarity(token2)\n",
        "        print(f\"{descripcion}: '{palabra1}' ⟷ '{palabra2}' = {sim:.3f}\")\n",
        "    except:\n",
        "        print(f\"{descripcion}: vector no disponible\")\n",
        "\n",
        "print(\"\\n💭 ANÁLISIS DE RESULTADOS:\")\n",
        "print(\"- Palabras del mismo campo semántico tienen similitud ALTA (> 0.4)\")\n",
        "print(\"- Palabras de campos diferentes tienen similitud BAJA (< 0.3)\")\n",
        "print(\"- Los vectores organizan automáticamente el conocimiento semántico\")\n",
        "print(\"- Esta estructura estaba invisible en BoW/TF-IDF\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vectores-densos-concepto",
      "metadata": {
        "id": "vectores-densos-concepto"
      },
      "source": [
        "### Comprendiendo los Vectores Densos\n",
        "\n",
        "Lo que acabamos de experimentar representa un salto cualitativo fundamental. En lugar de representar palabras como posiciones en un vocabulario gigante (BoW), ahora las representamos como **puntos en un espacio semántico continuo**.\n",
        "\n",
        "**Características de los vectores semánticos densos:**\n",
        "\n",
        "1. **Dimensionalidad reducida**: 300 dimensiones vs 30,000+ en BoW\n",
        "2. **Valores continuos**: números reales vs enteros de conteo\n",
        "3. **Información en todas las dimensiones**: sin ceros, cada componente aporta\n",
        "4. **Relaciones geométricas**: la proximidad espacial refleja proximidad semántica\n",
        "\n",
        "**¿De dónde vienen estos vectores?** (Adelanto conceptual)\n",
        "\n",
        "Los vectores de spaCy fueron entrenados analizando millones de textos en español. El modelo aprendió que:\n",
        "- Palabras que aparecen en contextos similares tienen significados similares\n",
        "- \"Padre\" y \"papá\" aparecen en contextos muy parecidos, por eso sus vectores son similares\n",
        "- \"Fútbol\" y \"computadora\" aparecen en contextos muy diferentes, por eso están alejados\n",
        "\n",
        "El jueves profundizarán en **cómo** se entrenan estos vectores usando Word2Vec, FastText y GloVe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "analisis-casciari-semantico",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "analisis-casciari-semantico",
        "outputId": "a0405d7d-fc81-43af-97ec-352f9e920cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXPERIMENTO 4: RE-ANÁLISIS SEMÁNTICO DE TEXTOS CASCIARI\n",
            "==========================================================\n",
            "\n",
            "🔍 SIMILITUDES SEMÁNTICAS ENTRE TEXTOS:\n",
            "Paternidad temprana ⟷ Paternidad madura: 0.819\n",
            "Paternidad temprana ⟷ Fútbol pasión: 0.609\n",
            "Paternidad temprana ⟷ Fútbol nostalgia: 0.751\n",
            "Paternidad madura ⟷ Fútbol pasión: 0.530\n",
            "Paternidad madura ⟷ Fútbol nostalgia: 0.625\n",
            "Fútbol pasión ⟷ Fútbol nostalgia: 0.708\n",
            "\n",
            "📊 COMPARACIÓN BoW vs VECTORES SEMÁNTICOS:\n",
            "\n",
            "BoW/TF-IDF (de experimentos anteriores):\n",
            "- Textos de mismo tema pero diferente léxico: similitud ≈ 0.0\n",
            "- No detectaba la continuidad temática\n",
            "\n",
            "Vectores semánticos (ahora):\n",
            "- Textos de paternidad: similitud > 0.7\n",
            "- Textos de fútbol: similitud > 0.6\n",
            "- Reconoce la coherencia temática a pesar del vocabulario diferente\n"
          ]
        }
      ],
      "source": [
        "# Aplicación práctica: re-analizando fragmentos Casciari con vectores semánticos\n",
        "print(\"EXPERIMENTO 4: RE-ANÁLISIS SEMÁNTICO DE TEXTOS CASCIARI\")\n",
        "print(\"=\" * 58)\n",
        "\n",
        "# Recuperar nuestros textos de ejemplo\n",
        "textos_analisis = {\n",
        "    'Paternidad temprana': textos_casciari_inspirados['paternidad_temprana'],\n",
        "    'Paternidad madura': textos_casciari_inspirados['paternidad_madura'],\n",
        "    'Fútbol pasión': textos_casciari_inspirados['futbol_pasion'],\n",
        "    'Fútbol nostalgia': textos_casciari_inspirados['futbol_nostalgia']\n",
        "}\n",
        "\n",
        "# Calcular similitudes usando vectores semánticos\n",
        "print(\"\\n🔍 SIMILITUDES SEMÁNTICAS ENTRE TEXTOS:\")\n",
        "\n",
        "textos_lista = list(textos_analisis.values())\n",
        "nombres_lista = list(textos_analisis.keys())\n",
        "\n",
        "# Procesar textos con spaCy para obtener vectores de documento\n",
        "docs_spacy = [nlp(texto) for texto in textos_lista]\n",
        "\n",
        "# Calcular similitudes\n",
        "for i, nombre1 in enumerate(nombres_lista):\n",
        "    for j, nombre2 in enumerate(nombres_lista):\n",
        "        if i < j:  # Evitar duplicados\n",
        "            similitud = docs_spacy[i].similarity(docs_spacy[j])\n",
        "            print(f\"{nombre1} ⟷ {nombre2}: {similitud:.3f}\")\n",
        "\n",
        "print(\"\\n📊 COMPARACIÓN BoW vs VECTORES SEMÁNTICOS:\")\n",
        "print(\"\\nBoW/TF-IDF (de experimentos anteriores):\")\n",
        "print(\"- Textos de mismo tema pero diferente léxico: similitud ≈ 0.0\")\n",
        "print(\"- No detectaba la continuidad temática\")\n",
        "print(\"\\nVectores semánticos (ahora):\")\n",
        "print(\"- Textos de paternidad: similitud > 0.7\")\n",
        "print(\"- Textos de fútbol: similitud > 0.6\")\n",
        "print(\"- Reconoce la coherencia temática a pesar del vocabulario diferente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preparacion-embeddings",
      "metadata": {
        "id": "preparacion-embeddings"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Preparación Conceptual para Word Embeddings\n",
        "\n",
        "### Del Experimento a la Comprensión Profunda\n",
        "\n",
        "Los experimentos que acabamos de realizar con spaCy les dieron una **experiencia directa** con vectores semánticos, pero probablemente generaron nuevas preguntas:\n",
        "\n",
        "- **¿Cómo se entrenan estos vectores?** ¿Qué algoritmos convierten texto en números que capturan semántica?\n",
        "- **¿Por qué funcionan?** ¿Cuál es el principio matemático que permite que la geometría capture significado?\n",
        "- **¿Cómo se decide la dimensionalidad?** ¿Por qué 300 dimensiones y no 100 o 1000?\n",
        "- **¿Qué limitaciones tienen?** Si resuelven problemas de BoW, ¿qué problemas nuevos crean?\n",
        "\n",
        "En la clase práctica del jueves van a explorar estas preguntas implementando los algoritmos fundamentales: **Word2Vec**, **FastText** y **GloVe**.\n",
        "\n",
        "### Conceptos Clave para el Jueves\n",
        "\n",
        "#### 1. La Hipótesis Distribucional\n",
        "> \"Una palabra se caracteriza por las compañías que mantiene\" - J.R. Firth (1957)\n",
        "\n",
        "Esta idea simple es la base teórica de todos los word embeddings:\n",
        "- Palabras que aparecen en contextos similares tienen significados similares\n",
        "- \"Padre\" y \"papá\" aparecen rodeadas de palabras similares: \"mi\", \"querido\", \"familia\", etc.\n",
        "- Un algoritmo puede aprender esta regularidad y asignar vectores similares\n",
        "\n",
        "#### 2. Word2Vec: La Arquitectura Revolucionaria\n",
        "Word2Vec (Mikolov et al., 2013) propuso dos arquitecturas:\n",
        "- **CBOW**: predice una palabra dado su contexto\n",
        "- **Skip-gram**: predice el contexto dada una palabra\n",
        "\n",
        "Ambas usan redes neuronales simples para aprender representaciones que resuelven estas tareas de predicción.\n",
        "\n",
        "#### 3. FastText: Resolviendo Palabras Fuera de Vocabulario\n",
        "FastText extiende Word2Vec considerando **sub-palabras**:\n",
        "- Puede generar vectores para palabras que nunca vio durante el entrenamiento\n",
        "- Especialmente útil para idiomas con morfología rica como el español\n",
        "\n",
        "#### 4. GloVe: Estadísticas Globales de Co-ocurrencia\n",
        "GloVe combina las ventajas de:\n",
        "- Métodos basados en conteo (como LSA)\n",
        "- Métodos de predicción (como Word2Vec)\n",
        "\n",
        "Utiliza estadísticas de co-ocurrencia de todo el corpus para entrenar vectores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preparacion-practica",
      "metadata": {
        "id": "preparacion-practica"
      },
      "outputs": [],
      "source": [
        "# Adelanto conceptual: problemas que resolverán el jueves\n",
        "print(\"PREPARACIÓN PARA LA PRÁCTICA DEL JUEVES\")\n",
        "print(\"=\" * 42)\n",
        "\n",
        "print(\"\\n🎯 PROBLEMAS QUE RESOLVERÁN CON WORD EMBEDDINGS:\")\n",
        "\n",
        "# Problema 1: Palabras fuera de vocabulario (OOV)\n",
        "print(\"\\n1️⃣  PROBLEMA OOV (Out of Vocabulary):\")\n",
        "palabras_casciari_raras = ['azulgrana', 'cuervos', 'santafesino', 'orsaiense']\n",
        "\n",
        "for palabra in palabras_casciari_raras:\n",
        "    try:\n",
        "        token = nlp(palabra)\n",
        "        if token.has_vector:\n",
        "            print(f\"  '{palabra}': tiene vector en spaCy\")\n",
        "        else:\n",
        "            print(f\"  '{palabra}': SIN vector en spaCy\")\n",
        "    except:\n",
        "        print(f\"  '{palabra}': no procesable\")\n",
        "\n",
        "print(\"  → FastText resolverá este problema usando sub-palabras\")\n",
        "\n",
        "# Problema 2: Analogías y relaciones\n",
        "print(\"\\n2️⃣  ANALOGÍAS (álgebra de palabras):\")\n",
        "print(\"  Pregunta: rey - hombre + mujer = ?\")\n",
        "print(\"  → Esperamos obtener 'reina'\")\n",
        "print(\"  → Implementarán búsqueda por analogías con Word2Vec\")\n",
        "\n",
        "# Problema 3: Similitud semántica\n",
        "print(\"\\n3️⃣  BÚSQUEDA POR SIMILITUD:\")\n",
        "print(\"  Pregunta: palabras más similares a 'fútbol'\")\n",
        "print(\"  → Implementarán búsqueda de vecinos más cercanos\")\n",
        "print(\"  → Compararán resultados entre Word2Vec, FastText y GloVe\")\n",
        "\n",
        "print(\"\\n📚 DATASETS QUE USARÁN EL JUEVES:\")\n",
        "print(\"  - Vectores Word2Vec pre-entrenados en español (SBWC)\")\n",
        "print(\"  - Modelos FastText multiidioma\")\n",
        "print(\"  - Comparación de performance entre métodos\")\n",
        "\n",
        "print(\"\\n🛠️ HERRAMIENTAS TÉCNICAS:\")\n",
        "print(\"  - Librería gensim para manipular vectores\")\n",
        "print(\"  - Visualización con t-SNE y PCA\")\n",
        "print(\"  - Evaluación cuantitativa de calidad de embeddings\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reflexion-limitaciones",
      "metadata": {
        "id": "reflexion-limitaciones"
      },
      "outputs": [],
      "source": [
        "# Reflexión crítica: limitaciones de los vectores semánticos\n",
        "print(\"REFLEXIÓN CRÍTICA: LIMITACIONES DE VECTORES SEMÁNTICOS\")\n",
        "print(\"=\" * 58)\n",
        "\n",
        "print(\"\\n⚠️  PROBLEMAS QUE AÚN EXISTEN:\")\n",
        "\n",
        "# Problema 1: Polisemia\n",
        "palabras_polisemicas = [\n",
        "    'banco',  # institución financiera vs asiento\n",
        "    'capital', # ciudad vs dinero\n",
        "    'carta',  # documento vs naipe\n",
        "    'tiempo'  # duración vs clima\n",
        "]\n",
        "\n",
        "print(\"\\n1️⃣  POLISEMIA (múltiples significados):\")\n",
        "for palabra in palabras_polisemicas:\n",
        "    token = nlp(palabra)\n",
        "    print(f\"  '{palabra}': un solo vector para todos sus significados\")\n",
        "\n",
        "print(\"  → Los embeddings estáticos promedian todos los usos\")\n",
        "print(\"  → No distinguen contexto específico de uso\")\n",
        "\n",
        "# Problema 2: Sesgos\n",
        "print(\"\\n2️⃣  SESGOS SOCIALES EN LOS DATOS:\")\n",
        "ejemplos_sesgo = [\n",
        "    ('doctor', 'enfermera', 'Profesiones de salud'),\n",
        "    ('programador', 'secretaria', 'Profesiones técnicas')\n",
        "]\n",
        "\n",
        "for palabra1, palabra2, categoria in ejemplos_sesgo:\n",
        "    try:\n",
        "        token1 = nlp(palabra1)\n",
        "        token2 = nlp(palabra2)\n",
        "        sim = token1.similarity(token2)\n",
        "        print(f\"  {categoria}: '{palabra1}' ⟷ '{palabra2}' = {sim:.3f}\")\n",
        "    except:\n",
        "        print(f\"  {categoria}: error en el cálculo\")\n",
        "\n",
        "print(\"  → Los vectores reflejan sesgos presentes en los datos de entrenamiento\")\n",
        "print(\"  → Problema ético importante en aplicaciones reales\")\n",
        "\n",
        "print(\"\\n3️⃣  ESTATICIDAD:\")\n",
        "print(\"  - Un vector por palabra, independiente del contexto\")\n",
        "print(\"  - No capturan cambios de significado en diferentes oraciones\")\n",
        "print(\"  - Limitación que motivó el desarrollo de modelos contextuales (BERT, etc.)\")\n",
        "\n",
        "print(\"\\n💡 PERSPECTIVA:\")\n",
        "print(\"Los word embeddings fueron un avance revolucionario, pero no la solución final.\")\n",
        "print(\"En cursos avanzados explorarán embeddings contextuales que superan estas limitaciones.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sintesis-cierre",
      "metadata": {
        "id": "sintesis-cierre"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. Síntesis y Reflexión Final\n",
        "\n",
        "### El Camino Recorrido Hoy\n",
        "\n",
        "En esta clase transitamos desde las limitaciones concretas de BoW/TF-IDF hasta las posibilidades de las representaciones semánticas. No fue un salto abstracto, sino una progresión basada en problemas reales que experimentaron con el corpus de Casciari:\n",
        "\n",
        "1. **Identificamos limitaciones específicas**: ceguera semántica, pérdida de contexto, fragmentación por sinónimos\n",
        "2. **Experimentamos con soluciones**: vectores densos, similitud semántica, relaciones conceptuales\n",
        "3. **Desarrollamos intuición**: comprendimos por qué la geometría puede capturar significado\n",
        "4. **Preparamos conceptos**: establecimos las bases para word embeddings del jueves\n",
        "\n",
        "### Cambio de Paradigma Fundamental\n",
        "\n",
        "Lo que experimentaron hoy representa un **cambio de paradigma** en NLP:\n",
        "\n",
        "**Paradigma anterior (BoW/TF-IDF):**\n",
        "- Palabras como símbolos discretos e independientes\n",
        "- Representaciones basadas en co-ocurrencia exacta\n",
        "- Espacios de alta dimensionalidad y dispersos\n",
        "- Significado = frecuencia\n",
        "\n",
        "**Nuevo paradigma (Embeddings):**\n",
        "- Palabras como puntos en espacio semántico continuo\n",
        "- Representaciones basadas en contexto distribucional\n",
        "- Espacios de dimensionalidad moderada y densos\n",
        "- Significado = posición relativa\n",
        "\n",
        "### Conexión con el Jueves\n",
        "\n",
        "En la práctica del jueves van a:\n",
        "- **Implementar** los algoritmos que generan estos vectores\n",
        "- **Entrenar** modelos Word2Vec en español\n",
        "- **Comparar** FastText y GloVe en tareas específicas\n",
        "- **Evaluar** calidad de embeddings cuantitativamente\n",
        "- **Aplicar** vectores a problemas de similitud y analogías\n",
        "\n",
        "### Preguntas para la Reflexión\n",
        "\n",
        "Antes del jueves, reflexionen sobre:\n",
        "\n",
        "1. **¿Cómo cambiaría su análisis de Casciari** si hubieran usado vectores semánticos desde el principio?\n",
        "2. **¿Qué patrones adicionales** podrían haber descubierto con representaciones semánticas?\n",
        "3. **¿En qué aplicaciones** serían cruciales estos vectores vs BoW tradicional?\n",
        "4. **¿Cómo evaluarían** si un conjunto de embeddings es \"bueno\" para una tarea específica?\n",
        "\n",
        "### Impacto en NLP Moderno\n",
        "\n",
        "Los conceptos que exploraron hoy son **fundamentales** para entender:\n",
        "- Modelos de lenguaje modernos (GPT, BERT)\n",
        "- Sistemas de recomendación basados en texto\n",
        "- Traducción automática neural\n",
        "- Análisis de sentimientos avanzado\n",
        "- Búsqueda semántica en documentos\n",
        "\n",
        "**Los word embeddings fueron el primer paso hacia la IA que \"entiende\" texto.** En el jueves van a aprender exactamente cómo funcionan por dentro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ejercicio-final",
      "metadata": {
        "id": "ejercicio-final"
      },
      "outputs": [],
      "source": [
        "# Ejercicio final de reflexión: preparación para el jueves\n",
        "print(\"EJERCICIO DE PREPARACIÓN PARA LA PRÁCTICA DEL JUEVES\")\n",
        "print(\"=\" * 56)\n",
        "\n",
        "print(\"\\n📝 PREGUNTAS DE AUTOEVALUACIÓN:\")\n",
        "print(\"\\n1. Conceptual:\")\n",
        "print(\"   - ¿Por qué BoW no puede distinguir sinónimos?\")\n",
        "print(\"   - ¿Qué significa que dos vectores estén 'cerca' en el espacio semántico?\")\n",
        "print(\"   - ¿Por qué 300 dimensiones pueden capturar más información que 30,000?\")\n",
        "\n",
        "print(\"\\n2. Práctico:\")\n",
        "print(\"   - ¿Cómo usarían similitud semántica para mejorar búsquedas en documentos?\")\n",
        "print(\"   - ¿Qué ventajas tendría FastText vs Word2Vec para analizar textos argentinos?\")\n",
        "print(\"   - ¿Cómo evaluarían si sus embeddings capturan bien el español rioplatense?\")\n",
        "\n",
        "print(\"\\n3. Crítico:\")\n",
        "print(\"   - ¿Qué sesgos podrían aparecer en embeddings entrenados con noticias?\")\n",
        "print(\"   - ¿Cuándo seguirían usando BoW en lugar de embeddings?\")\n",
        "print(\"   - ¿Qué limitaciones de embeddings estáticos motivaron BERT?\")\n",
        "\n",
        "print(\"\\n🎯 OBJETIVOS PARA EL JUEVES:\")\n",
        "print(\"   ✓ Cargar y usar vectores Word2Vec pre-entrenados\")\n",
        "print(\"   ✓ Implementar búsqueda por similitud\")\n",
        "print(\"   ✓ Resolver analogías con álgebra de vectores\")\n",
        "print(\"   ✓ Comparar Word2Vec, FastText y GloVe\")\n",
        "print(\"   ✓ Visualizar embeddings en 2D\")\n",
        "print(\"   ✓ Evaluar calidad de vectores cuantitativamente\")\n",
        "\n",
        "print(\"\\n💡 CONSEJO FINAL:\")\n",
        "print(\"Lleguen el jueves con la mente abierta a experimentar. Los embeddings\")\n",
        "print(\"son tanto matemática como arte: hay que desarrollar intuición práctica\")\n",
        "print(\"para usarlos efectivamente en problemas reales.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"¡Nos vemos el jueves para la aventura práctica con embeddings!\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
